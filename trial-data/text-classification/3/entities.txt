23	17	37	focus on the role of
23	38	66	preprocessing the input text
23	69	88	particularly in how
23	95	105	split into
23	106	145	individual ( meaning - bearing ) tokens
23	157	183	affects the performance of
23	184	226	standard neural text classification models
32	94	141	github.com/pedrada88/preproc-textclassification
63	3	8	tried
63	14	39	two classification models
64	4	16	first one is
64	19	37	standard CNN model
64	38	48	similar to
64	59	103	using ReLU as non-linear activation function
65	7	13	second
65	14	19	model
65	25	28	add
65	31	46	recurrent layer
65	49	64	specifically an
65	65	69	LSTM
65	72	86	before passing
65	91	106	pooled features
65	107	118	directly to
68	4	19	embedding layer
68	24	41	initialized using
68	42	84	300 - dimensional CBOW Word2vec embeddings
68	85	95	trained on
68	100	129	3B - word UMBC WebBase corpus
68	130	134	with
68	135	159	standard hyperparameters
2	15	65	Text Preprocessing in Neural Network Architectures
4	0	18	Text preprocessing
23	30	66	role of preprocessing the input text
95	19	25	use of
95	26	63	more complex preprocessing techniques
95	64	71	such as
95	72	108	lemmatization and multiword grouping
95	109	117	does not
95	118	122	help
100	22	29	observe
100	32	80	different trend , with multiwordenhanced vectors
100	81	91	exhibiting
100	94	112	better performance
100	113	120	both on
100	125	141	single CNN model
100	144	171	best overall performance in
100	172	198	seven of the nine datasets
100	212	228	CNN + LSTM model
100	231	250	best performance in
100	251	264	four datasets
100	276	292	same ballpark as
100	297	309	best results
100	310	320	in four of
100	325	348	remaining five datasets
109	16	21	using
109	22	49	multiword - wise embeddings
109	50	52	on
109	57	72	vanilla setting
109	73	81	leads to
109	82	109	consistently better results
109	110	128	than using them on
109	133	175	same multiwordgrouped preprocessed dataset
109	8	10	in
109	179	205	eight of the nine datasets
111	50	56	use of
111	61	71	embeddings
111	72	82	trained on
111	85	125	simple tokenized corpus ( i.e. vanilla )
