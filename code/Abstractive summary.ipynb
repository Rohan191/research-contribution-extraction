{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate abstractive summary for every document based on predicted contribution statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence, pad_packed_sequence\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from constants import BASE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Pegasus model not used as it generates very short summaries\n",
    "\n",
    "# from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "# import torch\n",
    "# model_name = \"google/pegasus-xsum\"\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "# model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Facebook Bart model to generate abstract summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fb1d58a82e94e1787e56ba9285bce1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0a970b9aa5493b93fb3d2fe378c9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "722be3af863d40ea92bb4d7272978fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6210c65d36d34f9ea76db135f8e969a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.76k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbfbf4533e414c828433f6fa88ebcd5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.14G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through data predicted in validation set and generate summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_num</th>\n",
       "      <th>sentence</th>\n",
       "      <th>target</th>\n",
       "      <th>doc_path</th>\n",
       "      <th>abstract</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>We present a memory augmented neural network f...</td>\n",
       "      <td>0</td>\n",
       "      <td>/Users/rohantondulkar/Projects/Typeset/trial-d...</td>\n",
       "      <td>We present a memory augmented neural network f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>NSE is equipped with a novel memory update rul...</td>\n",
       "      <td>0</td>\n",
       "      <td>/Users/rohantondulkar/Projects/Typeset/trial-d...</td>\n",
       "      <td>We present a memory augmented neural network f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>NSE can also access 1 multiple and shared memo...</td>\n",
       "      <td>0</td>\n",
       "      <td>/Users/rohantondulkar/Projects/Typeset/trial-d...</td>\n",
       "      <td>We present a memory augmented neural network f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>In this paper , we demonstrated the effectiven...</td>\n",
       "      <td>1</td>\n",
       "      <td>/Users/rohantondulkar/Projects/Typeset/trial-d...</td>\n",
       "      <td>We present a memory augmented neural network f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>For example , our shared - memory model showed...</td>\n",
       "      <td>0</td>\n",
       "      <td>/Users/rohantondulkar/Projects/Typeset/trial-d...</td>\n",
       "      <td>We present a memory augmented neural network f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_num                                           sentence  target  \\\n",
       "0        7  We present a memory augmented neural network f...       0   \n",
       "1        7  NSE is equipped with a novel memory update rul...       0   \n",
       "2        7  NSE can also access 1 multiple and shared memo...       0   \n",
       "3        7  In this paper , we demonstrated the effectiven...       1   \n",
       "4        7  For example , our shared - memory model showed...       0   \n",
       "\n",
       "                                            doc_path  \\\n",
       "0  /Users/rohantondulkar/Projects/Typeset/trial-d...   \n",
       "1  /Users/rohantondulkar/Projects/Typeset/trial-d...   \n",
       "2  /Users/rohantondulkar/Projects/Typeset/trial-d...   \n",
       "3  /Users/rohantondulkar/Projects/Typeset/trial-d...   \n",
       "4  /Users/rohantondulkar/Projects/Typeset/trial-d...   \n",
       "\n",
       "                                            abstract  predicted  \n",
       "0  We present a memory augmented neural network f...          0  \n",
       "1  We present a memory augmented neural network f...          1  \n",
       "2  We present a memory augmented neural network f...          1  \n",
       "3  We present a memory augmented neural network f...          1  \n",
       "4  We present a memory augmented neural network f...          1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = pd.read_csv(os.path.join(BASE_DIR, 'generated_data', 'val_df_predicted.csv'))\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_nums = val_df['doc_num'].unique()\n",
    "generated_summaries = []\n",
    "abstract_ground_truth = []\n",
    "for doc_num in doc_nums:\n",
    "    abstract = val_df[(val_df['doc_num'] == doc_num) & (val_df['predicted'] == 1)]['abstract'].iloc[0]\n",
    "    abstract_ground_truth.append(abstract)\n",
    "    contrib_sentences = val_df[(val_df['doc_num'] == doc_num) & (val_df['predicted'] == 1)]['sentence'].to_list()\n",
    "    contrib_sentences = ' '.join(contrib_sentences)\n",
    "    batch = tokenizer(contrib_sentences, truncation=True, padding=\"longest\", return_tensors=\"pt\").to(device)\n",
    "    translated = model.generate(**batch)\n",
    "    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    generated_summaries.append(tgt_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate scores for various Rouge metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score(precision=0.76156014117662, recall=0.36139191482009186, fmeasure=0.479370646063431)\n",
      "Score(precision=0.5570170173965329, recall=0.2697756784048152, fmeasure=0.35291120305554746)\n",
      "Score(precision=0.6452480020385981, recall=0.31132330177128353, fmeasure=0.4095962525242688)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "rouge_score = load_metric(\"rouge\")\n",
    "scores = rouge_score.compute(\n",
    "    predictions=generated_summaries, references=abstract_ground_truth\n",
    ")\n",
    "print(scores['rouge1'].mid)\n",
    "print(scores['rouge2'].mid)\n",
    "print(scores['rougeL'].mid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate scores for Bert score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert scores - Precision: 0.9055407404899597, Recall: 0.8540546655654907, F1 score: 0.8788825869560242\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "bert_score_metric = load_metric(\"bertscore\")\n",
    "bert_scores = bert_score_metric.compute(\n",
    "    predictions=generated_summaries, references=abstract_ground_truth, lang='en'\n",
    ")\n",
    "precision = np.average(bert_scores['precision'])\n",
    "recall = np.average(bert_scores['recall'])\n",
    "f1_score = np.average(bert_scores['f1'])\n",
    "print(f'Bert scores - Precision: {precision}, Recall: {recall}, F1 score: {f1_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store metric scores in results folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rouge1</td>\n",
       "      <td>0.761560</td>\n",
       "      <td>0.361392</td>\n",
       "      <td>0.479371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rouge2</td>\n",
       "      <td>0.557017</td>\n",
       "      <td>0.269776</td>\n",
       "      <td>0.352911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rougeL</td>\n",
       "      <td>0.645248</td>\n",
       "      <td>0.311323</td>\n",
       "      <td>0.409596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bert score</td>\n",
       "      <td>0.905541</td>\n",
       "      <td>0.854055</td>\n",
       "      <td>0.878883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       metric  precision    recall        f1\n",
       "0      rouge1   0.761560  0.361392  0.479371\n",
       "1      rouge2   0.557017  0.269776  0.352911\n",
       "2      rougeL   0.645248  0.311323  0.409596\n",
       "3  bert score   0.905541  0.854055  0.878883"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_records = []\n",
    "score_records.append({\n",
    "    'metric': 'rouge1', 'precision':scores['rouge1'].mid.precision, 'recall':scores['rouge1'].mid.recall, 'f1':scores['rouge1'].mid.fmeasure\n",
    "})\n",
    "score_records.append({\n",
    "    'metric': 'rouge2', 'precision':scores['rouge2'].mid.precision, 'recall':scores['rouge2'].mid.recall, 'f1':scores['rouge2'].mid.fmeasure\n",
    "})\n",
    "score_records.append({\n",
    "    'metric': 'rougeL', 'precision':scores['rougeL'].mid.precision, 'recall':scores['rougeL'].mid.recall, 'f1':scores['rougeL'].mid.fmeasure\n",
    "})\n",
    "score_records.append({\n",
    "    'metric': 'bert score', 'precision':np.average(bert_scores['precision']), 'recall':np.average(bert_scores['recall']), \n",
    "    'f1':np.average(bert_scores['f1'])\n",
    "})\n",
    "scores_df = pd.DataFrame(score_records)\n",
    "scores_df.to_csv(os.path.join(BASE_DIR, 'results', 'scores.csv'), index=False)\n",
    "scores_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl_practice]",
   "language": "python",
   "name": "conda-env-dl_practice-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
