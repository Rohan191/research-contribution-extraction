{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to use previous generated trained model and get summmary for new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import nltk\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence, pad_packed_sequence\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from models import LSTMNet\n",
    "from constants import BASE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_FILE = os.path.join(BASE_DIR, 'papers', 'P19-1106.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data from new pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_file_abstract(file_name):\n",
    "    \"\"\" Extract abstract from pdf file text\"\"\"\n",
    "    with fitz.open(file_name) as doc:\n",
    "        for page in doc:\n",
    "            blocks = page.get_text('blocks')    \n",
    "            start_abstract = False\n",
    "            for block in blocks:\n",
    "                if block[4].lower().strip() == 'abstract':\n",
    "                    start_abstract = True\n",
    "                    continue\n",
    "                if start_abstract:\n",
    "                    abstract=block[4]\n",
    "                    start_abstract = False\n",
    "                    break\n",
    "    return abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_file_sentences(file_name):\n",
    "    \"\"\" Extract all sentences from pdf file text\"\"\"\n",
    "    all_sentences = []\n",
    "    with fitz.open(file_name) as doc:\n",
    "        for page in doc:\n",
    "            blocks = page.get_text('blocks')    \n",
    "            for block in blocks:\n",
    "                if len(block[4].split()) > 10 and '@' not in block[4] and 'Proceedings' not in block[4]:\n",
    "                    all_sentences.extend(nltk.sent_tokenize(block[4]))\n",
    "    return all_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = get_new_file_abstract(PDF_FILE)\n",
    "sentences = get_new_file_sentences(PDF_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings for sentences in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use trained model to get contributing statements for given file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_net = torch.load(os.path.join(BASE_DIR, 'models', 'trained_model.pt'))\n",
    "HIDDEN_SIZE = 200\n",
    "THRESHOLD = 0.6\n",
    "inputs = torch.Tensor(embeddings)\n",
    "inputs = torch.unsqueeze(inputs, 0)\n",
    "random_inputs = torch.zeros(inputs.size())\n",
    "final_inputs = torch.cat((inputs, random_inputs), 0)\n",
    "packed_input = pack_padded_sequence(final_inputs, torch.IntTensor([290,290]), batch_first=True, enforce_sorted=False)\n",
    "h_0 = Variable(torch.zeros(1, final_inputs.shape[0], HIDDEN_SIZE))\n",
    "c_0 = Variable(torch.zeros(1, final_inputs.shape[0], HIDDEN_SIZE))\n",
    "outputs = lstm_net(packed_input, h_0, c_0)\n",
    "outputs = torch.squeeze(outputs)\n",
    "predicted = (outputs > THRESHOLD).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrib_predictions = predicted[0]\n",
    "contrib_predictions = (contrib_predictions == 1).nonzero()\n",
    "contributing_sentences = [sentences[i[0]] for i in contrib_predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate abstract summary for contributing statements using Bart model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' The present-day peer review process is not straightforward and demands the domain knowledge, expertise, and intelligence of human reviewers. We investigate the role of reviewersâ€™sentiments embedded within peer review texts to predict the peer review outcome. We attribute this to the use of deep neural networks and augmentation of review sentiment information in our new architecture.']\n"
     ]
    }
   ],
   "source": [
    "batch = tokenizer(' '.join(contributing_sentences), truncation=True, padding=\"longest\", return_tensors=\"pt\").to(device)\n",
    "translated = model.generate(**batch)\n",
    "tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "print(tgt_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate scores for various Rouge and Bert Score metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score(precision=0.9137931034482759, recall=0.2245762711864407, fmeasure=0.36054421768707484)\n",
      "Score(precision=0.6140350877192983, recall=0.14893617021276595, fmeasure=0.23972602739726026)\n",
      "Score(precision=0.7068965517241379, recall=0.17372881355932204, fmeasure=0.27891156462585037)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "rouge_score = load_metric(\"rouge\")\n",
    "scores = rouge_score.compute(\n",
    "    predictions=[tgt_text], references=[abstract]\n",
    ")\n",
    "print(scores['rouge1'].mid)\n",
    "print(scores['rouge2'].mid)\n",
    "print(scores['rougeL'].mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert scores - Precision: 0.8878797888755798, Recall: 0.7869012355804443, F1 score: 0.834346354007721\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "bert_score_metric = load_metric(\"bertscore\")\n",
    "bert_scores = bert_score_metric.compute(\n",
    "    predictions=[tgt_text], references=[abstract], lang='en'\n",
    ")\n",
    "precision = np.average(bert_scores['precision'])\n",
    "recall = np.average(bert_scores['recall'])\n",
    "f1_score = np.average(bert_scores['f1'])\n",
    "print(f'Bert scores - Precision: {precision}, Recall: {recall}, F1 score: {f1_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl_practice]",
   "language": "python",
   "name": "conda-env-dl_practice-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
