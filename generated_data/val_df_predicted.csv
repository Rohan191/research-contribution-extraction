doc_num,sentence,target,doc_path,abstract,predicted
4,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",1
4,"The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"Finally , a similarity score is estimated over the composed feature vectors .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,It plays an important role fora variety of tasks in both NLP and IR communities .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,1 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,There is a lexical gap between semantically equivalent sentences .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"Take the E 1 and E 2 in Table 1 for example , they have the similar meaning but with different lexicons .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"2 . Semantic similarity should be measured at different levels of granularity ( word - level , phrase - level and syntax - level ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"E.g. , "" not related "" in E 2 is an indivisible phrase when matching with "" irrelevant "" in E 1 ( shown in square brackets ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,3 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"The dissimilarity ( shown in angle brackets ) between two sentences is also a significant clue ( Qiu et al. , 2006 ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"For example , by judging the dissimilar parts , we can easily identify that E 3 and E 5 share the similar meaning "" The study is about salmon "" , because "" sockeye "" belongs to the salmon family , and "" flounder "" does not .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"Whereas the meaning of E 4 is quite different from E 3 , which emphasizes "" The study is about red ( a special kind of ) salmon "" , because both "" sockeye "" and "" coho "" are in the salmon family .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,How we can extract and utilize those information becomes another challenge .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"In order to handle the above challenges , researchers have been working on sentence similarity algorithms fora longtime .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"To bridge the lexical gap ( challenge 1 ) , some word similarity metrics were proposed to match different but semantically related words .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",1
4,"Examples include knowledge - based metrics ( Resnik , 1995 ) and corpus - based metrics ( Jiang and Conrath , 1997 ; Yin and Schtze , 2015 ; He et al. , 2015 ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",1
4,"To measure sentence similarity from various granularities ( challenge 2 ) , researchers have explored features extracted from n-grams , continuous phrases , discontinuous phrases , and parse trees ( Yin and Schtze , 2015 ; He et al. , 2015 ; Heilman and Smith , 2010 ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,The research is to sockeye .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,The study is [ not related ] to salmon .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,The research is relevant to salmon .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"The study is relevant to sockeye , instead of coho .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"Examples for sentence similarity learning , where sockeye means "" red salmon "" , and coho means "" silver salmon "" .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,""" coho "" and "" sockeye "" are in the salmon family , while "" flounder "" is not .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"attention in the past , the only related work of explored the dissimilarity between sentences in a pair for paraphrase identification task , but they require human annotations in order to train a classifier , and their performance is still below the state of the art .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",1
4,"In this paper , we propose a novel model to tackle all these challenges jointly by decomposing and composing lexical semantics over sentences .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",1
4,"Given a sentence pair , the model represents each word as a low -dimensional vector ( challenge 1 ) , and calculates a semantic matching vector for each word based on all words in the other sentence ( challenge 2 ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",1
4,"Then based on the semantic matching vector , each word vector is decomposed into two components : a similar component and a dissimilar component ( challenge 3 ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"We use similar components of all the words to represent the similar parts of the sentence pair , and dissimilar components of every word to model the dissimilar parts explicitly .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"After this , a two - channel CNN operation is performed to compose the similar and dissimilar components into a feature vector ( challenge 2 and 3 ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"Finally , the composed feature vector is utilized to predict the sentence similarity .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"Experimental results on two tasks show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"In following parts , we start with a brief overview of our model ( Section 2 ) , followed by the details of our end - to - end implementation ( Section 3 ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,Then we evaluate our model on answer sentence selection and paraphrase identifications tasks ( Section 4 ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"In this section , we propose a sentence similarity learning model to tackle all three challenges ( mentioned in Section 1 ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",1
4,"To deal with the first challenge , we represent each word as a distributed vector , so that we can calculate similarities for formally different but semantically related words .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",1
4,"To tackle the second challenge , we assume that each word can be semantically matched by several words in the other sentence , and we calculate a semantic matching vector for each word vector based on all the word vectors in the other side .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",1
4,"To cope with the third challenge , we assume that each semantic unit ( word ) can be partially matched , and can be decomposed into a similar component and a dissimilar component based on its semantic matching vector .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,shows an overview of our sentence similarity model .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,Word Representation .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"Word embedding of is an effective way to handle the lexical gap challenge in the sentence similarity task , as it represents each word with a distributed vector , and words appearing in similar contexts tend to have similar meanings .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"With those pre-trained embeddings , we transform Sand T into sentence matrixes S = [ s 1 , ... , s i , ... , s m ] and T = [t 1 , ... , t j , ... , tn ] , where s i and t j are d-dimension vectors of the corresponding words , and m and n are sentence length of Sand T respectively .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,Semantic Matching .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"In order to judge the similarity between two sentences , we need to check whether each semantic unit in one sentence is covered by the other sentence , or vice versa .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"For example , in , to check whether E 2 is a paraphrase of E 1 , we need to know the single word "" irrelevant "" in E 1 is matched or covered by the phrase "" not related "" in E 2 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,i for each word s i by composing part or full word vectors in the other sentence T .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"In this way , we can match a word s i to a word or phrase in T . Similarly , for the reverse direction , we also calculate all semantic matching vectorst j in T .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,We explore different f match functions later in Section 3 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,Decomposition .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,i and t j .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,i ( ort j ) as a semantic coverage of word s i ( or t j ) by the sentence T ( or S ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,i ( ort j ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"Take the E 1 and E 2 in for example , the word "" sockeye "" in E 1 is only partially matched by the word "" salmon "" ( the similar part ) in E 2 , as the full meaning of "" sockeye "" is "" red salmon "" ( the semantic meaning of "" red "" is the dissimilar part ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"i ( ort j ) , into two components : similar component s + i ( or t + j ) and dissimilar component s ? i ( or t ? j ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,", our goal in this step is how to utilize those information .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"Besides the suggestion from that the significance of the dissimilar parts alone between two sentences has a great effect of their similarity , we also think that the dissimilar and similar components have strong connections .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"For example , in , if we only look at the dissimilar or similar part alone , it is hard to judge which one between E 4 and E 5 is more similar to E 3 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"We can easily identify that E 5 is more similar to E 3 , when we consider both the similar and dissimilar parts .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,Similarity assessing .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,3 An End - to - End Implementation Section 2 gives us a glance of our model .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"In this section , we describe details of each phase .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,This subsection describes our specifications for the semantic matching function f match in Eq.,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,( 1 ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,i for s i by composing the vectors from T .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"where k = argmax j a i , j .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,The idea of the global function is to consider all word vectors t j in T .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"i is a weighted sum vector of all words t j in T , where each weight is the normalized word similarity a i , j .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,The max function moves to the other extreme .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,It generates the semantic matching vector by selecting the most similar word vector t k from T .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"The local -w function takes a compromise between global and max , where w indicates the size of the window to consider centered at k ( the most similar word position ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,So the semantic matching vector is a weighted average vector from t k?w tot k+w .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,This subsection describes the implementations for the decomposition function f decomp in Eq.,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,( 2 ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,i indicates the uncovered part .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"We implement three types of decomposition function : rigid , linear and orthogonal .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,The rigid decomposition only adapts to the max version off match .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,i .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,0 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,i .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"i , the higher proportion of s i should be assigned to the similar component .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,i .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"Then , we decompose s i linearly based on ?.",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,The orthogonal decomposition is to decompose a vector in the geometric space .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"i , our model decomposes s i into a parallel component and a perpendicular component .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,i .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,Eq. gives the concrete definitions .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,The aim of composition function f comp in Eq. is to extract features from both the similar component matrix and the dissimilar component matrix .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,We also want to acquire similarities and dissimilarities of various granularity during the composition phase .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"Inspired from Kim , we utilize a two - channel convolutional neural networks ( CNN ) and design filters based on various order of n-grams , e.g. , unigram , bigram and trigram .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,The CNN model involves two sequential operations : convolution and max - pooling .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"For the convolution operation , we define a list of filters {w o }.",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"The shape of each filter is d h , where dis the dimension of word vectors and h is the window size .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"Each filter is applied to two patches ( a window size h of vectors ) from both similar and dissimilar channels , and generates a feature .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,Eq. ( 10 ) expresses this process .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"To deal with variable feature size , we perform a max - pooling operation over co by selecting the maximum value co = max co .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"Therefore , after these two operations , each filter generates only one feature .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,We define several filters by varying the window size and the initial values .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"Eventually , a vector of features is captured by composing the two component matrixes , and the feature dimension is equal to the number of filters .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,The similarity assessment function f sim in Eq. ( 4 ) predicts a similarity score by taking two feature vectors as input .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"We employ a linear function to sum up all the features and apply a sigmoid function to constrain the similarity within the range [ 0 , 1 ] .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,We train our sentence similariy model by maximizing the likelihood on a training set .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"Each training instance in the training set is represented as a triple ( S i , Ti , Li ) , where Si and Ti area pair of sentences , and Li ? { 0 , 1 } indicates the similarity between them .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"We assign Li = 1 if Ti is a paraphrase of Si for the paraphrase identification task , or Ti is a correct answer for Si for the answer sentence selection task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"Otherwise , we assign Li = 0 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,We implement the mathematical expressions with Theano and use Adam for optimization .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,We evaluate our model on two tasks : answer sentence selection and paraphrase identification .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"The answer sentence selection task is to rank a list of candidate answers based on their similarities to a question sentence , and the performance is measured by mean average precision ( MAP ) and mean reciprocal rank ( MRR ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,We experiment on two datasets : QASent and Wiki QA .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"The statistics of the two datasets can be found in , where QASent was created from the TREC QA track , and WikiQA ( Yang et al. , 2015 ) is constructed from real queries of Bing and Wikipedia .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,The paraphrase identification task is to detect whether two sentences are paraphrases based on the similarity between them .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,The metrics include the accuracy and the positive class F 1 score .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"We experiment on the Microsoft Research Paraphrase corpus ( MSRP ) , which includes 2753 true and 1323 false instances in the training set , and 1147 true and 578 false instances in the test set .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,We build a development set by randomly selecting 100 true and 100 false instances from the training set .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"In all experiments , we set the size of word vector dimension as d = 300 , and pre-train the vectors with the word2 vec toolkit on the English Gigaword ( LDC2011T07 ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"There are several alternative options in our model , e.g. , the semantic matching functions , the decomposition operations , and the filter types .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,The choice of these options may affect the final performance .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"In this subsection , we present some experiments to demonstrate the properties of our model , and find a good configuration that we use to evaluate our final model .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,All the experiments in this subsection were performed on the QASent dataset and evaluated on the development set .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"First , we evaluated the effectiveness of various semantic matching functions .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"We switched the semantic matching functions among { max , global , local - l} , where l ? { 1 , 2 , 3 , 4 } , and fixed the other options as : the linear decomposition , the filter types including {unigram , bigram , trigram } , and 500 filters for each type .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,presents the results .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,We found that the max function worked better than the global function on both MAP and MRR .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"By increasing the window size , the local -l function acquired progressive improvements when the window size is smaller than 4 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"But after we enlarged the window size to 4 , the performance dropped .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"The local - 3 function worked better than the max function in term of the MAP , and also got a comparable MRR .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"Therefore , we use the local - 3 function in the following experiments .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"Second , we studied the effect of various decomposition operations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"We varied the decomposition operation among { rigid , linear , orthogonal } , and kept the other options unchanged .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,shows the performance .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,We found that the rigid operation got the worst result .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"This is reasonable , because the rigid operation decomposes word vectors by exactly matching words .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"The orthogonal operation got a similar MAP as the linear operation , and it worked better in term of MRR .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"Therefore , we choose the orthogonal operation in the following experiments .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"Third , we tested the influence of various filter types .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"We constructed 5 groups of filters : win - 1 contains only the unigram filters , win - 2 contains both unigram and bigram filters , win - 3 contains all the filters in win - 2 plus trigram filters , win - 4 extends filters in win - 3 with 4 - gram filters , and win - 5 adds 5 - gram filters into win - 4 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,We generate 500 filters for each filter type ( with different initial values ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,Experimental results are shown in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"At the beginning , adding higher - order ngram filters was helpful for the performance .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"The performance reached to the peak , when we used the win - 3 filters .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"After that , adding more complex filters decreased the performance .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"Therefore , the trigram is the best granularity for our model .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"In the following experiments , we utilize filter types in win - 3 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"In this subsection , we evaluated our model on the test sets of QASent , WikiQA and MSRP .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,QASent dataset .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,1 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"Given a pair of sentences , Severyn and Moschitti ( 2015 ) employed a CNN model to compose each sentence into a vector separately , and joined the two sentence vectors to compute the sentence similarity .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"Because only the sentencelevel granularity was used , the performance is much lower ( the second row of ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"After adding some word overlap features between the two sentences , the performance was improved significantly ( the third row of ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"Therefore , the lower - level granularity is an indispensable factor fora good performance .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"conducted word alignment fora sentence pair based on word vectors , and measured the sentence similarity based on a couple of word alignment features .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"They got a slightly better performance ( the fourth row of ) , which indicates that the vector representation for words is helpful to bridging the lexical gap problem .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"dos introduced the attention mechanism into the CNN model , and learnt sentence representation by considering the influence of the other sentence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,They got better performance than all the other previous work .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,Our model makes use of all these useful factors and also considers the dissimilarities of a sentence pair .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"We can see that our model ( the last row of ) got the best MAP among all previous work , and a comparable MRR than dos .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,Wiki QA dataset .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,presents the results of our model and several state - of - the - art models .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,constructed the dataset and reimplemented several baseline models .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,The best performance ( shown at the second row of ) was acquired by a bigram CNN model combining with the word overlap features .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",1
4,models the sentence similarity by enriching LSTMs with a latent stochastic attention mechanism .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",1
4,The corresponding performance is given at the fourth row of .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",1
4,"introduced the attention mechanism into the CNN model , and captured the best performance ( the fifth row of ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",1
4,The semantic matching phase in our model is similar to the attention mechanism .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",1
4,"But different from the previous models , our model utilizes both the similarity and dissimilarity simultaneously .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",1
4,The last row of shows that our model is more effective than the other models .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,MSRP dataset .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,granularity and modeled interaction features at each level fora pair of sentences .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,They obtained their best performance by pretraining the model on a language modeling task ( the 3rd row of ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",1
4,"However , their model heavily depends on the pretraining strategy .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",1
4,"Without pretraining , they got a much worse performance ( the second row of ) .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",1
4,proposed a similar model to .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"Similarly , they also used a CNN model to extract features at multiple levels of granularity .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"Differently , they utilized some extra annotated resources , e.g. , embeddings from part - of - speech ( POS ) tags and PARAGRAM vectors trained from the Paraphrase Database .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,Their model outperformed without the need of pretraining ( the sixth row of ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"However , the performance was reduced after removing the extra resources ( the fourth and fifth rows of ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,applied their attention - based CNN model on this dataset .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"By adding a couple of sparse features and using a layerwise training strategy , they got a pretty good performance .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"Comparing to these neural network based models , our model obtained a comparable performance ( the last row of ) without using any sparse features , extra annotated resources and specific training strategies .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"However , the best performance so far on this dataset is obtained by .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"In their model , they just utilized several hand - crafted features in a Support Vector Machine ( SVM ) model .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"Therefore , the deep learning methods still have along way to go for this task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,The semantic matching functions in subsection 3.1 are inspired from the attention - based neural machine translation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"However , most of the previous work using the attention mechanism in only LSTM models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,Whereas our model introduces the attention mechanism into the CNN model .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,A similar work is the attention - based CNN model proposed by .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"They first build an attention matrix fora sentence pair , and then directly take the attention matrix as anew channel of the CNN model .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"Differently , our model uses the attention matrix ( or similarity matrix ) to decompose the original sentence matrix into a similar component matrix and a dissimilar component matrix , and then feeds these two matrixes into a two - channel CNN model .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,The model can then focus much on the interactions between similar and dissimilar parts of a sentence pair .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"In this work , we proposed a model to assess sentence similarity by decomposing and composing lexical semantics .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"To bridge the lexical gap problem , our model represents each word with its context vector .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"To extract features from both the similarity and dissimilarity of a sentence pair , we designed several methods to decompose the word vector into a similar component and a dissimilar component .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,"To extract features at multiple levels of granularity , we employed a two - channel CNN model and equipped it with multiple types of ngram filters .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
4,Experimental results show that our model is quite effective on both the answer sentence selection task and the paraphrase identification task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/3/1602.07019v2-Stanza-out.txt,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences . In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences . The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence . Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector . After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components . Finally , a similarity score is estimated over the composed feature vectors . Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task . Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences . It plays an important role fora variety of tasks in both NLP and IR communities . For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) . For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
9,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",1
9,"Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",1
9,Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",1
9,"Recently , the idea of training machine comprehension models that can read , understand , and answer questions about a text has come closer to reality principally through two factors .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",1
9,"The first is the advent of deep learning techniques , which allow manipulation of natural language beyond its surface forms and generalize beyond relatively small amounts of labeled data .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",1
9,"The second factor is the formulation of standard machine comprehension benchmarks based on Cloze - style queries , which permit fast integration loops between model conception and experimental evaluation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",1
9,Cloze - style queries are created by deleting a particular word in a natural - language statement .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,The task is to guess which word was deleted .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"Ina pragmatic approach , recent work formed such questions by extracting a sentence from a larger document .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"In contrast to considering a stand - alone statement , the system is now required to handle a larger amount of information that may possibly influence the prediction of the missing word .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,Such contextual dependencies may also be injected by removing a word from a short human - crafted summary of a larger body of text .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,The abstractive nature of the summary is likely to demand a higher level of comprehension of the original text .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"In both cases , the machine comprehension system is presented with an ablated query and the document to which the original query refers .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,The missing word is assumed to appear in the document .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"Encouraged by the recent success of deep learning attention architectures , we propose a novel neural attention - based inference model designed to perform machine reading comprehension tasks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,The model first reads the document and the query using a recurrent neural network .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",1
9,"Then , it deploys an iterative inference process to uncover the inferential links that exist between the missing query word , the query , and the document .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",1
9,"This phase involves a novel alternating attention mechanism ; it first attends to some parts of the query , then finds their corresponding matches by attending to the document .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",1
9,The result of this alternating search is fed back into the iterative inference process to seed the next search step .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",1
9,"This permits our model to reason about different parts of the query in a sequential way , based on the information that has been gathered previously from the document .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",1
9,"After a fixed number of iterations , the model uses a summary of its inference process to predict the answer .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",1
9,This paper makes the following contributions .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",1
9,"We present a novel iterative , alternating attention mechanism that , unlike existing models , does not compress the query to a single representation , but instead alternates its attention between the query and the document to obtain a fine - grained query representation within a fixed computation time .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",1
9,Our architecture tightly integrates previous ideas related to bidirectional readers and iterative attention processes .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",1
9,It obtains state - of - theart results on two machine comprehension datasets and shows promise for application to abroad range of natural language processing tasks .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",1
9,One of the advantages of using Cloze - style questions to evaluate machine comprehension systems is that a sufficient amount of training and test data can be obtained without human intervention .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",1
9,The CBT and corpora are two such datasets .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,The CBT 1 corpus was generated from well - known children 's books available through Project Gutenberg .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,Documents consist of 20 - sentence excerpts from these books .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,The related query is formed from an excerpt 's 21st sentence by replacing a single word with an anonymous placeholder token .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,The dataset is divided into four subsets depending on the type of the word replaced .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"The subsets are named entity , common noun , verb , and preposition .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"We will focus our evaluation solely on the first two subsets , i.e. CBT - NE ( named entity ) and CBT - CN ( common nouns ) , since the latter two are relatively simple as demonstrated by .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,The CNN 2 corpus was generated from news articles available through the CNN website .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"The documents are given by the full articles themselves , which are accompanied by short , bullet - point summary statements .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"Instead of extracting a query from the articles themselves , the authors replace a named entity within each article summary with an anonymous placeholder token .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,Our model is represented in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,It s workflow has three steps .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"First is the encoding phase , in which we compute a set of vector representations , acting as a memory of the content of the input document and query .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"Next , the inference phase aims to untangle the complex semantic relationships linking the document and the query in order to provide sufficiently strong evidence for the answer prediction to be successful .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"To accomplish this , we use an iterative process that , at each iteration , alternates attentive memory accesses to the query and the document .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"Finally , the prediction phase uses the information gathered from the repeated attentions through the query and the document to maximize the probability of the correct answer .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,We describe each of the phases in the following sections .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"The input to the encoding phase is a sequence of words X = ( x 1 , . . . , x | X | ) , such as a document or a query , drawn from a vocabulary V .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,Rd stored in a word embedding matrix X ? R | V |d .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,The sequence X is processed using a recurrent neural network encoder with gated recurrent units ( GRU ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"For each position i in the input sequence , the GRU takes as input the word embedding xi and updates a hidden Figure 1 : Our model first encodes the query and the document by means of bidirectional GRU networks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"Then , it deploys an iterative inference mechanism that alternates between attending query encodings ( 1 ) and document encodings ( 2 ) given the query attended state .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,The results of the alternating attention is gated and fed back into the inference GRU .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"Even if the encodings are computed only once , the query representation is dynamic and changes throughout the inference process .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"After a fixed number of steps T , the weights of the document attention are used to estimate the probability of the answer P ( a|Q , D ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,is the sigmoid function and is the elementwise multiplication .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,The hidden state hi acts as a representation of the word x i in the context of the preceding sequence inputs x < i .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"In order to incorporate information from the future tokens x > i , we choose to process the sequence in reverse with an additional GRU .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,R 2h the contextual encodings for word i in the query Q and the document D respectively .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,This phase can be considered a means to uncover a possible inference chain that starts at the query and the document and leads to the answer .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,The inference is modelled by an additional recurrent GRU network .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,The recurrent network iteratively performs an alternating search step to gather information that maybe useful to predict the answer .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"In particular , at each time step : ( 1 ) it performs an attentive read on the query encodings , resulting in a query glimpse , qt , and ( 2 ) given the current query glimpse , it extracts a conditional document glimpse , d t , representing the parts of the document that are relevant to the current query glimpse .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"In turn , both attentive reads are conditioned on the previous hidden state of the inference GRU s t ?1 , summarizing the information that has been gathered from the query and the document up to time t.",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,The inference GRU uses both glimpses to update its recurrent state and thus decides which information needs to be gathered to complete the inference process .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,R 2 h .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"The attention we use here is similar to the formulation used in , but with two differences .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"First , we use a bilinear term instead of a simple dot product in order to compute the importance of each query term in the current time step .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,This simple bilinear attention has been successfully used in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"Second , we add a term a q that allows to bias the attention mechanism towards words which tend to be important across the questions independently of the search key s t?1 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,This is similar to what is achieved by the original attention mechanism proposed in without the burden of the additional tanh layer .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,The alternating attention continues by probing the document given the current query glimpse qt .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,R 2 h .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,Note that the document attention is also conditioned on s t?1 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"This allows the model to perform transitive reasoning on the document side , i.e. to use previously obtained document information to bias future attended locations , which is particularly important for natural language inference tasks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"In order to update its recurrent state , the inference GRU may evolve on the basis of the information gathered from the current inference step , i.e.",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"However , the current query glimpse maybe too general or the document may not contain the information specified in the query glimpse , i.e. the query or the document attention weights maybe nearly uniform .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,We include a gating mechanism that is designed to reset the current query and document glimpses in the case that the current search is not fruitful .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,R 2 h .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,The gate g takes the form of a 2 - layer feed - forward network with sigmoid output unit activation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"The fourth argument of the gate takes into account multiplicative interactions between query and document glimpses , making it easier to determine the degree of matching between them .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,.,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"Intuitively , the model reviews the query glimpse with respect to the contents of the document glimpse and vice versa .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"After a fixed number of time - steps T , the document attention weights obtained in the last search step d i , T are used to predict the probability of the answer given the document and the query P ( a|Q , D ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"where I ( a , D ) is a set of positions where a occurs in the document .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"The model is trained to maximize log P ( a|Q , D ) over the training corpus .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"To train our model , we used stochastic gradient descent with the ADAM optimizer ( Kingma and Ba , 2014 ) , with an initial learning rate of 0.001 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"We set the batch size to 32 and we decay the learning rate by 0.8 if the accuracy on the validation set does not increase after a half - epoch , i.e. 2000 batches ( for CBT ) and 5000 batches for ( CNN ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"We initialize all weights of our model by sampling from the normal distribution N ( 0 , 0.05 ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"Following , the GRU recurrent weights are initialized to be orthogonal and biases are initialized to zero .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"In order to stabilize the learning , we clip the gradients if their norm is greater than 5 and those marked with 2 are from .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,the inputs to both the query and the document attention mechanisms .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"We found that setting embedding regularization to 0.0001 , T = 8 , d = 384 , h = 128 , s = 512 worked robustly across the datasets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"Our model is implemented in Theano , using the Keras library .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"Computational Complexity Similar to previous state - of - the - art models which use a bidirectional encoder , the major bottleneck of our method is computing the document and query encodings .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"The alternating attention mechanism runs only fora fixed number of steps ( T = 8 in our tests ) , which is orders of magnitude smaller than atypical document or query in our datasets ( see ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,The repeated attentions each require a softmax over ? 1000 locations which is typically fast on recent GPU architectures .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"Thus , our computation cost is comparable to , but we outperform the latter models on the datasets tested .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"We report the results of our model on the CBT - CN , CBT - NE and CNN datasets , previously described in Section 2 . reports our results on the CBT - CN and CBT - NE dataset .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"The Humans , LSTMs and Memory Networks ( Mem NNs ) results are taken from and the Attention - Sum Reader ( AS Reader ) is a state - of - the - art result recently obtained by .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,Our model ( line 7 ) sets anew stateof - the - art on the common noun category by gaining 3.6 and 5.6 points in validation and test over the best baseline AS Reader ( line 5 ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,This performance gap is only partially reflected on the CBT - NE dataset .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"We observe that the 1.4 accuracy points on the validation set do not reflect better performance on the test set , which sits on par with the best baseline .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"In CBT - NE , the missing word is a named entity appearing in the story which is likely to be less frequent than a common noun .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,We found that approximatively 27.5 % of validation examples and 29.6 % of test examples contain an answer that has never been predicted in the training set .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"These numbers are considerably lower for the CBT - CN , for which only 2.5 % and 4.6 % of validation and test examples respectively contain an answer that has not been previously seen .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,Ensembles Fusing multiple models generally achieves better generalization .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"In order to investigate whether this could help achieving better held - out performance on CBT - NE , we adopt a simple strategy and average the predictions of 5 models trained with different random seeds ( line 9 , 3 from and 4 from .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,improvements over the single model and sits at 74.1 on validation and 71.0 on test .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"In order to measure the impact of the query attention step in our model , we constrain the query attention weights q i , t to be uniform , i.e. q i , t = 1 / | Q | , for all t = 1 , . . . , T ( line 6 ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,This corresponds to fixing the query representation to the average pooling over the bidirectional query encodings and is similar in spirit to previous work .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"By comparing line 6 and line 7 , we see that the query attention mechanism allows improvements up to 2.3 points in validation and 4.9 points in test with respect to fixing the query representation throughout the search process .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,A similar scenario was observed on the CNN dataset ..,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,The results show that our model ( line 8 ) improves state - of - the - art accuracy by 4 percent absolute on validation and 3.4 on test with respect to the most recent published result ( AS Reader ) ( line 7 ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,We also report the very recent results of the Stanford AR system that came to our attention during the writeup of this article ) ( line 9 ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,Our model slightly improves over this strong baseline by 0.2 percent on validation and 0.9 percent on test .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,We note that the latter comparison maybe influenced by different training and initialization strategies .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"First , Stanford AS uses Glo Ve embeddings , pre-trained from a large external corpus .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"Second , the system normalizes the output probabilities only over the candidate answers in the document .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,We also report the results using ensembled models .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"Similarly to the single model case , our ensembles achieve state - of - the - art test performance of 75.2 and 76.1 on validation and test respectively , outperforming previously published results .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,Category analysis classified a sample of 100 CNN stories based on the type of inference required to guess the answer .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"Categories that only require local context matching around the placeholder and the answer in the text are Exact Match , Paraphrasing , and Partial Clue , while those which require higher reasoning skills are Multiple Sentences and Ambiguous .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"For example , in Exact Match examples , the question placeholder and the answer in the document share several neighboring exact words .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,Category - specific results are reported in : Per-category performance of the Stanford AR and our system .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"The first three categories require local context matching , the next two global context matching and coreference errors are unanswerable questions .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"tackled by the neural models , which perform similarly .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,It seems that the iterative alternating attention inference is better able to solve more difficult examples such as Ambiguous / Hard .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"One hypothesis is that , in contrast to Stanford AR , which uses only one fixedquery attention step , our iterative attention may better explore the documents and queries .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"Finally , Coreference Errors ( ? 25 % of the corpus ) includes examples with critical coreference resolution errors which may make the questions "" unanswerable "" .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,This is a barrier to achieving accuracies considerably above 75 % .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"If this estimate is accurate , our ensemble model ( 76.1 % ) maybe approaching near-optimal performance on this dataset .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,We inspect the query and document attention weights for an example article from the CNN dataset .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",1
9,"The title of the article is "" Dante turns in his grave as Italian language declines "" , and it discusses the decline of Italian language in schools .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",1
9,"The plot is shown in . 2 , where locations attended to in the query and document are in the left and right column respectively .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",1
9,"8 . At the first step , the query attention focuses on the placeholder token , as its local context is generally important to discriminate the answer .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",1
9,"The model first focuses on @entity148 , which corresponds to "" Greek "" in this The approach to teaching @entity6 in @placeholder schools needs a makeover , she says : Visualization of the alternated attention mechanism for an article in CNN , treating about the decline of the Italian language in schools .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",1
9,The title of the plot is the query .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",1
9,Each row correspond to a timestep .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",1
9,"The target is @entity3 which corresponds to the word "" Italian "" .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",1
9,across document locations ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",1
9,"At t = 2 , the query attention moves towards "" schools "" and the model hesitates between "" Italian "" and "" European Union "" ( @entity28 , see step 3 ) , both of which may satisfy the query .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",1
9,"At step 3 , the most likely candidates are "" European Union "" and "" Rome "" ( @entity159 ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",1
9,"As the timesteps unfold , the model learns that "" needs "" maybe important to infer the correct entity , i.e. "" Italian "" .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"The query sits on the same attended location , while the document attention evolves to become more confident about the answer .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"We find that , across CBT and CNN examples , the query attention wanders near or focuses on the placeholder location , attempting to discriminate its identity using only local context .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"For these particular datasets , the majority of questions can be answered after attending only to the words directly neighbouring the placeholder .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"This aligns with the findings of concerning CNN , which state that the required reasoning and inference levels for this dataset are quite simple .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"It would be worthwhile to formulate a dataset in which the placeholder is harder to infer using only local neighboring words , and thereby necessitates deeper query exploration .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"Finally , across this work we fixed the number of inference steps T .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,We found that using 8 timesteps works well consistently across the tested datasets .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"However , we hypothesize that more ( fewer ) timesteps would benefit harder ( easier ) examples .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,A straight - forward extension of the model would be to dynamically select the number of inference steps conditioned on each example .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,Neural attention models have been applied recently to a smrgsbord of machine learning and natural language processing problems .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"These include , but are not limited to , handwriting recognition , digit classification , machine translation , question answering and caption generation .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"In general , attention models keep a memory of states that can be accessed at will by learned attention policies .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"In our case , the memory is represented by the set of document and query contextual encodings .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"Our model is closely related to , which were also applied to question answering .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"The pointer - style attention mechanism that we use to perform the final answer prediction has been proposed by , which in turn was based on the earlier Pointer Networks of .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"However , differently from our work , perform only one attention step and embed the query into a single vector representation , corresponding to the concatenation of the last state of the forward and backward GRU networks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"To our knowledge , embedding the query into a single vector representation is a choice that is shared by most machine reading comprehension models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"In our model , the repeated , tight integration between query attention and document attention allows the model to explore dynamically which parts of the query are most important to predict the answer , and then to focus on the parts of the document that are most salient to the currently - attended query components .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,A similar attempt in attending different components of the query maybe found in .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"In that model , the document is processed once for each query word .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"This can be computationally intractable for large documents , since it involves unrolling a bidirectional recurrent neural network over the entire document multiple times .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"In contrast , our model only estimates query and document encodings once and can learn how to attend different parts of those encodings in a fixed number of steps .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,The inference network is responsible for making sense of the current attention step with respect to what has been gathered before .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"In addition to achieving state - of the - art performance , this technique may also prove to be more scalable than alternative query attention models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"Finally , our iterative inference process shares similarities to the iterative hops in Memory Networks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"In that model , the query representation is updated iteratively from hop to hop , although its different components are not attended to separately .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"Moreover , we substitute the simple linear update with a GRU network .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,The gating mechanism of the GRU network made it possible to use multiple steps of attention and to propagate the learning signal effectively back through to the first timestep .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,We presented an iterative neural attention model and applied it to machine comprehension tasks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"Our architecture deploys a novel alternating attention mechanism , and tightly integrates successful ideas from past works in machine reading comprehension to obtain state - of - the - art results on three datasets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,The iterative alternating attention mechanism continually refines its view of the query and document while aggregating the information required to answer a query .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,Multiple future research directions maybe envisioned .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,We plan to dynamically select the optimal number of inference steps required for each example .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"Moreover , we suspect that shifting towards stochastic attention should permit us to learn more interesting search policies .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
9,"Finally , we believe that our model is fully general and maybe applied in a straightforward way to other tasks such as information retrieval .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/5/1606.02245v4-Stanza-out.txt,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document . Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document . Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .",0
3,The capacity of a neural network to absorb information is limited by its number of parameters .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"In practice , however , there are significant algorithmic and performance challenges .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,"In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,"We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,A trainable gating network determines a sparse combination of these experts to use for each example .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,INTRODUCTION AND RELATED WORK 1 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Exploiting scale in both training data and model size has been central to the success of deep learning .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"When datasets are sufficiently large , increasing the capacity ( number of parameters ) of neural networks can give much better prediction accuracy .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"This has been shown in domains such as text , images , and audio .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"For typical deep learning models , where the entire model is activated for every example , this leads to a roughly quadratic blow - up in training costs , as both the model size and the number of training examples increase .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Unfortunately , the advances in computing power and distributed computation fall short of meeting such demand .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Various forms of conditional computation have been proposed as away to increase model capacity without a proportional increase in computational costs .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"In these schemes , large parts of a network are active or inactive on a per-example basis .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"The gating decisions maybe binary or sparse and continuous , stochastic or deterministic .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Various forms of reinforcement learning and back - propagation are proposed for trarining the gating decisions .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"While these ideas are promising in theory , no work to date has yet demonstrated massive improvements in model capacity , training time , or model quality .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Modern computing devices , especially GPUs , are much faster at arithmetic than at branching .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Most of the works above recognize this and propose turning on / off large chunks of the network with each gating decision .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Large batch sizes are critical for performance , as they amortize the costs of parameter transfers and updates .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Conditional computation reduces the batch sizes for the conditionally active chunks of the network .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Network bandwidth can be a bottleneck .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,A cluster of GPUs may have computational power thousands of times greater than the aggregate inter - device network bandwidth .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"To be computationally efficient , the relative computational versus network demands of an algorithm must exceed this ratio .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Embedding layers , which can be seen as a form of conditional computation , are handicapped by this very problem .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Since the embeddings generally need to be sent across the network , the number of ( example , parameter ) interactions is limited by network bandwidth instead of computational capacity .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Depending on the scheme , loss terms maybe necessary to achieve the desired level of sparsity per-chunk and / or per example .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,use three such terms .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,These issues can affect both model quality and load - balancing .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Model capacity is most critical for very large data sets .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"The existing literature on conditional computation deals with relatively small image recognition data sets consisting of up to 600,000 images .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"It is hard to imagine that the labels of these images provide a sufficient signal to adequately train a model with millions , let alone billions of parameters .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"In this work , we for the first time address all of the above challenges and finally realize the promise of conditional computation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,We obtain greater than 1000x improvements in model capacity with only minor losses in computational efficiency and significantly advance the state - of - the - art results on public language modeling and translation data sets .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Our approach to conditional computation is to introduce anew type of general purpose neural network component : a Sparsely - Gated Mixture - of - Experts Layer ( MoE ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,"The MoE consists of a number of experts , each a simple feed - forward neural network , and a trainable gating network which selects a sparse combination of the experts to process each input ( see ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,All parts of the network are trained jointly by back - propagation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"While the introduced technique is generic , in this paper we focus on language modeling and machine translation tasks , which are known to benefit from very large models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"In particular , we apply a MoE convolutionally between stacked LSTM layers , as in .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"The MoE is called once for each position in the text , selecting a potentially different combination of experts at each position .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,The different experts tend to become highly specialized based on syntax and semantics ( see Appendix E ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"On both language modeling and machine translation benchmarks , we improve on best published results at a fraction of the computational cost .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Since its introduction more than two decades ago , the mixture - of - experts approach has been the subject of much research .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Different types of expert architectures hae been proposed such as SVMs , Gaussian Processes , Dirichlet Processes , and deep networks .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Other work has focused on different expert configurations such as a hierarchical structure , infinite numbers of experts , and adding experts sequentially .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,suggest an ensemble model in the format of mixture of experts for machine translation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,The gating network is trained on a pre-trained ensemble NMT model .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,The works above concern top - level mixtures of experts .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,The mixture of experts is the whole model .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,MoEs with their own gating networks as parts of a deep model .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"It is intuitive that the latter approach is more powerful , since complex problems may contain many sub-problems each requiring different experts .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"They also allude in their conclusion to the potential to introduce sparsity , turning MoEs into a vehicle for computational computation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Our work builds on this use of MoEs as a general purpose neural network component .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"While uses two stacked MoEs allowing for two sets of gating decisions , our convolutional application of the MoE allows for different gating decisions at each position in the text .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,We also realize sparse gating and demonstrate its use as a practical way to massively increase model capacity .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"The Mixture - of - Experts ( MoE ) layer consists of a set of n "" expert networks "" E 1 , , E n , and a "" gating network "" G whose output is a sparse n-dimensional vector .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,shows an overview of the MoE module .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"The experts are themselves neural networks , each with their own parameters .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Although in principle we only require that the experts accept the same sized inputs and produce the same - sized outputs , in our initial investigations in this paper , we restrict ourselves to the case where the models are feed - forward networks with identical architectures , but with separate parameters .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Let us denote by G ( x ) and E i ( x ) the output of the gating network and the output of the i - th expert network fora given input x .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,We save computation based on the sparsity of the output of G ( x ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Wherever G (x ) i = 0 , we need not compute E i ( x ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"In our experiments , we have up to thousands of experts , but only need to evaluate a handful of them for every example .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"If the number of experts is very large , we can reduce the branching factor by using a two - level hierarchical MoE. Ina hierarchical MoE , a primary gating network chooses a sparse weighted combination of "" experts "" , each of which is itself a secondary mixture - of - experts with its own gating network .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,In the following we focus on ordinary MoEs .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,We provide more details on hierarchical MoEs in Appendix B.,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Our implementation is related to other models of conditional computation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,A MoE whose experts are simple weight matrices is similar to the parameterized weight matrix proposed in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"A MoE whose experts have one hidden layer is similar to the block - wise dropout described in , where the dropped - out layer is sandwiched between fully - activated layers .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,A simple choice of non-sparse gating function is to multiply the input by a trainable weight matrix W g and then apply the Sof tmax function .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,We add two components to the Softmax gating network : sparsity and noise .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,( which causes the corresponding gate values to equal 0 ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"The sparsity serves to save computation , as described above .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"While this form of sparsity creates some theoretically scary discontinuities in the output of gating function , we have not yet observed this to be a problem in practice .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"The noise term helps with load balancing , as will be discussed in Appendix A .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,The amount of noise per component is controlled by a second trainable weight matrix W noise .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"We train the gating network by simple back - propagation , along with the rest of the model .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"If we choose k > 1 , the gate values for the top k experts have nonzero derivatives with respect to the weights of the gating network .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,This type of occasionally - sensitive behavior is described in with respect to noisy rectifiers .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Gradients also backpropagate through the gating network to its inputs .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Our method differs here from who use boolean gates and a REINFORCE - style approach to train the gating network .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"On modern CPUs and GPUs , large batch sizes are necessary for computational efficiency , so as to amortize the overhead of parameter loads and updates .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"If the gating network chooses k out of n experts for each example , then fora batch of b examples , each expert receives a much smaller batch of approximately kb n b examples .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,This causes a naive MoE implementation to become very inefficient as the number of experts increases .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,The solution to this shrinking batch problem is to make the original batch size as large as possible .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"However , batch size tends to be limited by the memory necessary to store activations between the forwards and backwards passes .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Ina conventional distributed training setting , multiple copies of the model on different devices asynchronously process distinct batches of data , and parameters are synchronized through a set of parameter servers .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"In our technique , these different batches run synchronously so that they can be combined for the MoE layer .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"We distribute the standard layers of the model and the gating network according to conventional data - parallel schemes , but keep only one shared copy of each expert .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Each expert in the MoE layer receives a combined batch consisting of the relevant examples from all of the data - parallel input batches .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,The same set of devices function as data - parallel replicas ( for the standard layers and the gating networks ) and as model - parallel shards ( each hosting a subset of the experts ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"If the model is distributed over d devices , and each device processes a batch of size b , each expert receives a batch of approximately kbd n examples .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Thus , we achieve a factor of d improvement inexpert batch size .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"In the case of a hierarchical MoE ( Section B ) , the primary gating network employs data parallelism , and the secondary MoEs employ model parallelism .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Each secondary MoE resides on one device .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,This technique allows us to increase the number of experts ( and hence the number of parameters ) by proportionally increasing the number of devices in the training cluster .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"The total batch size increases , keeping the batch size per expert constant .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"The memory and bandwidth requirements per device also remain constant , as do the step times , as does the amount of time necessary to process a number of training examples equal to the number of parameters in the model .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,It is our goal to train a trillionparameter model on a trillion - word corpus .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"We have not scaled our systems this far as of the writing of this paper , but it should be possible by adding more hardware .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"In our language models , we apply the same MoE to each time step of the previous layer .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"If we wait for the previous layer to finish , we can apply the MoE to all the time steps together as one big batch .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Doing so increases the size of the input batch to the MoE layer by a factor of the number of unrolled time steps .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,We suspect that even more powerful models may involve applying a MoE recurrently .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"For example , the weight matrices of a LSTM or other RNN could be replaced by a MoE. Sadly , such models break the convolutional trick from the last paragraph , since the input to the MoE atone timestep depends on the output of the MoE at the previous timestep .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Gruslys et al . ( 2016 ) describe a technique for drastically reducing the number of stored activations in an unrolled RNN , at the cost of recomputing forward activations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,This would allow fora large increase in batch size .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Another major performance concern in distributed computing is network bandwidth .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Since the experts are stationary ( see above ) and the number of gating parameters is small , most of the communication involves sending the inputs and outputs of the experts across the network .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"To maintain computational efficiency , the ratio of an expert 's computation to the size of its input and output must exceed the ratio of computational to network capacity of the computing device .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"For GPUs , this maybe thousands to one .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"In our experiments , we use experts with one hidden layer containing thousands of RELU - activated units .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Since the weight matrices in the expert have sizes input_sizehidden_size and hidden_size output_size , the ratio of computation to input and output is equal to the size of the hidden layer .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Conveniently , we can increase computational efficiency simply by using a larger hidden layer , or more hidden layers .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,We have observed that the gating network tends to converge to a state where it always produces large weights for the same few experts .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"This imbalance is self - reinforcing , as the favored experts are trained more rapidly and thus are selected even more by the gating network .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"describe the same phenomenon , and use a hard constraint at the beginning of training to avoid this local minimum .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,include a soft constraint on the batch - wise average of each gate .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,We take a soft constraint approach .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,We define the importance of an expert relative to a batch of training examples to be the batchwise sum of the gate values for that expert .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"We define an additional loss L importance , which is added to the overall loss function for the model .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"This loss is equal to the square of the coefficient of variation of the set of importance values , multiplied by a hand - tuned scaling factor w importance .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,This additional loss encourages all experts to have equal importance .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,2 . The number of parameters in the LSTM layers of these models vary from 2 million to 151 million .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Quality increases greatly with parameter count , as do computational costs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Results for these models form the top line of - right .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Our models consist of two stacked LSTM layers with a MoE layer between them ( see ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,We vary the sizes of the layers and the number of experts .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"For full details on model architecture , training regimen , additional baselines and results , see Appendix C .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,The results of these models are shown in - left .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"The model with 4 always - active experts performed ( unsurprisingly ) similarly to the computationally - matched baseline models , while the largest of the models ( 4096 experts ) achieved an impressive 24 % lower perplexity on the test set .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"In addition to the largest model from the previous section , we trained two more MoE models with similarly high capacity ( 4 billion parameters ) , but higher computation budgets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"These models had larger LSTMs , and fewer but larger and experts .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Details can be found in Appendix C.2 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Results of these three models form the bottom line of - right .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,compares the results of these models to the best previously - published result on this dataset .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Even the fastest of these models beats the best published result ( when controlling for the number of training epochs ) , despite requiring only 6 % of the computation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Efficiency : We trained our models using TensorFlow on clusters containing 16 - 32 Tesla K40 GPUs .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"For each of our models , we determine computational efficiency in TFLOPS / GPU by dividing the number of floating point operations required to process one training batch by the observed step time and the number of GPUs in the cluster .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"The operation counts used here are higher than the ones we report in our ops / timestep numbers in that we include the backwards pass , we include the importance - sampling - based training of the softmax layer , and we count a multiply - and - add as two separate operations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"For all of our MoE models , the floating point operations involved in the experts represent between 37 % and 46 % of the total .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"For our baseline models wtih no MoE , observed computational efficiency ranged from 1.07 - 1.29 TFLOPS / GPU .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"For our low-computation MoE models , computation efficiency ranged from 0.74 - 0.90 TFLOPS / GPU , except for the 4 - expert model which did not make full use of the available parallelism .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,"Our highest - computation MoE model was more efficient at 1.56 TFLOPS / GPU , likely due to the larger matrices .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,These numbers represent a significant fraction of the theoretical maximum of 4.29 TFLOPS / GPU claimed by NVIDIA .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Detailed results are in Appendix C , .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"On the 1 - billion - word corpus , adding additional capacity seems to produce diminishing returns as the number of parameters in the MoE layer exceeds 1 billion , as can be seen in - left .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"We hypothesized that fora larger training set , even higher capacities would produce significant quality improvements .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"We constructed a similar training set consisting of shuffled unique sentences from Google 's internal news corpus , totalling roughly 100 billion words .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Similarly to the previous section , we tested a series of models with similar computational costs of about 8 million ops / timestep .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"In addition to a baseline LSTM model , we trained models augmented with MoE layers containing 32 , experts .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,This corresponds to up to 137 billion parameters in the MoE layer .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Details on architecture , training , and results are given in Appendix D.",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,Results : shows test perplexity as a function of capacity after training on 10 billion words ( top line ) and 100 billion words ( bottom line ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,"When training over the full 100 billion words , test perplexity improves significantly up to 65536 experts ( 68 billion parameters ) , dropping 39 % lower than the computationally matched baseline , but degrades at 131072 experts , possibly a result of too much sparsity .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,The widening gap between the two lines demonstrates ( unsurprisingly ) that increased model capacity helps more on larger training sets .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,"Even at 65536 experts ( 99.994 % layer sparsity ) , computational efficiency for the model stays at a respectable 0.72 TFLOPS / GPU .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,Our model was a modified version of the GNMT model described in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"To reduce computation , we decreased the number of LSTM layers in the encoder and decoder from 9 and 8 to 3 and 2 respectively .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,We inserted MoE layers in both the encoder ( between layers 2 and 3 ) and the decoder ( between layers 1 and 2 ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Each MoE layer contained up to 2048 experts each with about two million parameters , adding a total of about 8 billion parameters to the models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Further details on model architecture , testing procedure and results can be found in Appendix E.",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"De corpora , whose training sets have 36M sentence pairs and 5 M sentence pairs , respectively .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"The experimental protocols were also similar to those in : newstest2014 was used as the test set to compare against previous work , while the combination of newstest2012 and newstest2013 was used as the development set .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Results : show the results of our largest models , compared with published results .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Our approach achieved BLEU scores of 40.56 and 26.03 on the WMT ' 14 En?Fr and En ? De benchmarks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"As our models did not use RL refinement , these results constitute significant gains of 1.34 and 1.12 BLEU score on top of the strong baselines in .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,The perplexity scores are also better .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"On the Google Production dataset , our model achieved 1.01 higher test BLEU score even after training for only one sixth of the time .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,"Results for the single - pair GNMT models , the multilingual GNMT model and the multilingual MoE model are given in .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,The MoE model achieves 19 % lower perplexity on the dev set than the multilingual GNMT model .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,"On BLEU score , the MoE model significantly beats the multilingual GNMT model on 11 of the 12 language pairs ( by as much as 5.84 points ) , and even beats the monolingual GNMT models on 8 of 12 language pairs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,"Korean seems to be a result of severe overtraining , as for the rarer language pairs a small number of real examples were highly oversampled in the training corpus .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,This work is the first to demonstrate major wins from conditional computation in deep networks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,We carefully identified the design considerations and challenges of conditional computing and addressed them with a combination of algorithmic and engineering solutions .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,"While we focused on text , conditional computation may help in other domains as well , provided sufficiently large training sets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,We look forward to seeing many novel implementations and applications of conditional computation in the years to come .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,"As discussed in section 4 , for load - balancing purposes , we want to define an additional loss function to encourage experts to receive roughly equal numbers of training examples .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Unfortunately , the number of examples received by an expert is a discrete quantity , so it can not be used in backpropagation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Instead , we define a smooth estimator Load ( X ) of the number of examples assigned to each expert fora batch X of inputs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,The smoothness allows us to back - propagate gradients through the estimator .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,This is the purpose of the noise term in the gating function .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"We define P ( x , i ) as the probability that G (x ) i is nonzero , given anew random choice of noise on element i , but keeping the already - sampled choices of noise on the other elements .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"To compute P ( x , i ) , we note that the G ( x ) i is nonzero if and only if H ( x ) i is greater than the k th - greatest element of H ( x ) excluding itself .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Where kth_excluding ( v , k , i ) means the kth highest component of v , excluding component i .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,is the CDF of the standard normal distribution .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"We can now define the load loss to be the square of the coefficient of variation of the load vector , multiplied by a hand - tuned scaling factor w load .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"To avoid out - of - memory errors , we need to initialize the network in a state of approximately equal expert load ( since the soft constraints need sometime to work ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"To accomplish this , we initialize the matrices W g and W noise to all zeros , which yields no signal and some noise .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"We trained a set of models with identical architecture ( the MoE - 256 model described in Appendix C ) , using different values of w importance and w load .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"We trained each model for 10 epochs , then measured perplexity on the test set .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"We also measured the coefficients of variation in Importance and Load , as well as ratio of the load on the most overloaded expert to the average load .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,This last value is significant for load balancing purposes on distributed hardware .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,All of these metrics were averaged over several training batches .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Results are reported in .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"All the combinations containing at least one the two losses led to very similar model quality , where having no loss was much worse .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Models with higher values of w load had lower loads on the most overloaded expert .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"If the number of experts is very large , we can reduce the branching factor by using a two - level hierarchical MoE. Ina hierarchical MoE , a primary gating network chooses a sparse weighted combination of "" experts "" , each of which is itself a secondary mixture - of - experts with its own gating network .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"If the hierarchical MoE consists of a groups of b experts each , we denote the primary gating network by G primary , the secondary gating networks by , and the expert networks by ( E 0 , 0 , E 0 , 1 ..E a , b ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Load primary and Load i deonte the Load functions for the primary gating network and i th secondary gating network respectively .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,0 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"It would seem simpler to let Load H ( X ) i , j = Load i ( X i ) j , but this would not have a gradient with respect to the primary gating network , so we use the formulation above .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Our model consists of five layers : a word embedding layer , a recurrent Long Short - Term Memory ( LSTM ) layer , a MoE layer , a second LSTM layer , and a softmax layer .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"The dimensionality of the embedding layer , the number of units in each LSTM layer , and the input and output dimensionality of the MoE layer are all equal to 512 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"For every layer other than the softmax , we apply drouput to the layer output , dropping each activation with probability DropP rob , otherwise dividing by ( 1 ? DropP rob ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"After dropout , the output of the previous layer is added to the layer output .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,This residual connection encourages gradient flow .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"For the hierarchical MoE layers , the first level branching factor was 16 , corresponding to the number of GPUs in our cluster .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,We use Noisy - Top - K Gating ( see Section 2.1 ) with k = 4 for the ordinary MoE layers and k = 2 at each level of the hierarchical MoE layers .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Thus , each example is processed by exactly 4 experts fora total of 4M ops / timestep .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,The two LSTM layers contribute 2M ops / timestep each for the desired total of 8 M .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"The MoE - 4 model does not employ sparsity , since all 4 experts are always used .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"The MoE layer consists of a single "" expert "" containing one ReLU - activated hidden layer of size 4096 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"The MoE layer consists of a single "" expert "" containing four ReLU - activated hidden layers , each with size 1024 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,4xLSTM - 512 : We replace the MoE layer with two additional 512 - unit LSTM layers .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,The model contains one 2048 - unit LSTM layer ( and no MoE ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,The output of the LSTM is projected down to 512 dimensions .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,The next timestep of the LSTM receives the projected output .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,This is identical to one of the models published in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"We re-ran it to account for differences in training regimen , and obtained results very similar to the published ones .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,The models were trained on a cluster of 16 K40 GPUs using the synchronous method described in Section 3 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Each batch consisted of a set of sentences totaling roughly 300,000 words .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"In the interest of time , we limited training to 10 epochs , ( 27,000 steps ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Training took 12 - 16 hours for all models , except for MoE - 4 , which took 18 hours ( since all the expert computation was performed on only 4 of 16 GPUs ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,We used the Adam optimizer .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"The base learning rate was increased linearly for the first 1000 training steps , and decreased after that so as to be proportional to the inverse square root of the step number .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,The Softmax output layer was trained efficiently using importance sampling similarly to the models in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"For each model , we performed a hyper - parmeter search to find the best dropout probability , in increments of 0.1 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"To ensure balanced expert utilization we set w importance = 0.1 and w load = 0.1 , as described in Section 4 and Appendix A.",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"We evaluate our model using perplexity on the holdout dataset , used by .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,We follow the standard procedure and sum overall the words including the end of sentence symbol .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Results are reported in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"For each model , we report the test perplexity , the computational budget , the parameter counts , the value of DropP rob , and the computational efficiency .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,We implement several memory optimizations in order to fit up to 1 billion parameters per GPU .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"First , we do not store the activations of the hidden layers of the experts , but instead recompute them on the backwards pass .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,The Adam optimizer keeps first and second moment estimates of the perparameter gradients .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,This triples the required memory .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"1 = 0 . To reduce the size of the second moment estimator , we replace it with a factored approximation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"For a matrix of parameters , instead of maintaining a full matrix of second - moment estimators , we maintain vectors of row - wise and column - wise averages of that matrix .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"At each step , the matrix of estimators is taken to be the outer product of those two vectors divided by the mean of either one .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,This technique could similarly be applied to Adagrad .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,We evaluate our model using perplexity on a holdout dataset .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Results are reported in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Perplexity after 100 billion training words is 39 % lower for the 68 - billion - parameter MoE model than for the baseline model .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,It is notable that the measured computational efficiency of the largest model ( 0.30 TFLOPS / GPU ) is very low compared to the other models .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"This is likely a result of the fact that , for purposes of comparison to the other models , we did not increase the training batch size proportionally to the number of GPUs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"For comparison , we include results fora computationally matched baseline model consisting of 4 LSTMs , and for an unpruned 5 - gram model with Kneser - Ney smoothing .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Our model is a modified version of the GNMT model described in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"To reduce computation , we decrease the number of LSTM layers in the encoder and decoder from 9 and 8 to 3 and 2 respectively .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,We insert MoE layers in both the encoder ( between layers 2 and 3 ) and the decoder ( between layers 1 and 2 ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"We use an attention mechanism between the encoder and decoder , with the first decoder LSTM receiving output from and providing input for the attention 5 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,All of the layers in our model have input and output dimensionality of 512 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"Our LSTM layers have 2048 hidden units , with a 512 - dimensional output projection .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,We add residual connections around all LSTM and MoE layers to encourage gradient flow .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"( Schuster & Nakajima , 2012 ) for inputs and outputs in our system .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,We use a shared source and target vocabulary of 32 K wordpieces .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"We used noisy - top - k gating as described in Section 2.1 , not the scheme from Appendix F. The MoE layers in the encoder and decoder are non-hierarchical MoEs with n = 512 experts , and k = 2 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Each expert has a larger hidden layer of size 8192 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"This doubles the amount of computation in the MoE layers , raising the computational budget of the entire model from 85 M to 102M ops / timestep .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,We trained our networks using the Adam optimizer .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"The base learning rate was increased linearly for the first 2000 training steps , held constant for an additional 8000 steps , and decreased after that so as to be proportional to the inverse square root of the step number .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,"For the single - language - pair models , similarly to , we applied dropout to the output of all embedding , LSTM and MoE layers , using DropP rob = 0.4 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,Training was done synchronously on a cluster of up to 64 GPUs as described in section 3 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,Each training batch consisted of a set of sentence pairs containing roughly 16000 words per GPU .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,"To ensure balanced expert utilization we set w importance = 0.01 and w load = 0.01 , as described in Section 4 and Appendix A.",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,Metrics : We evaluated our models using the perplexity and the standard BLEU score metric .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,"We reported tokenized BLEU score as computed by the multi -bleu.pl script , downloaded from the public implementation of Moses ( on Github ) , which was also used in .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,Results : and 4 in Section 5.3 show comparisons of our results to other published methods .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,shows test perplexity as a function of number of words in the ( training data 's ) source sentences processed for models with different numbers of experts .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,"As can be seen from the as we increased the number of experts to approach 2048 , the test perplexity of our model continued to improve .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,"We found that the experts indeed become highly specialized by syntax and / or semantics , as can be seen in .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,"For example , one expert is used when the indefinite article "" a "" introduces the direct object in a verb phrase indicating importance or leadership .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,"Due to some peculiarities in our infrastructure which have since been fixed , at the time we ran some of the machine translation experiments , our models ran faster if every expert received exactly the same batch size .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,"To accommodate this , we used a different gating function which we describe below .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",1
3,( x ) component - wise with a sparse mask M ( G ? ( x ) ) and normalize the output .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"As our experiments suggest and also observed in , using a batchwise function during training ( such as M batchwise ) requires modifications to the inference when we may not have a large batch of examples .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Our solution to this is to train a vector T of per-expert threshold values to approximate the effects of the batchwise mask .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"To learn the threshold values , we apply an additional loss at training time which is minimized when the batchwise mask and the threshold mask are identical .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"The attention mechanism described in GNMT involves a learned "" Attention Function "" A ( x i , y j ) which takes a "" source vector "" x i and a "" target vector "" y j , and must be computed for every source time step i and target time step j .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"In GNMT , the attention function is implemented as a feed forward neural network with a hidden layer of size n.",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,Where U and Ware trainable weight matrices and V is a trainable weight vector .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,"With our attention function , we can simultaneously compute the attention function on multiple source time steps and multiple target time steps using optimized matrix multiplications .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
3,We found little difference in quality between the two functions .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/7/1701.06538v1-Stanza-out.txt,"The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",0
1,Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,"However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,"In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,"On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We can still achieve BLEU = 36.3 even without using an attention mechanism .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,Our models are also validated on the more difficult WMT ' 14 English - to - German task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,Neural machine translation ( NMT ) has attracted a lot of interest in solving the machine translation ( MT ) problem in recent years .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,"Unlike conventional statistical machine translation ( SMT ) systems which consist of multiple separately tuned components , NMT models encode the source sequence into continuous representation space and generate the target sequence in an end - to - end fashon .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,"Moreover , NMT models can also be easily adapted to other tasks such as dialog systems , question answering systems and image caption generation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,"In general , there are two types of NMT topologies : the encoder - decoder network and the attention network .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,The encoder - decoder network represents the source sequence with a fixed dimensional vector and the target sequence is generated from this vector word byword .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,The attention network uses the representations from all time steps of the input sequence to build a detailed relationship between the target words and the input words .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,Recent results show that the systems based on these models can achieve similar performance to conventional SMT systems .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"However , a single neural model of either of the above types has not been competitive with the best conventional system when evaluated on the WMT ' 14 English - to - French task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,The best BLEU score from a single model with six layers is only 31.5 while the conventional method of achieves 37.0 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We focus on improving the single model perfor - mance by increasing the model depth .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,Deep topology has been proven to outperform the shallow architecture in computer vision .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,In the past two years the top positions of the ImageNet contest have always been occupied by systems with tensor even hundreds of layers .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"But in NMT , the biggest depth used successfully is only six .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,We attribute this problem to the properties of the Long Short - Term Memory ( LSTM ) which is widely used in NMT .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,"In the LSTM , there are more non-linear activations than in convolution layers .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,"These activations significantly decrease the magnitude of the gradient in the deep topology , especially when the gradient propagates in recurrent form .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,"There are also many efforts to increase the depth of the LSTM such as the work by , where the shortcuts do not avoid the nonlinear and recurrent computation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,"In this work , we introduce anew type of linear connections for multi - layer recurrent networks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,"These connections , which are called fast - forward connections , play an essential role in building a deep topology with depth of 16 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"In addition , we introduce an interleaved bi-directional architecture to stack LSTM layers in the encoder .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,This topology can be used for both the encoder - decoder network and the attention network .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"On the WMT ' 14 Englishto - French task , this is the deepest NMT topology that has ever been investigated .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,"With our deep attention model , the BLEU score can be improved to 37.7 outperforming the shallow model which has six layers by 6.2 BLEU points .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,This is also the first time on this task that a single NMT model achieves state - of - the - art performance and outperforms the best conventional SMT system with an improvement of 0.7 .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,"Even without using the attention mechanism , we can still achieve 36.3 with a single model .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,"After model ensembling and unknown word processing , the BLEU score can be further improved to 40.4 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,"When evaluated on the subset of the test corpus without unknown words , our model achieves 41.4 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,"As a reference , previous work showed that oracle rescoring of the 1000 - best sequences generated by the SMT model can achieve the BLEU score of about 45 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,Our models are also validated on the more difficult WMT ' 14 English - to - German task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,"Neural machine translation aims at generating the target word sequence y = {y 1 , . . . , y n } given the source word sequence x = {x 1 , . . . , x m } with neural models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,where y 0:j?1 is the sub sequence from y 0 toy j?1 . y 0 and y m + 1 denote the start mark and end mark of target sequence respectively .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"The process can be explicitly split into an encoding part , a decoding part and the interface between these two parts .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"In the encoding part , the source sequence is processed and transformed into a group of vectors e = {e 1 , , em } for each time step .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,Further operations will be used at the interface part to extract the final representation c of the source sequence from e.,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"At the decoding step , the target sequence is generated from the representation c.",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"Recently , there have been two types of NMT models which are different in the interface part .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"In the encoder - decoder model , a single vector extracted from e is used as the representation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"In the attention model , c is dynamically obtained according to the relationship between the target sequence and the source sequence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"The recurrent neural network ( RNN ) , or its specific form the LSTM , is generally used as the basic unit of the encoding and decoding part .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"However , the topology of most of the existing models is shallow .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"In the attention network , the encoding part and the decoding part have only one LSTM layer respectively .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"In the encoder - decoder network , researchers have used at most six LSTM layers .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"Because machine translation is a difficult problem , we believe more complex encoding and decoding architecture is needed for modeling the relationship between the source sequence and the target sequence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"In this work , we focus on enhancing the complexity of the encoding / decoding architecture by increasing the model depth .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,Deep neural models have been studied in a wide range of problems .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"In computer vision , models with more than ten convolution layers outperform shallow ones on a series of image tasks in recent years .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,Different kinds of shortcut connections are proposed to decrease the length of the gradient propagation path .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"Training networks based on LSTM layers , which are widely used in language problems , is a much more challenging task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"Because of the existence of many more nonlinear activations and the recurrent computation , gradient values are not stable and are generally smaller .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"Following the same spirit for convolutional networks , a lot of effort has also been spent on training deep LSTM networks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"introduced depth - gated shortcuts , connecting LSTM cells at adjacent layers , to provide a fast way to propagate the gradients .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,They validated the modification of these shortcuts on an MT task and a language modeling task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"However , the best score was obtained using models with three layers .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"Similarly , proposed a two dimensional structure for the LSTM .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,Their structure decreases the number of nonlinear activations and path length .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"However , the gradient propagation still relies on the recurrent computation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"The investigations were also made on question - answering to encode the questions , whereat most two LSTM layers were stacked .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"Based on the above considerations , we propose new connections to facilitate gradient propagation in the following section .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We build the deep LSTM network with the new proposed linear connections .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,The shortest paths through the proposed connections do not include any nonlinear transformations and do not rely on any recurrent computation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We call these connections fastforward connections .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"Within the deep topology , we also introduce an interleaved bi-directional architecture to stack the LSTM layers .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,Our entire deep neural network is shown in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"This topology can be divided into three parts : the encoder part ( P -E ) on the left , the decoder part ( P - D ) on the right and the interface between these two parts ( P - I ) which extracts the representation of the source sequence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"We have two instantiations of this topology : Deep - ED and Deep - Att , which correspond to the extension of the encoder - decoder network and the attention network respectively .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,Our main innovation is the novel scheme for connecting adjacent recurrent layers .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We will start with the basic RNN model for the sake of clarity .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,where the bias parameter is not included for simplicity .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We use a red circle and a blue empty square to denote an input and a hidden state .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"A blue square with a "" - "" denotes the previous hidden state .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,A dotted line means that the hidden state is used recurrently .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"Feed-Forward computation : ft = W f x t . Left part in ) . "" f "" block .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"Recurrent computation : RNN ( f t , h t?1 ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,Right part and the sum operation ( + ) followed by activation in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,""" r "" block .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,1 ( denoted by h k?1 ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"In our work , we add fast - forward connections ( F - F connections ) which connect two feed - forward computation blocks "" f "" of adjacent recurrent layers .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"It means that each block "" f "" at recurrent layer k takes both the outputs of block "" f "" and block "" r "" at its previous layer as input ( ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,F - F connections are denoted by dashed red lines in and .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,The path of F - F connections contains neither nonlinear activations nor recurrent computation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"It provides a fast path for information to propagate , so we call this path fast - forward connections .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"Additionally , in order to learn more temporal dependencies , the sequences can be processed in different directions at each pair of adjacent recurrent layers .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,The opposite directions are marked by the direction term ( ? 1 ) k .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"At the first recurrent layer , the block "" f "" takes x t as the input .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"[ , ] denotes the concatenation of vectors .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,This is shown in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We add a connection between f kt and f k ?1 t .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"Without f k ?1 t , our model will be reduced to the traditional stacked model .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We alternate the RNN direction at different layers k with the direction term ( ? 1 ) k .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"If we fix the direction term to ? 1 , all layers work in the forward direction .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"In our experiments , instead of an RNN , a specific type of recurrent layer called LSTM ) is used .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,The LSTM is structurally more complex than the basic RNN in Eq .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"2 . We define the computation of the LSTM as a function which maps the input f and its state - output pair ( h , s ) at the previous time step to the current stateoutput pair .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,are the parameters of the LSTM .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,It is slightly different from the standard notation in that we do not have a matrix to multiply with the input fin our notation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,where x t is the input to the deep bi-directional LSTM model .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"For the encoder , x t is the embedding of the t th word in the source sentence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,For the decoder x t is the concatenation of the embedding of the t th word in the target sentence and the encoder representation for step t.,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"In our final model two additional operations are used with Eq. 5 , which is shown in Eq .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"6 . Half ( f ) denotes the first half of the elements off , and Dr ( h ) is the dropout operation which randomly sets an element of h to zero with a certain probability .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,The use of Half ( ) is to reduce the parameter size and does not affect the performance .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"We observed noticeable performance degradation when using only the first third of the elements of "" f "" .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"With the F - F connections , we build a fast channel to propagate the gradients in the deep topology .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,F - F connections can accelerate the model convergence and while improving the performance .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,A similar idea was also used in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,Encoder : The LSTM layers are stacked following Eq. 5 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We call this type of encoder interleaved bidirectional encoder .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"In addition , there are two similar columns ( a 1 and a 2 ) in the encoder part .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,Each column consists of n e stacked LSTM layers .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,There is no connection between the two columns .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,The first layers of the two columns process the word representations of the source sequence in different directions .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"At the last LSTM layers , there are two groups of vectors representing the source sequence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,The group size is the same as the length of the input sequence .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,Interface : Prior encoder - decoder models and attention models are different in their method of extracting the representations of the source sequences .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"In our work , as a consequence of the introduced F - F connections , we have 4 output vectors ( h For Deep - Att , we do not need the above two operations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"We only concatenate the 4 output vectors at each time step to obtain e t , and a soft attention mechanism is used to calculate the final representation ct from e t .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,Note that the vector dimensionality off is four times larger than that of h ( see Eq. 4 ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"the concatenated vector e t to a vector with 1 / 4 dimension size , denoted by the ( fully connected ) block "" fc "" in .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,The decoder follows Eq. 5 and Eq. 6 with fixed direction term ? 1 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,y t?1 is the target word embedding at the previous time step and y 0 is zero .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,There is a single column of n d stacked LSTM layers .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We also use the F - F connections like those in the encoder and all layers are in the forward direction .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"Note that at the last LSTM layer , we only use ht to make the prediction with a softmax layer .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"Although the network is deep , the training technique is straightforward .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We will describe this in the next part .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We take the parallel data as the only input without using any monolingual data for either word representation pre-training or language modeling .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"Because of the deep bi-directional structure , we do not need to reverse the sequence order as .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"The deep topology brings difficulties for the model training , especially when first order methods such as stochastic gradient descent ( SGD ) are used .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,The parameters should be properly initialized and the converging process can be slow .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"We tried several optimization techniques such as AdaDelta ( Zeiler , 2012 ) , RMSProp ( Tieleman and and .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We found that all of them were able to speedup the process a lot compared to simple SGD while no significant performance difference was observed among them .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"In this work , we chose Adam for model training and do not present a detailed comparison with other optimization methods .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,Dropout is also used to avoid over-fitting .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,It is utilized on the LSTM nodes h kt ( See Eq. 5 ) with a ratio of pd for both the encoder and decoder .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"During the whole model training process , we keep all hyper parameters fixed without any intermediate interruption .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,The hyper parameters are selected according to the performance on the development set .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"For such a deep and large network , it is not easy to determine the tuning strategy and this will be considered in future work .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We use the common left - to - right beam - search method for sequence generation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"1 . We keep n b best candidates according to Eq. 11 at each time step , until the end of sentence mark is generated .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"The hypotheses are ranked by the total likelihood of the generated sequence , although normalized likelihood is used in some works .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We evaluate our method mainly on the widely used WMT ' 14 English - to - French translation task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"In order to validate our model on more difficult language pairs , we also provide results on the WMT ' 14 English - to - German translation task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,Our models are implemented in the PADDLE ( PArallel Distributed Deep LEarning ) platform .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"For both tasks , we use the full WMT ' 14 parallel corpus as our training data .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"In total , the English - to - French corpus includes 36 million sentence pairs , and the English - to - German corpus includes 4.5 million sentence pairs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"The news - test - 2012 and news - test - 2013 are concatenated as our development set , and the news - test - 2014 is the test set .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,Our data partition is consistent with previous works on NMT to ensure fair comparison .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"For the source language , we select the most frequent 200K words as the input vocabulary .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,For the target language we select the most frequent 80 K French words and the most frequent 160K German words as the output vocabulary .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"The full vocabulary of the German corpus is larger , so we select more German words to build the target vocabulary .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,Out - of - vocabulary words are replaced with the unknown symbol unk .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"For complete comparison to previous work on the Englishto - French task , we also show the results with a smaller vocabulary of 30K input words and 30 K output words on the sub train set with selected 12M parallel sequences .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"We have two models as described above , named Deep - ED and Deep - Att.",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,Both models have exactly the same configuration and layer size except the interface part P - I.,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We use 256 dimensional word embeddings for both the source and target languages .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"All LSTM layers , including the 2n e layers in the encoder and then d layers in the decoder , have 512 memory cells .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,The output layer size is the same as the size of the target vocabulary .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,The dimension of ct is 5120 and 1280 for Deep - ED and Deep - Att respectively .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"For each LSTM layer , the activation functions for gates , inputs and outputs are sigmoid , tanh , and tanh respectively .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,Our network is narrow on word embeddings and LSTM layers .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"Note that in previous work , 1000 dimensional word embeddings and 1000 dimensional LSTM layers are used .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We also tried larger scale models but did not obtain further improvements .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"Note that each LSTM layer includes two parts as described in Eq. 3 , feed - forward computation and recurrent computation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"Since there are non-linear activations in the recurrent computation , a larger learning rate l r = 5 10 ? 4 is used , while for the feed - forward computation a smaller learning rate l f = 4 10 ? 5 is used .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,Word embeddings and the softmax layer also use this learning rate l f .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We refer all the parameters not used for recurrent computation as non-recurrent part of the model .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"Here r is the regularization strength , l is the corresponding learning rate , g stands for the gradients of v.",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,The two embedding layers are not regularized .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,All the other layers have the same r = 2 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,The parameters of the recurrent computation part are initialized to zero .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,All non-recurrent parts are randomly initialized with zero mean and standard deviation of 0.07 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,A detailed guide for setting hyperparameters can be found in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,The dropout ratio pd is 0.1 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"In each batch , there are 500 ? 800 sequences in our work .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,The exact number depends on the sequence lengths and model size .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We also find that larger batch size results in better convergence although the improvement is not large .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"However , the largest batch size is constrained by the GPU memory .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We use 4 ? 8 GPU machines ( each has 4 K40 GPU cards ) running for 10 days to train the full model with parallelization at the data batch level .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,It takes nearly 1.5 days for each pass .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,One thing we want to emphasize here is that our deep model is not sensitive to these settings .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,Small variation does not affect the final performance .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We evaluate the same way as previous NMT works .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,All reported BLEU scores are computed with the multi-bleu. perl 1 script which is also used in the above works .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,The results are for tokenized and case sensitive evaluation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,English - to - French : First we list our single model results on the English - to - French task in Tab .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,1 . In the first block we show the results with the full corpus .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,The previous best single NMT encoderdecoder model ( Enc - Dec ) with six layers achieves BLEU = 31.5 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"From Deep - ED , we obtain the BLEU score of 36.3 , which outperforms Enc - Dec model by 4.8 BLEU points .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"This result is even better than the ensemble result of eight Enc - Dec models , which is 35.6 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"This shows that , in addition to the convolutional layers for computer vision , deep topologies can also work for LSTM layers .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"For Deep - Att , the performance is further improved to 37.7 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We also list the previous state - of - the - art performance from a conventional SMT system with the BLEU of 37.0 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,This is the first time that a single NMT model trained in an end - to - end form beats the best conventional system on this task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We also show the results on the smaller data set with 12M sentence pairs and 30 K vocabulary in the second block .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"The two attention models , RNNsearch and RNNsearch - LV , achieve BLEU scores of 28.5 and 32.7 respectively .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,Note that RNNsearch - LV uses a large output vocabulary of 500K words based on the standard attention model RNNsearch .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We obtain BLEU = 35.9 which outperforms its corresponding shallow model RNNsearch by 7.4 BLEU points .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,The SMT result from is also listed and falls behind our model by 2.6 BLEU points .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,Data Voc BLEU RNNsearch 4.5M 50K 16.5 RNNsearch-LV 4.5M 500K 16.9 SMT 4.5 M Full 20.7 Deep - Att ( Ours ) 4.5M 160K 20.6 : English - to - German task : BLEU scores of single neural models .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We also list the conventional SMT system for comparison .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,Two post processing techniques are used to improve the performance further on the English - to - French task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"First , three Deep - Att models are built for ensemble results .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"They are initialized with different random parameters ; in addition , the training corpus for these models is shuffled with different random seeds .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We sum over the predicted probabilities of the target words and normalize the final distribution to generate the next word .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,It is shown in Tab .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,8 that the model ensemble can improve the performance further to 38.9 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"In and there are eight models for the best scores , but we only use three models and we do not obtain further gain from more models . : BLEU scores of different models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,The first two blocks are our results of two single models and models with post processing .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,In the last block we list two baselines of the best conventional SMT system and NMT system .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,"Second , we recover the unknown words in the generated sequences with the Positional Unknown ( Pos Unk ) model introduced in .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,The full parallel corpus is used to obtain the word mappings .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"We find this method provides an additional 1.5 BLEU points , which is consistent with the conclusion in .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We obtain the new BLEU score of 39.2 with a single Deep - Att model .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"For the ensemble models of Deep - Att , the BLEU score rises to 40.4 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"In the last two lines , we list the conventional SMT model and the previous best neural models based system Enc - Dec for comparison .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We find our best score outperforms the previous best score by nearly 3 points .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"On the English - to - French task , we analyze the effect of the source sentence length on our models as shown in .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"Here we show five curves : our Deep - Att single model , our Deep - Att ensemble model , our Deep - ED model , a previously proposed Enc - Dec model with four layers and an SMT model .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We find our Deep - Att model works better than the previous two models ( Enc - Dec and SMT ) on nearly all sentence lengths .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"It is also shown that for very long sequences with length over 70 words , the performance of our Deep - Att does not degrade , when compared to another NMT model Enc - Dec.",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"Our Deep - ED also has much better performance than the shallow Enc - Dec model on nearly all lengths , although for long sequences it degrades and starts to fall behind Deep - Att .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,Next we look into the detail of the effect of unknown words on the English - to - French task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We select the subset without unknown words on target sentences from the original test set .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,There are 1705 such sentences ( 56.8 % ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We compute the BLEU scores on this subset and the results are shown in Tab .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,9 . We also list the results from SMT model the score 37.7 on the full test set .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"On this subset , the SMT model achieves 37.5 , which is similar to its score 37.0 on the full test set .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,This suggests that the difficulty on this subset is not much different from that on the full set .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We therefore attribute the larger gap for Deep - att to the existence of unknown words .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We also compute the BLEU score on the subset of the ensemble model and obtain 41.4 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"As a reference related to human performance , in , it has been tested that the BLEU score of oracle re-scoring the LIUM 1000 - best results is 45 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"Deep models have more parameters , and thus have a stronger ability to fit the large data set .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"However , our experimental results suggest that deep models are less prone to the problem of over-fitting .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,"In , we show three results from models with a different depth on the English - to - French task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,"These three models are evaluated by token error rate , which is defined as the ratio of incorrectly predicted words in the whole target sequence with correct historical input .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,The curve with square marks corresponds to Deep - Att with n e = 9 and n d = 7 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,The curve with circle marks corresponds ton e = 5 and n d = 3 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,The curve with triangle marks corresponds ton e = 1 and n d = 1 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,We find that the deep model has better performance on the test set when the token error rate is the same as that of the shallow models on the training set .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"This shows that , with decreased token error rate , the deep model is more advantageous in avoiding the over - fitting phenomenon .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"We only plot the early training stage curves because , during the late training stage , the curves are not smooth .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"With the introduction of fast - forward connections to the deep LSTM network , we build a fast path with neither non-linear transformations nor recurrent computation to propagate the gradients from the top to the deep bottom .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,"On this path , gradients decay much slower compared to the standard deep network .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,This enables us to build the deep topology of NMT models .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,We trained NMT models with depth of 16 including 25 LSTM layers and evaluated them mainly on the WMT ' 14 English - to - French translation task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,This is the deepest topology that has been investigated in the NMT area on this task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,"We showed that our Deep - Att exhibits 6.2 BLEU points improvement over the previous best single model , achieving a 37.7 BLEU score .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,This single end - toend NMT model outperforms the best conventional SMT system and achieves a state - of - the - art performance .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,"After utilizing unknown word processing and model ensemble of three models , we obtained a BLEU score of 40.4 , an improvement of 2.9 BLEU points over the previous best result .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,"When evaluated on the subset of the test corpus without unknown words , our model achieves 41.4 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
1,Our model is also validated on the more difficult English - to - German task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
1,Our model is also efficient in sequence generation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
0,The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",1
0,"For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",1
0,The submitted systems were trained using Transformer models .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",1
0,Neural machine translation ( NMT ) is a rapidly changing research area .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",1
0,"Since 2016 when NMT systems first showed to achieve significantly better results than statistical machine translation ( SMT ) systems , the dominant neural network ( NN ) architectures for NMT have changed on a yearly ( and even more frequent ) basis .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",1
0,The state - of - the - art in 2016 were shallow attention - based recurrent neural networks ( RNN ) with gated recurrent units ( GRU ) in recurrent layers .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",1
0,"In 2017 , multiplicative long short - term memory ( MLSTM ) units and deep GRU models were introduced in NMT .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",1
0,"The same year , selfattentional ( Transformer ) models were introduced .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,1 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"However , it is already evident that the state - of - the - art architectures will 1 All 14 of the best automatically scored systems according to the information provided by participants in the official submission portal http://matrix.statmt.org were indicated as being based on Transformer models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,be pushed even further in 2018 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"For instance , have recently proposed RNMT + models that combine deep LSTM - based models with multi-head attention and showed that the models outperform Transformer models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"In WMT 2017 , Tilde participated with MLSTM - based NMT systems .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"In this paper , we compare the MLSTMbased models with Transformer models for English - Estonian and Estonian - English and we show that the state - of - the - art of WMT 2017 is well behind the new models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"Therefore , for WMT 2018 , Tilde submitted NMT systems that were trained using Transformer models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"Section 2 provides an overview of systems submitted for the WMT 2018 shared task on news translation , Section 3 describes the data used to train the NMT systems and the data pre-processing workflows , Section 4 describes all NMT systems trained and experiments on handling of named entities and combination of systems , Section 5 provides automatic evaluation results , and Section 6 concludes the paper .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"For the WMT 2018 shared task on news translation , Tilde submitted both constrained and unconstrained NMT systems ( 7 in total ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,Constrained English - Estonian and Estonian - English NMT systems ( tilde - c-nmt ) that were deployed as ensembles of averaged factored data ( see Section 3 ) Transformer models .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,The models were trained using parallel data and back - translated data in a 1 - to - 1 proportion .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,Unconstrained English - Estonian and Estonian - English NMT systems ( tilde - nc - nmt ) that were deployed as averaged Transformer models .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"These models were also trained using back - translated data similarly to the constrained systems , however , the data , taking into account their relatively large size , were not factored .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,A constrained Estonian - English NMT system ( tilde - c - nmt - comb ) that is a system combination of six factored data NMT systems .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,Constrained English - Estonian and Estonian - English NMT systems ( tilde - c - nmt - 2 bt ) averaged from multiple best NMT models .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,The models were trained using two sets of back - translated data in a 1 - to - 1 proportion to the clean parallel data - one set was backtranslated using a system trained on parallelonly data and the other set -using an NMT system trained on parallel data and the first set of back - translated data .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"Data preparation was done using one of two distinct workflows - we used the full workflow for tilde - c - nmt , tilde - nc - nmt and tilde - c - nmt - comb submissions .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,For the tilde - c - nmt - 2 bt submission we used the light data preparation workflow .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"First , we trained constrained system baseline models using the filtered datasets .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"For baseline models , we used the MLSTM and transf configurations ( see ) .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"Then , we used the best - performing models ( based on translation quality on the vali -dation set ) , which were the Transformer models ( see , and back - translated monolingual data .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"As mentioned before , for the unconstrained systems , we back - translated the monolingual data using pre-existing MLSTM - based NMT systems .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"Then , using the final training data ( parallel and the two synthetic corpora ) , we trained final Transformer models .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"For the constrained scenario , we trained multiple models ( three for each translation direction ) by experimenting with multiple model configurations .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"For the unconstrained scenario , we trained one model in each of the directions .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"For the tilde - c - nmt ( constrained NMT ) systems , we performed model averaging of the best four models ( according to perplexity ) of the three different run NMT systems and deployed the averaged models in an ensemble .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"For the tilde - nc - nmt ( unconstrained NMT ) systems , we performed model averaging of the best four models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"For the tilde - c - nmt - comb Estonian - English system , we performed majority voting ( see Section 4.3 ) of translations produced by six different runs of different constrained systems ( using best BLEU models , averaged models , ensembled averaged models , ensembled models , and larger beam search ( 10 instead of 5 ) ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"As NMT systems are sensitive to noise in parallel data , all parallel data were filtered using the parallel data filtering methods described by .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"The parallel corpora filtering methods remove sentence pairs that have indications of data corruption or low parallelity ( e.g. , source - target length ratio , content overlap , digit mismatch , language adherence , etc. ) issues .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"Contrary to Tilde 's submissions for WMT 2017 , isolated sentence pair filtering for the WMT 2018 submissions was supplemented with a maximum content overlap filter ( i.e. only one target sentence for each source sentence was preserved and vice versa based on the content overlap filter 's score for each sentence pair ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"For filtering , we required probabilistic dictionaries , which were obtained from the parallel corpora ( different dictionaries for the constrained and unconstrained scenarios ) using fast align .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,The dictionaries were filtered using the transliteration - based probabilistic dictionary filtering method by .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"During filtering , we identified that one of the corpora that were provided by the organisers contained a significant amount of data corruption .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,3 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,The corpus consisted of 1.30 million sentence pairs out of which 0.77 million were identified as being corrupt .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"To reduce the high level of noise , this corpus was filtered using stricter content overlap ( a threshold of 0.3 instead of 0.1 ) and language adherence filters ( both the language detection and the valid alphabet filters had to validate a sentence pair instead of just one of the filters ) than all other corpora .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"As a result , only 0.17 million sentence pairs from the ParaCrawl corpus were used for training of the constrained systems .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"Due to the quality concerns , the corpus was not used for training of the unconstrained systems .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,The corpora statistics before and after filtering are provided in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"First , parallel corpora are cleaned by removing HTML and XML tags , decoding escaped symbols , normalising whitespaces and punctuation marks , replacing control characters with spaces , etc .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,This step is performed only on the training data .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"Then , non-translatable entities , such as email addresses , URLs , file paths , etc. are identified and replaced with place - holders .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,This allows reducing data sparsity where it is not needed .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"Then , the data are tokenised using the Tilde MT regular expression - based tokeniser .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,The Moses truecasing script truecase .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,perl is used to truecase the first word of every sentence .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"Then , tokens are split into sub - word units using byte - pair encoding ( BPE ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"For the constrained and unconstrained systems , we use BPE models consisting of 24,500 and 49,500 merging operations respectively .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"Finally , data for the constrained systems are factored using an averaged perceptron - based morpho-syntactic tagger for Estonian and the lexicalized probabilistic parser , we introduce also a factor indicating a word part 's position in a word ( beginning , middle , end , or the word part represents the whole word - B , I , E , or O ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"As a result , the Estonian data consist of the the following factors : word part , position , lemma , and morpho-syntactic tag .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"The English data consist of the following factors : word part , position , lemma , part - of - speech tag , and syntactic function .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"1 ) back - translated data , and 2 ) data infused with unknown token identifiers .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"The back - translated data allow performing domain adaptation and the second type of synthetic data allow training NMT models that are robust to unknown phenomena ( e.g. , code - mixed content , target language words in the source text , rare or unseen words , etc . ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"To create the synthetic corpora with unknown phenomena , we extracted fast align the parallel corpora and randomly replaced one to three unambiguously ( one - to - one ) aligned content words with unknown word identifiers .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"These synthetic corpora were added to the parallel corpora , thereby almost doubling the sizes of the available training data .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"1 ) the constrained system data were acquired from initial Transformer - based NMT systems that were trained on the filtered and preprocessed parallel data , which were supplemented with the unknown phenomena infused data , and 2 ) the unconstrained system data were acquired from pre-existing unconstrained MLSTM - based NMT systems - the NMT systems that were developed by Tilde for the Estonian EU Council Presidency in 2017 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"In order to limit noise , the back - translated data were filtered using the same parallel data filtering methods that were described in Section 3.1.1 ( although with a higher threshold for the content overlap filter ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"Furthermore , in order to train the final systems , we also generated unknown phenomena infused data for the back - translated filtered data , thereby also almost doubling the sizes of the back - translated data .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,The synthetic corpora statistics and the sizes of the total training data are given in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,The light workflow was used to produce the tilde - c - nmt - 2 bt ( constrained NMT with two sets of back - translated data ) systems .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"First , we trained baseline models using only filtered parallel datasets ( Parallel - only in ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"Then , we back - translated the first batches of monolingual news data and trained intermediate NMT systems ( Parallel + First Back - translated ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"Finally , we used the intermediate NMT systems to backtranslate the second batches of monolingual news data and trained final NMT systems ( Parallel + Second Back - translated ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"The training progress in shows that the English - Estonian system benefits from the additional data , but the system in the other direction - not so much .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"For the final translations , we used a postprocessing script to replace consecutive repeating n-grams and repeating ngrams that have a preposition between them ( i.e. , victim of the victim ) with a single n-gram .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"This problem was more apparent in RNN - based NMT systems , but it was also noticable in our Transformer model outputs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"In order to train the NMT systems , we used the Nematus ( Sennrich et al. , 2017 b ) ( for MLSTM models ) and Sockeye ) ( for Transformer models ) toolkits .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"All models were trained until convergence ( i.e. , until an early stopping criterion was met ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,Estonian - English ( right ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"NMT models so far have struggled with translating rare or unseen words ( not different surface forms , but rather different words ) correctly .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"Named entities and non-translatable entities ( various product names , identifiers , etc. ) are often rare or unknown .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"In order to aid the NMT model in translating such tokens better , we extracted named entity and non-translatable token dictionaries from the parallel corpora .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"This was done by performing word alignment of the parallel corpora using fast align and searching ( in a language - agnostic manner ) for transliterated source - target word pairs using a similarity metric based on Levenshtein distance , which start with upper-case letters .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,The dictionaries consist of 15.6 ( 94.7 ) thousand and 6.2 ( 149.8 ) thousand entries for the constrained ( unconstrained ) English - Estonian and Estonian - English NMT systems respectively .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"When the NMT systems had translated a sentence , source - to - target word alignment was extracted from the source sentence and the translation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"Then named entity recognition ( based on dictionary look - up ) was performed on the source text and , if a named entity was found , the target translation was validated against the entries in the dic-tionary .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"In order to capture different surface forms , a stemming tool was used .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"If a translation was contradicting the entries in the dictionary , it was replaced with the closest matching ( by looking for the longest matching suffix ) translation from the dictionary .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"The automatic post-editing method for named entities has a marginal impact on translation quality , however , manual analysis showed that more named entities were corrected than ruined .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,We attempted to increase the quality of existing translations by employing a voting scheme in which multiple machine translation outputs are combined to produce a single translation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,We used a custom implementation of the majority voting algorithm to combine six of our best - scoring outputs in the Estonian - English translation direction in the constrained scenario .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,We did not perform the combination for English - Estonian due to lack of support for alignment extraction for Estonian in Meteor .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,MT system translation combination happens on the sentence level .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,The majority voting scheme assumes a single base translation hypothesis ( primary hypothesis ) which is aligned at the word level to each of the other hypotheses ( secondary hypotheses ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,The alignments are used to generate a table of all possible word translations relative to each position in the primary hypothesis .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,The table is then used to count the number of occurrences of different translations .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,The word translations with the highest count at each position constitute the resulting combined hypothesis .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,To acquire the necessary word alignments we used Meteor .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,Meteor outputs were then converted to a more easily manageable form using the Jane toolkit ) ( we used an awk script distributed with Jane ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,The majority voting algorithm was implemented in Python .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,We performed automatic evaluation of the NMT systems using the SacreBLEU evaluation tool .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,The results ( see ) show that the Transformer models achieved better results than the MLSTM - based models .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"For the constrained scenarios , both ensembles of averaged models achieved higher scores than each individual averaged model .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,It is also evident that the unconstrained models ( tilde - nc - nmt ) achieved the best results .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"Although the unconstrained models were not trained on factored data , the datasets were 17 times larger than the constrained datasets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"However , the difference is rather minimal and shows that the current NMT architectures may notable to learn effectively from large datasets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,The official human evaluation results ( see Table 5 ) from the WMT 2018 shared task on news translation our unconstrained scenario systems ( tilde - nc - nmt ) ranked significantly higher than any other submission for both translation directions .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,"Our best constrained systems were the second highest ranked systems among all constrained scenario systems , at the same time sharing the same cluster with the highest ranked systems .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,The paper described the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,We compared Transformer models to MLSTMbased models and showed that the Transformer models outperform the older NMT architecture .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,We also showed that double back - translation may improve translation quality further than single back - translation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,Top three systems for the constrained ( C ) and unconstrained ( U ) scenarios according to the official results of the WMT 2018 shared task on news translation ; ordered by the direct assessment ( DA ) standardized mean score sembling different run averaged models .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
0,Estonian language pair .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/5/W18-6423-Stanza-out.txt,"The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation . We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results . For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions . The submitted systems were trained using Transformer models .",0
7,Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,"In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,"To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,"Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,"Classifying semantic relations between entity pairs in sentences plays a vital role in various NLP tasks , such as information extraction , question answering and knowledge base population .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,A task of relation classification is defined as predicting a semantic relationship between two tagged entities in a sentence .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,"For example , given a sentence with tagged entity pair , crash and attack , this sentence is classified into the re-lation Cause - Effect ( e1 , e2 ) 1 between the entity pair like .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,"A first entity is surrounded by e 1 and / e 1 , and a second entity is surrounded by e 2 and / e 2 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,"Most previous relation classification models rely heavily on high - level lexical and syntactic features obtained from NLP tools such as WordNet , dependency parser , part - of - speech ( POS ) tagger , and named entity recognizer ( NER ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,The classification models relying on such features suffer from propagation of implicit error of the tools and they are computationally expensive .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"Recently , many studies therefore propose end - toend neural models without the high - level features .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"Among them , attention - based models , which focus to the most important semantic information in a sentence , show state - of - the - art results in a lot of NLP tasks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"Since these models are mainly proposed for solving translation and language modeling tasks , they could not fully utilize the information of tagged entities in relation classification task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"However , tagged entity pairs could be powerful hints for solving relation classification task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"For example , even if we do not consider other words except the crash and attack , we intuitively know that the entity pair has a relation Cause - Effect ( e1 , e2 ) 1 better than Component - Whole ( e1 , e2 ) 1 in To address these issues , We propose a novel endto - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"To capture the context of sentences , We obtain word representations by self attention mechanisms and build the recurrent neural architecture with Bidirectional Long Short - Term Memory ( LSTM ) networks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,Entity - aware attention focuses on the most important semantic information considering entity pairs with word positions relative to these pairs and latent types obtained by LET .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,"We show that our model is more interpretable since it 's decision making process could be visualized with self attention , entity - aware attention , and LET .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,There are several studies for solving relation classification task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,Early methods used handcrafted features through a series of NLP tools or manually designing kernels .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,"These approaches use high - level lexical and syntactic features obtained from NLP tools and manually designing kernels , but the classification models relying on such features suffer from propagation of implicit error of the tools .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,"On the other hands , deep neural networks have shown outperform previous models using handcraft features .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"Especially , many researches tried to solve the problem based on end - to - end models using only raw sentences and pre-trained word representations learned by Skip - gram and Continuous Bag - of - Words .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,Zeng et al .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,employed a deep convolutional neural network ( CNN ) for extracting lexical and sentence level features .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,Dos Santos et al.,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,proposed model for learning vector of each relation class using ranking loss to reduce the impact of artificial classes .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,Zhang and Wang used bidirectional recurrent neural network ( RNN ) to learn long - term dependency between entity pairs .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"Fur-thermore , Zhang et al. proposed bidirectional LSTM network ( BLSTM ) utilizing position of words , POS tags , named entity information , dependency parse .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,This model resolved vanishing gradient problem appeared in RNNs by using BLSTM .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"Recently , some researcher have proposed attentionbased models which can focus to the most important semantic information in a sentence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,Zhou et al. combined attention mechanisms with BLSTM .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,Xiao and Liu split the sentence into two entities and used two attention - based BLSTM hierarchically .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,Shen and Huang proposed attention - based CNN using word level attention mechanism that is able to better determine which parts of the sentence are more influential .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"In contrast with end - to - end model , several works proposed models utilizing the shortest dependency path ( SDP ) between entity pairs of dependency parse trees .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,SDP - LSTM model proposed by Yan et al .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,and deep recurrent neural networks ( DRNNs ) model proposed by Xu et al eliminate irrelevant words out of SDP and use neural network based on the meaningful words composing SDP .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"In this section , we introduce a novel recurrent neural model that incorporate an entity - aware attention mechanism with a LET method in detail .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"As shown inure 2 , our model consists of four main components : Word Representation that maps each word in a sentence into vector representations ; ( 2 ) Self Attention that captures the meaning of the correlation between words based on multi-head attention ; ( 3 ) BLSTM which sequentially encodes the representations of self attention layer ; ( 4 ) Entity - aware Attention that calculates attention weights with respect to the entity pairs , word positions relative to these pairs , and their latent types obtained by LET .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"After that , the features are averaged along the time steps to produce the sentencelevel features .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,where n is the number of words .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"R dw |V | , where d w is the dimension of the vector and | V | is the size of vocabulary .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,R dw are fed into the next layer .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,We can obtain the richer word representations by using self attentions .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,These word representations are considered the context based on correlation between words in a sentence .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"The illustrates the results of the self attention in the sentence , "" the ?e1 ? pollution ? / e1 ?was caused by the ?e2 ? shipwrek ?/e2 ? "" , which is labeled Cause - Effect ( e1 , e2 ) .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,There are visualizations of the two heads in the multi-head attention applied for self attention .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"The color density indicates the attention values , results of Equation 3.1 , which means how much an entity focuses on each word in a sentence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"In , the left represents the words that pollution , the first entity , focuses on and the right represents the words that shipwreck , the second entity , focuses on .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"We can recognize that the entity pair is commonly concentrated on was , caused , and each other .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"Actually , these words play the most important role in semantically predicting the Cause - Effect ( e1 , e2 ) , which is the relation class of this entity pair .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"shows where the model focuses on the sentence to compute relations between entity pairs , which is the result of visualizing the alpha vectors in Equation 3.9 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"The important words in sentence are highlighted in yellow , which means that the more clearly the color is , the more important it is .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,"For example , in the first sentence , the inside is strongly highlighted , which is actually the best word representing the relation Component - whole ( e 1 , e2 ) between the given entity pair .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,"As another example , in the third sentence , the highlighted assess and using represent the relation , Instrument - Agency ( e2 , e1 ) between entity pair , analysts and frequency , well .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,"We can see that the using is more highlighted than the assess , because the former represents the relation better .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,"Since the dimensionality of representation vectors are too large to visualize , we applied the t - SNE , one of the most popular dimensionality reduction methods .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,"In , the red points represent latent type vectors c i?K and the rests are latent type representations t j , where the colors of points are determined by the closest of the latent type vectors in the vector space of the original dimensionality .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,The points are generally well divided and are almost uniformly distributed without being biased to one side .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,summarizes the results of extracting 50 entities in close order with each latent type vector .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,This allows us to roughly understand what latent types of entities are .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,We use a total of three types and find that similar characteristics appear in words grouped by together .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,"In the type 1 , the words are related to human 's jobs and foods .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"The type2 has a lot of entities related to machines and engineering like engine , woofer , and motor .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,Sets of Entities grouped by Latent Types drugs .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"As a result , each type has a set of words with similar characteristics , which can prove that LET works effectively .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"For sequentially encoding the output of self attention layer , we use a BLSTM that consists of two sub LSTM networks : a forward LSTM network which encodes the context of a input sentence and a backward LSTM network which encodes that one of the reverse sentence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,The representation vectors M obtained from self attention layer are forwarded into to the network step by step .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,Although many models with attention mechanism achieved state - of - the - art performance in many NLP tasks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"However , for the relation classification task , these models lack of prior knowledge forgiven entity pairs , which could be powerful hints for solving the task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,Relation classification differs from sentence classification in that information about entities is given along with sentences .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,We propose a novel entity - aware attention mechanism for fully utilizing informative factors in given entity pairs .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"In relation classification , the position of each word relative to entities has been widely used for word representations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,"Recently , position - aware attention is published as away to use the relative position features more effectively .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,"It is a variant of attention mechanisms , which use not only outputs of BLSTM but also the relative position features when calculating attention weights .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,We adopt this method with slightly modification as shown in Equation 3.8 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,"R dp corresponds to the position of the i - th word relative to the first entity ( e 1 - th word ) and second entity ( e 2 - th word ) in a sentence respectively , where e j ?{ 1 , 2 } is a index of j-th entity .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"R dp ( 2L?1 ) , where d p is the dimension of the relative position vectors and L is the maximum sentence length .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"Finally , the representations of BLSTM layer take into account the context and the positional relationship with entities by concatenating hi , p e 1 i , and p e 2 i .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,R da ( 2 d h + 2 dp ) as in the Equation 3.8 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"Since entity pairs are powerful hints for solving relation classification task , we involve the entity pairs and their types in the attention mechanism to effectively train relations between entity pairs and other words in a sentence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,We employ the two entity - aware features .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"The first is the hidden states of BLSTM corresponding to positions of entity pairs , which are high - level features representing entities .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"R 2d h , where e i is index of i - th entity .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"In addition , latent types of the entities obtained by LET , our proposed novel method , are the second one .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"Using types as features can be a great way to improve performance , since the types of entities alone can be inferred the approximate relations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"Because the annotated types are not given , we use the latent type representations by applying the LET inspired by latent topic clustering , a method for predicting latent topic of texts in question answering task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,The LET constructs the type representations by weighting K latent type vectors based on attention mechanisms .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,where c i is the i - th latent type vector and K is the number of latent entity types .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"As a result , entity features are constructed by concatenating the hidden states corresponding entity positions and types of entity pairs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,R 2 d h is computed by Equations from 3.8 to 3.10 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,The sentence representation obtained from the entity - aware attention z is fed into a fully connected softmax layer for classification .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,where y is a target relation class and S is the input sentence .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,where | R| is the number of relation classes .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"where | D| is the size of training dataset and ( S ( i ) , y ( i ) ) is the i - th sample in the dataset .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,We minimize the loss L using AdaDelta optimizer to compute the parameters ? of our model .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"To alleviate overfitting , we constrain the L2 regularization with the coefficient ?.",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"In addition , the dropout method is applied afterword embedding , LSTM network , and entity - aware attention to prevent co-adaptation of hidden units by randomly omitting feature detectors .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"We evaluate our model on the SemEval - 2010 Task 8 dataset , which is an commonly used benchmark for relation classification and compare the results with the state - of - the - art models in this area .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"The dataset contains 10 distinguished relations , Cause - Effect , Instrument - Agency , Product - Producer , Content - Container , Entity - Origin , Entity - Destination , Component - Whole , Member - Collection , Message - Topic , and Other .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"The former 9 relations have two directions , whereas Other is not directional , so the total number of relations is 19 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"There are 10,717 annotated sentences which consist of 8,000 samples for training and 2,717 samples for testing .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"We adopt the official evaluation metric of SemEval - 2010 Task 8 , which is based on the macro -averaged F1 - score ( excluding Other ) , and takes into consideration the directionality .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,We tune the hyperparameters for our model on the development set randomly sampled 800 sentences for validation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,The best hyperparameters in our proposed model are shown in following .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"Description Value We use pre-trained weights of the publicly available Glo Ve model to initialize word embeddings in our model , and other weights are randomly initialized from zero-mean Gaussian distribution .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,compares our Entity - aware Attention LSTM model with state - of - theart models on this relation classification dataset .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"We divide the models into three groups , Non-Neural Model , SDP - based Model , and End - to - End Model .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"First , the SVM , Non-Neural Model , was top of the SemEval - 2010 task , during the official competition period .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,They used many handcraft feature and SVM classifier .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"As a result , they achieved an F1-score of 82.2 % .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"The second is SDP - based Model such as MVRNN , FCM , DepNN , de pLCNN + NS , SDP - LSTM , and DRNNs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,The SDP is reasonable features for detecting semantic structure of sentences .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"Actually , the SDP - based models show high performance , but SDP may not always be accurate and the parsing time is exponentially increased by long sentences .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,The last model is End - to - End Model automatically learned internal representations can occur between the original inputs and the final outputs in deep learning .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"There are CNN - based models such as CNN , CR - CNN , and Attention - CNN and RNN - based models such as BLSTM , Attention - BLSTM , and Hierarchical - BLSTM ( Hier - BLSTM ) for this task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"Non Our proposed model achieves an F1-score of 85.2 % which outperforms all competing state - of - theart approaches except depLCNN + NS , DRNNs , and Attention - CNN .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"However , they rely on high - level lexical features such as WordNet , dependency parse trees , POS tags , and NER tags from NLP tools .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,The experimental results show that the LET is effective for relation classification .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,The LET improve a performance of 0.5 % than the model not applied it .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,The model showed the best performance with three types .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,There are three different visualization to demonstrate that our model is more interpretable .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"First , the visualization of self attention shows where each word focus on parts of a sentence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"By showing the words that the entity pair attends , we can find the words that well represent the relation between them .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"Next , the entity - aware attention visualization shows where the model pays attend to a sentence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"This visualization result highlights important words in a sentence , which are usually important keywords for classification .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"Finally , we visualize representation of type in LET by using t- SNE , a method for dimensionality reduction , and group the whole entities in the dataset by the its latent types .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",0
7,"In this paper , we proposed entity - aware attention mechanism with latent entity typing and a novel end - to - end recurrent neural model which incorporates this mechanism for relation classification .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,Task 8 using only raw sentence and word embeddings without any high - level features from NLP tools and it outperforms existing state - of - the - art methods .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,"In addition , our three visualizations of attention mechanisms applied to the model demonstrate that our model is more interpretable than previous models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,We expect our model to be extended not only the relation classification task but also other tasks that entity plays an important role .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,"Especially , latent entity typing can be effectively applied to sequence modeling task using entity information without NER .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
7,"In the future , we will propose anew method in question answering or knowledge base population based on relations between entities extracted from our model .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/6/1901.08163v1-Stanza-out.txt,"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) . Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) . In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification . To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method . Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET . Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",1
2,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",1
2,"The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",1
2,"On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",1
2,Extracting entities and their semantic relations from raw text is a key information extraction task .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",1
2,"For example , given the sentence "" David Foster is the AP 's Northwest regional reporter , based in Seattle "" in the CoNLL04 dataset , our goal is to recognize "" David Foster "" as person , "" AP "" as organization , and "" Northwest "" and "" Seattle "" as location entities , then classifiy entity pairs to extract structured information : Work For ( David Foster , AP ) , OrgBased In ( AP , Northwest ) and OrgBased In ( AP , Seattle ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",1
2,Such information is useful in many other NLP tasks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",1
2,"Especially in IR applications such as entity search , structured search and question answering , it helps provide end users with significantly better search experience .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",1
2,A common relation extraction approach is to construct pipeline systems with separate sub-systems for the two tasks of named entity recognition and relation classification .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",1
2,"More recently , end - to - end systems which jointly learn to extract entities and relations have been proposed with strong potential to obtain high performance .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",1
2,Traditional joint approaches are feature - based supervised learning methods which employ numerous syntactic and lexical features based on external NLP tools as well as knowledge base resources .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,State - of - the - art relation extraction performance has been obtained by end - to - end models based on neural networks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"Specifically , proposed a RNNbased model which achieved top results on the CoNLL04 dataset .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,Their approach relies on various manually extracted features .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,Other neural models employ dependency parsing - based information .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"In particular , applied bottom - up and top - down tree - structured LSTMs to model dependency paths between entities .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,integrated implicit syntactic information by using latent feature representations extracted from a pre-trained BiLSTM - based dependency parser .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"entity recognition , and a CNN on top of the BiLSTM for classifying relations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"Adel and Schtze ( 2017 ) assumed that entity boundaries are given , and trained a CNN to extract context features around the entities , and using these features for entity and relation classification .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"Recently , formulated the joint entity and relation extraction problem as a directed graph and proposed a BiLSTM - and transition - based approach to generate the graph incrementally . [ 4 ] extended the multi-head selection - based joint model with adversarial training .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"In , the joint task is formulated as a sequence tagging problem , and a BiLSTM with a softmax output layer can then be used for joint prediction .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"In this paper , we present a novel end - to - end neural model for joint entity and relation extraction .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"As illustrated in , our model architecture can be viewed as a mixture of a named entity recognition ( NER ) component and a relation classification ( RC ) component .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,Our NER component employs a BiLSTM - CRF architecture to predict entities from input word tokens .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"Based on both the input words and the predicted NER labels , the RC component uses another BiLSTM to learn latent features relevant for relation classification .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"In most previous neural joint models , the relation classification part relies on a common "" linear "" concatenation - based mechanism over the latent features associated with entity pairs , i.e. the latent features are first concatenated into a single feature vector which is then linearly transformed before being fed into a softmax classifier .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"In contrast , our RC component takes into account second - order interactions over the latent features via a tensor .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"In particular , for relation classification we propose a novel use of the deep biaffine attention mechanism which was first introduced in dependency parsing .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"Experimental results on the benchmark "" relation and entity recognition "" dataset CoNLL04 show that our model outperforms previous models , obtaining new stateof - the - art scores .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"In addition , using the biaffine attention improves the performance compared to using the linear mechanism significantly .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,We also provide an ablation study to investigate effects of different contributing factors in our model .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,This section details our end - to - end relation extraction model .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"Here , for each word type w , we use a one - layer BiLSTM ( BiLSTM char ) to learn its character - level word embedding e ( C ) w.",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"The NER component feeds the sequence of vectors v 1:n with an additional context position index i into another BiLSTM ( BiLSTM NER ) to learn a "" latent "" feature vector representing the i th word token .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,The output layer size of FFNN NER is the number of BIOLU - based NER labels .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,The NER component feeds the output vectors h 1:n into a linear - chain CRF layer for NER label prediction .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"A cross-entropy loss L NER is computed during training , while the Viterbi algorithm is used for decoding .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,Our NER component thus is the BiLSTM - CRF model with additional LSTM - based character - level word embeddings .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"Relation classification ( RC ) : Assume that t 1 , t 2 , ... , tn are NER labels predicted by the NER component for the input words .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,We represent each i th predicted label by a vector embedding e ti .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,The RC component further uses these latent vectors r i for relation classification .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,We propose a novel use of the deep biaffine attention mechanism for relation classification .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"The biaffine attention mechanism was proposed for dependency parsing , helping to produce the best reported parsing performance to date .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"Following , our RC component incrementally constructs relation candidates using all possible combinations of the last word tokens of predicted entities , i.e. words with L or U labels .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,We assign an entity pair to a negative relation class ( NEG ) when the pair has no relation or when the predicted entities are not correct .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"For example , for , we would have two relation candidates : NEG ( Paris , International ) and OrgBased In ( International , Paris ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,A realistic scenario where entity boundaries are not given .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,( 2 ) EC&RC : A less realistic scenario where the entity boundaries are given ] .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",1
2,Thus the NER task which identifies both entity boundaries and classes reduces to the entity classification ( EC ) task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",1
2,"Following , we encode the gold entity boundaries in the BILOU scheme .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",1
2,"Then we represent each B , I , O , L or U boundary tag as a vector embedding .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",1
2,"As a result , the vector vi in Equation 1 now also includes the boundary tag embedding in addition to the word embedding and character - level word embedding .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",1
2,"Dataset : We use the benchmark "" entity and relation recognition "" dataset CoNLL04 from .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"Following , we use the 64%/16%/20 % training / development / test presplit available from Adel and Schtze ( 2017 ) , in which the test set was previously also used by .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,Our model is implemented using DYNET v 2.0 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"We optimize the objective loss using Adam , no mini-batches and run for 100 epochs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,We compute the average of NER / EC score and RC score after each training epoch .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"We choose the model with the highest average score on the development set , which is then applied to the test set for the final evaluation phase .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,More details of the implementation as well as optimal hyper - parameters are in the Appendix .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"Metric : Similar to previous works in , we use the macro -averaged F1 - score over the entity classes to score NER / EC and over the relation classes to score RC .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,More details of the metric are also in the Appendix .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"Unlike previous neural models , we report results as mean and standard deviation of the scores over 10 runs with 10 random seeds .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,The first six rows in compare our results with previous state - of - the - art published results on the same test set .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"In particular , our model obtains 2 + % absolute higher NER and RC scores ( Setup 1 ) than the BiLSTM - CRF - based multihead selection model .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,We also obtain 7 + % higher EC and RC scores ( Setup 2 ) than Adel and Schtze ( 2017 ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"Note that Gupta et al. ( 2016 ) use the same test set as we do , however they report final results on a 80/0 / 20 training / development / test split rather than our 64/16 /20 , i.e. Gupta et al. ( 2016 ) use a larger training set , but producing about 1.5 % lower EC score and similar RC score against ours .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"These results show that our model performs better than previous state - of - the - art models , using the same setup .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"In , the last two rows present results reported in and on the dataset CoNLL04 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"However , these results are not comparable due to their random sampling of the test set , i.e. using different train - test splits .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,Both and employ additional extra features based on external NLP tools and use larger training sets than ours .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"Specifically , integrate syntactic features by using a pre-trained BiLSTM - based dependency parser to extract BiLSTM - based latent feature representations for words in the input sentence , and then using these latent representations directly as part of the input embeddings in their model .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,We plan to extend our model with their syntactic integration approach to further improve our model performance in future work .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"We provide in the results of a pipeline approach where we treat our two NER and RC components as independent networks , and train them separately .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"Here , the RC network uses gold NER labels when training , and uses predicted labels produced by the NER network when decoding .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"We find that the joint approach does slightly better than the pipeline approach in relation classification , although the .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"Ablation results on the development set . * and ** denote the statistically significant differences against the full results at p < 0.05 and p < 0.01 , respectively ( using the two - tailed paired t- test ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,differences are not significant .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,A similar observation is also found in .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"Also , in preliminary experiments , we do not find any significant difference in performance of our joint model when feeding gold NER labels instead of predicted NER labels into the RC component during training .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",1
2,This is not surprising as the training NER score is at 99 +% .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",1
2,also presents ablation tests over 5 factors of our joint model on the development set .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",1
2,"In particular , Setup 1 performances significantly degrade by 4 + % absolutely , when not using the character - level word embeddings .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",1
2,"The performances also decrease when using a softmax classifier for NER label prediction rather than a CRF layer ( here , the decrease is significant ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",1
2,"In contrast , we do not find any significant difference in Setup 2 scores when not using either the character - level embeddings or the CRF layer , clearly showing the usefulness of the given gold entity boundaries .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",1
2,"The 3 remaining factors , including removing NER label embeddings and not taking either the Bilinear or Linear part ( in Equation 8 ) into the Biaffine attention layer , do not affect the NER / EC score .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",1
2,"However , they significantly decrease the RC score .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",1
2,"This is reasonable because those 3 factors are part of the RC component only , thus helpful in predicting relations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"More specifically , using the Biaffine attention produces about 1.5 % significant improvements to a common Linear transformation mechanism in relation classification , i.e. , "" w / o Bilinear "" results against the full results in : 65.4 % vs. 66.9 % and 72.0 % vs. 73.3 % ( although using Biaffine increases training time over using Linear by 35 % , relatively ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,"In this paper , we have presented an end - to - end neural network - based relation extraction model .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",0
2,Our model employs a BiLSTM - CRF architecture for entity recognition and a biaffine attention mechanism for relation classification .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",1
2,"On the benchmark CoNLL04 dataset , our model produces new state - of - the - art performance .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/5/1812.11275v1-Stanza-out.txt,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features . The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship . On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",1
8,Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"We introduce BioBERT ( Bidirectional Encoder Representations from Transformers for Biomedical Text Mining ) , which is a domain - specific language representation model pre-trained on large - scale biomedical corpora .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"With almost the same architecture across tasks , BioBERT largely outperforms BERT and previous state - of - the - art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"While BERT obtains performance comparable to that of previous state - of - the - art models , BioBERT significantly outperforms them on the following three representative biomedical text mining tasks : biomedical named entity recognition ( 0.62 % F1 score improvement ) , biomedical relation extraction ( 2.80 % F1 score improvement ) and biomedical question answering ( 12.24 % MRR improvement ) .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,The volume of biomedical literature continues to rapidly increase .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"On average , more than 3000 new articles are published everyday in peer-reviewed journals , excluding pre-prints and technical reports such as clinical trial reports in various archives .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,PubMed alone has a total of 29M articles as of January 2019 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,Reports containing valuable information about new discoveries and new insights are continuously added to the already overwhelming amount of literature .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"Consequently , there is increasingly more demand for accurate biomedical text mining tools for extracting information from the literature .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,Recent progress of biomedical text mining models was made possible by the advancements of deep learning techniques used in natural language processing ( NLP ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"For instance , Long Short - Term Memory ( LSTM ) and Conditional Random Field ( CRF ) have greatly improved performance in biomedical named entity recognition ( NER ) over the last few years .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,Other deep learning based models have made improvements in biomedical text mining tasks such as relation extraction ( RE ) and question answering ( QA ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"However , directly applying state - of - the - art NLP methodologies to biomedical text mining has limitations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"First , as recent word representation models such as Word2 Vec , ELMo and BERT are trained and tested mainly on datasets containing general domain texts ( e.g. Wikipedia ) , it is difficult to estimate their performance on datasets containing biomedical texts .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"Also , the word distributions of general and biomedical corpora are quite different , which can often be a problem for biomedical text mining models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"As a result , recent models in biomedical text mining rely largely on adapted versions of word representations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"In this study , we hypothesize that current state - of - the - art word representation models such as BERT need to be trained on biomedical corpora to be effective in biomedical text mining tasks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"Previously , Word2 Vec , which is one of the most widely known context independent word representation models , was trained on biomedical corpora which contain terms and expressions that are usually not included in a general domain corpus .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"While ELMo and BERT have proven the effectiveness of contextualized word representations , they can not obtain high performance on biomedical corpora because they are pre-trained on only general domain corpora .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"As BERT achieves very strong results on various NLP tasks while using almost the same structure across the tasks , adapting BERT for the biomedical domain could potentially benefit numerous biomedical NLP researches .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"In this article , we introduce BioBERT , which is a pre-trained language representation model for the biomedical domain .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,The overall process of pre-training and fine - tuning BioBERT is illustrated in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"First , we initialize BioBERT with weights from BERT , which was pretrained on general domain corpora ( English Wikipedia and Books Corpus ) .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"Then , BioBERT is pre-trained on biomedical domain corpora ( PubMed abstracts and PMC full - text articles ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"To show the effectiveness of our approach in biomedical text mining , BioBERT is fine - tuned and evaluated on three popular biomedical text mining tasks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"We test various pre-training strategies with different combinations and sizes of general domain corpora and biomedical corpora , and analyze the effect of each corpus on pre-training .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,We also provide in - depth analyses of BERT and BioBERT to show the necessity of our pre-training strategies .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,BioBERT is the first domain - specific BERT based model pretrained on biomedical corpora for 23 days on eight NVIDIA V100 GPUs .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,We show that pre-training BERT on biomedical corpora largely improves its performance .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"BioBERT obtains higher F 1 scores in biomedical NER ( 0.62 ) and biomedical RE ( 2.80 ) , and a higher MRR score ( 12.24 ) in biomedical QA than the current state - of the - art models .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"Compared with most previous biomedical text mining models that are mainly focused on a single task such as NER or QA , our model BioBERT achieves state - of - the - art performance on various biomedical text mining tasks , while requiring only minimal architectural modifications .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"We make our pre-processed datasets , the pre-trained weights of BioBERT and the source code for fine - tuning BioBERT publicly available .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,BioBERT basically has the same structure as BERT .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"We briefly discuss the recently proposed BERT , and then we describe in detail the pre-training and fine - tuning process of BioBERT .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,Learning word representations from a large amount of unannotated text is a long - established method .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"While previous models ( e.g. Word2 Vec , GloVe ) focused on learning context independent word representations , recent works have focused on learning context dependent word representations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"For instance , ELMo uses a bidirectional language model , while uses machine translation to embed context information into word representations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,BERT is a contextualized word representation model that is based on a masked language model and pretrained using bidirectional transformers .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"Due to the nature of language modeling where future words can not be seen , previous language models were limited to a combination of two unidirectional language models ( i.e. left - to - right and right - toleft ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"BERT uses a masked language model that predicts randomly masked words in a sequence , and hence can be used for learning bidirectional representations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"Also , it obtains state - of - the - art performance on most NLP tasks , while requiring minimal task - specific architectural modification .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"According to the authors of BERT , incorporating information from bidirectional representations , rather than unidirectional representations , is crucial for representing words in natural language .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,We hypothesize that such bidirectional representations are also critical in biomedical text mining as complex relationships between biomedical terms often exist in a biomedical corpus .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"Due to the space limitations , we refer readers to fora more detailed description of BERT .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"As a general purpose language representation model , BERT was pretrained on English Wikipedia and Books Corpus .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"However , biomedical domain texts contain a considerable number of domain - specific .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"Overview of the pre-training and fine - tuning of BioBERT proper nouns ( e.g. BRCA1 , c.248T > C ) and terms ( e.g. transcriptional , antimicrobial ) , which are understood mostly by biomedical researchers .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"As a result , NLP models designed for general purpose language understanding often obtains poor performance in biomedical text mining tasks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"In this work , we pre-train BioBERT on PubMed abstracts ( PubMed ) and PubMed Central full - text articles ( PMC ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"The text corpora used for pre-training of BioBERT are listed in , and the tested combinations of text corpora are listed in .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"For computational efficiency , whenever the Wiki Books corpora were used for pre-training , we initialized BioBERT with the pre-trained BERT model provided by .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,We define BioBERT as a language representation model whose pre-training corpora includes biomedical corpora ( e.g. BioBERT ( PubMed ) ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"For tokenization , BioBERT uses WordPiece tokenization , which mitigates the out - of - vocabulary issue .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,I ##mm ##uno ##g ##lo # #bul # #in ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,We found that using cased vocabulary ( not lowercasing ) results in slightly better performances in downstream tasks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"Although we could have constructed new WordPiece vocabulary based on biomedical corpora , we used the original vocabulary of BERT BASE for the following reasons : ( i ) compatibility of BioBERT with BERT , which allows BERT pre-trained on general domain corpora to be re-used , and makes it easier to interchangeably use existing models based on BERT and BioBERT and ( ii ) any new words may still be represented and fine - tuned for the biomedical domain using the original WordPiece vocabulary of BERT .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"With minimal architectural modification , BioBERT can be applied to various downstream text mining tasks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"We fine - tune BioBERT on the following three representative biomedical text mining tasks : NER , RE and QA .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"Named entity recognition is one of the most fundamental biomedical text mining tasks , which involves recognizing numerous domain - specific proper nouns in a biomedical corpus .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"While most previous works were built upon different combinations of LSTMs and CRFs , BERT has a simple architecture based on bidirectional transformers .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,BERT uses a single output layer based on the representations from its last layer to compute only token level BIO2 probabilities .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"Note that while previous works in biomedical NER often used word embeddings trained on PubMed or PMC corpora , BioBERT directly learns WordPiece embeddings during pre-training and fine - tuning .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"For the evaluation metrics of NER , we used entity level precision , recall and F1 score .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,Relation extraction is a task of classifying relations of named entities in a biomedical corpus .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"We utilized the sentence classifier of the original version of BERT , which uses a [ CLS ] token for the classification of relations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,Sentence classification is performed using a single output layer based on a [ CLS ] token representation from BERT .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,We anonymized target named entities in a sentence using pre-defined tags such as @ GENE $ or @DISEASE $ .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"The precision , recall and F 1 scores on the RE task are reported .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,Question answering is a task of answering questions posed in natural language given related passages .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"To fine - tune BioBERT for QA , we used the same BERT architecture used for SQuAD .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,We used the BioASQ factoid datasets because their format is similar to that of SQuAD .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,Token level probabilities for the start / end location of answer phrases are computed using a single output layer .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"However , we observed that about 30 % of the BioASQ factoid questions were unanswerable in an extractive QA setting as the exact answers did not appear in the given passages .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"Like , we excluded the samples with unanswerable questions from the training sets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"Also , we used the same pre-training process of , which uses SQuAD , and it largely improved the performance of both BERT and BioBERT .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"We used the following evaluation metrics from BioASQ : strict accuracy , lenient accuracy and mean reciprocal rank .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,The statistics of biomedical NER datasets are listed in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"We used the pre-processed versions of all the NER datasets provided by except the 2010 i 2 b2 / VA , JNLPBA and Species - 800 datasets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,The pre-processed NCBI Disease dataset has fewer annotations than the original dataset due to the removal of duplicate articles from its training set .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,We used the CoNLL format ( https :// github.com/spyysalo/standoff2conll ) for pre-processing the 2010 i 2b2 / VA and JNLPBA datasets .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,The Species - 800 dataset was preprocessed and split based on the dataset of Pyysalo ( https://github. com/spyysalo/s800 ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"We did not use alternate annotations for the BC2 GM dataset , and all NER evaluations are based on entity - level exact matches .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"Note that although there are several other recently introduced high quality biomedical NER datasets , we use datasets that are frequently used by many biomedical NLP researchers , which makes it much easier to compare our work with theirs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,The RE datasets contain gene - disease relations and protein - chemical relations ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,Pre-processed GAD and EU - ADR datasets are available with our provided codes .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"For the CHEMPROT dataset , we used the same pre-processing procedure described in .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"We used the BioASQ factoid datasets , which can be converted into the same format as the SQuAD dataset ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,We used full abstracts ( PMIDs ) and related questions and answers provided by the BioASQ organizers .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,We have made the pre-processed BioASQ datasets publicly available .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"For all the datasets , we used the same dataset splits used in previous works ) fora fair evaluation ; however , the splits of LINAAEUS and Species - 800 could not be found from and maybe different .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"Like previous work , we reported the performance of 10 - fold cross-validation on datasets that do not have separate test sets ( e.g. GAD , EU - ADR ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,We compare BERT and BioBERT with the current state - of - theart models and report their scores .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,Note that the state - of - the - art models each have a different architecture and training procedure .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"For instance , the state - of - the - art model by trained on the JNLPBA dataset is based on multiple Bi - LSTM CRF models with character level CNNs , while the state - of - the - art model by trained on the LINNAEUS dataset uses a Bi - LSTM CRF model with character level LSTMs and is additionally trained on silver - standard datasets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"On the other hand , BERT and BioBERT have exactly the same structure , and use only the gold standard datasets and not any additional datasets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,We used the BERT BASE model pre-trained on English Wikipedia and Books Corpus for 1 M steps .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,BioBERT v1.0 ( PubMed PMC ) is the version of BioBERT ( PubMed PMC ) trained for 470 K steps .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"When using both the PubMed and PMC corpora , we found that 200K and 270K pre-training steps were optimal for PubMed and PMC , respectively .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"We also used the ablated versions of BioBERT v1.0 , which were pre-trained on only PubMed for 200 K steps ( Bio BERT v1.0 ( PubMed ) ) and PMC for 270K steps ( Bio BERT v1.0 ( PMC ) ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"After our initial release of BioBERT v 1.0 , we pre-trained BioBERT on PubMed for 1 M steps , and we refer to this version as BioBERT v 1.1 ( PubMed ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,Other hyper - parameters such as batch size and learning rate scheduling for pre-training BioBERT are the same as those for pre-training BERT unless stated otherwise .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"We pre-trained BioBERT using Naver Smart Machine Learning ( NSML ) , which is utilized for large - scale experiments that need to be run on several GPUs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,We used eight NVIDIA V100 ( 32GB ) GPUs for the pre-training .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"The maximum sequence length was fixed to 512 and the mini-batch size was set to 192 , resulting in 98 304 words per iteration .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,It takes more than 10 days to pre-train BioBERT v 1.0 ( PubMed PMC ) nearly 23 days for BioBERT v 1.1 ( PubMed ) in this setting .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"Despite our best efforts to use BERT LARGE , we used only BERT BASE due to the computational complexity of BERT LARGE .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,We used a single NVIDIA Titan Xp ( 12GB ) GPU to fine - tune BioBERT on each task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,Note that the fine - tuning process is more computationally efficient than pre-training BioBERT .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"For finetuning , a batch size of 10 , 16 , 32 or 64 was selected , and a learning rate of 5e5 , 3e5 or 1e5 was selected .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,Fine - tuning BioBERT on QA and RE tasks took less than an hour as the size of the training data is much smaller than that of the training data used by .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"On the other hand , it takes more than 20 epochs for BioBERT to reach its highest performance on the NER datasets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,The results of NER are shown in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"First , we observe that BERT , which was pre-trained on only the general domain corpus is quite effective , but the micro averaged F 1 score of BERT was lower ( 2.01 lower ) than that of the state - of - the - art models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"On the other hand , BioBERT achieves higher scores than BERT on all the datasets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"BioBERT outperformed the state - of - the - art models on six out of nine datasets , and BioBERT v 1.1 ( PubMed ) outperformed the state - of - the - art models by 0.62 in terms of micro averaged F1 score .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"The relatively low scores on the LINNAEUS dataset can be attributed to the following : ( i ) the lack of a silver - standard dataset for training previous state - of - the - art models and ( ii ) different training / test set splits used in previous work , which were unavailable .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,The RE results of each model are shown in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"BERT achieved better performance than the state - of - the - art model on the CHEMPROT dataset , which demonstrates its effectiveness in RE .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"On average ( micro ) , BioBERT v1.0 ( PubMed ) obtained a higher F1 score ( 2.80 higher ) than the state - of - the - art models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"Also , BioBERT achieved the highest F 1 scores on 2 out of 3 biomedical datasets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,The QA results are shown in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,We micro averaged the best scores of the state - of - the - art models from each batch .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,BERT obtained a higher micro averaged MRR score ( 7.0 higher ) than the state - of - the - art models .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"All versions of BioBERT significantly outperformed BERT and the state - of - the - art models , and in particular , BioBERT v1.1 ( PubMed ) obtained a strict accuracy of 38.77 , a lenient accuracy of 53.81 and a mean reciprocal rank score of 44.77 , all of which were micro averaged .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"On all the biomedical QA datasets , BioBERT achieved new state - of - the - art performance in terms of MRR .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,We used additional corpora of different sizes for pre-training and investigated their effect on performance .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"For BioBERT v1.0 ( PubMed ) , we set the number of pre-training steps to 200K and varied the size of the PubMed corpus .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"shows that the performance of BioBERT v 1.0 ( PubMed ) on three NER datasets ( NCBI Disease , BC2GM , BC4CHEMD ) changes in relation to the size of the PubMed corpus .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"Pre-training on 1 billion words is quite effective , and the performance on each dataset mostly improves until 4.5 billion words .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,We also saved the pre-trained weights from BioBERT v 1.0 ( PubMed ) at different pre-training steps to measure how the number of pre-training steps affects its performance on fine - tuning tasks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,shows the performance changes of BioBERT v 1.0 ( PubMed ) on the same three NER datasets in relation to the number of pre-training steps .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,The results clearly show that the performance on each dataset improves as the number of pre-training steps increases .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"Finally , shows the absolute performance improvements of BioBERT v 1.0 ( PubMed PMC ) over BERT on all 15 datasets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"F1 scores were used for NER / RE , and MRR scores were used for QA .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,BioBERT significantly improves performance on most of the datasets .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"As shown in , we sampled predictions from BERT and BioBERT v 1.1 ( PubMed ) to seethe effect of pre-training on downstream tasks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,entities .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"While BERT often gives incorrect answers to simple biomedical questions , BioBERT provides correct answers to such questions .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"Also , BioBERT can provide longer named entities as answers .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"In this article , we introduced BioBERT , which is a pre-trained language representation model for biomedical text mining .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,We showed that pre-training BERT on biomedical corpora is crucial in applying it to the biomedical domain .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"Requiring minimal task - specific architectural modification , BioBERT outperforms previous models on biomedical text mining tasks such as NER , RE and QA .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"The pre-released version of BioBERT ( January 2019 ) has already been shown to be very effective in many biomedical text mining tasks such as NER for clinical notes , human phenotype - gene RE and clinical temporal RE .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,The following updated versions of BioBERT will be available to the bioNLP community : ( i ) BioBERT BASE and BioBERT LARGE trained on only PubMed abstracts without initialization from the existing BERT model and ( ii ) BioBERT BASE and BioBERT LARGE trained on domain - specific vocabulary based on WordPiece .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"Notes : Precision ( P ) , Recall ( R ) and F1 ( F ) scores on each dataset are reported .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"The best scores are in bold , and the second best scores are underlined .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"We list the scores of the state - of - the - art ( SOTA ) models on different datasets as follows : scores of Xu et al . Notes : Precision ( P ) , Recall ( R ) and F1 ( F ) scores on each dataset are reported .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"The best scores are in bold , and the second best scores are underlined .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"The scores on GAD and EU - ADR were obtained from , and the scores on CHEMPROT were obtained from .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"Notes : Strict Accuracy ( S ) , Lenient Accuracy ( L ) and Mean Reciprocal Rank ( M ) scores on each dataset are reported .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"The best scores are in bold , and the second best scores are underlined .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,The best BioASQ 4 b / 5 b / 6 b scores were obtained from the BioASQ leaderboard ( http://participants-area.bioasq.org ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"BERT . . . a case of oral penicillin anaphylaxis is described , and the terminology . . .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,Corynebacterium minutissimum is the bacteria that leads to cutaneous eruptions of erythrasma . . .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,"Like the DMA , but unlike all other mammalian class II A genes , the zebrafish gene codes for two cysteine residues . . .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,A total of 25 women affected by clinical stress urinary incontinence ( SUI ) were enrolled .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,"Q-tip test , . . .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
8,Corynebacterium minutissimum is the bacteria that leads to cutaneous eruptions of erythrasma . . .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
8,Predicted named entities for NER and predicted answers for QA are in bold .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
6,Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",1
6,"While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",1
6,"In this work , we show that this is unfair : lexical features are actually quite useful .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",1
6,We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",1
6,"From this , we compute - offline - a feature vector representing each word .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"When used with a vanilla recurrent neural network model , this representation yields substantial improvements .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,This work is licensed under a Creative Commons Attribution 4.0 International License .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,Named - Entity Recognition ( NER ) is the task of identifying textual mentions and classifying them into a predefined set of types .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"Various approaches have been proposed to tackle the task , from hand - crafted feature - based machine learning models like conditional random fields and perceptron , to deep neural models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"Word representations , also known as word embeddings , area key element for multiple NLP tasks including NER .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",1
6,"Due to the small amount of named - entity annotated data , embeddings are used to extend , rather than replace , hand - crafted features in order to obtain state - of - the - art performance .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",1
6,Recent studies have explored methods for supplying deep sequential taggers with complementary features to standard embeddings .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",1
6,and tested special embeddings extracted from a neural language model ( LM ) trained on a large corpus .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",1
6,LM embeddings capture context - dependent aspects of word meaning using future ( forward LM ) and previous ( backward LM ) context words .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"When this information is added to standard features , it leads to significant improvements in NER .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"Also , showed that external knowledge resources ( namely gazetteers ) are crucial to NER performance .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,Gazetteer features encode the presence of word n-grams in predefined lists of NEs .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"In this work , we discuss some of the limitations of gazetteer features and propose an alternative lexical representation which is trained offline and that can be added to any neural NER system .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"Ina nutshell , we embed words and entity types into a joint vector space by leveraging WiFiNE , a ressource which automatically annotates mentions in Wikipedia with 120 entity types .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"From this vector space , we compute for each word a 120 - dimensional vector , where each dimension encodes the similarity of the word with an entity type .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"We call this vector an LS representation , for Lexical Similarity .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"When included in a vanilla LSTM - CRF NER model , LS representations lead to significant gains .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"We establish anew state - of - the - art F 1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance on the over - studied In the rest of this paper , we motivate our work in Section 2 .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,We describe how we compute LS vectors in Section 3 .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",1
6,"We present our system in Section 4 and report results in Section 5 . In Section 6 , we discuss related works before concluding in Section 7 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",1
6,Gazetteers are lists of entities that are associated with specific NE categories .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",1
6,"They are widely used as a feature source in NER , and have been successfully included in feature - based models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",1
6,"Typically , lists of entities are compiled from structured data sources such as DBpedia or Freebase .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",1
6,"The surface form of the title of a Wikipedia article , as well as aliases and redirects are mapped to an entity type using the object type attribute of the related DBpedia ( or Freebase ) page .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",1
6,"use this methodology to compile 30 lists of fine - grained entity types extracted from Wikipedia , while Chiu and Nichols ( 2016 ) create 4 gazetteers that map to CoNLL categories ( PER , LOC , ORG and MISC ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"Despite their importance , gazetteer - based features suffer from a number of limitations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,Binary representation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,Gazetteer features encode only the presence of an n-gram in each list and omit its relative frequency .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"For example , the word "" France "" can be used as a person , an organization , or a location , while it likely refers to the country most of the time .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,Binary features can not capture this preference .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,Generation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"At test time , we need to match every n-gram ( up to the length of the longest lexicon entry ) in a sentence against entries in the lexicons , which is time consuming .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"In their work , Chiu and Nichols ( 2016 ) use 4 lists that count over 2.3 M entries .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,Non-entity words .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"Gazetteer features do not capture signal from non-entity words , while earlier feature - based models strived to encode that some words ( or n-grams ) trigger specific entity types .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"For instance , words such as "" eat "" , "" directed "" or "" born "" are words that typically appear after a mention of type PER .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"To overcome those limitations , we propose an alternative approach where we embed annotations mined from Wikipedia into a vector space from which we compute a feature vector that represent words .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,This vector compactly and efficiently encodes both gazetteer and lexical information .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"Note that attest time , we only have to feed our model with this feature vector , which is efficient .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,Turning Wikipedia into a corpus of named - entities annotated with types is a task that received continuous attention over the years .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,It consists mainly in exploiting the hyperlink structure of Wikipedia in order to detect entity mentions .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"Then , structured data from a knowledge base ( for instance Freebase ) are used to map hyperlinks to entity types .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"Because the number of anchored strings in Wikipedia is no more than 3 % of the text tokens , proposed to augment Wikipedia articles with mentions unmarked in Wikipedia , thanks to a mix of heuristics that benefit the Wikipedia structure , as well as a coreference resolution system adapted specifically to Wikipedia .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"The authors applied their approach on English Wikipedia and produce coarse ( 4 classes ) and finegrained ( 120 labels ) named- entity annotations , leading to WiNER and WiFiNE .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"In this work , we adopt WiFiNE which is publicly available at http://rali.iro.umontreal.ca/rali/en/wifiner-wikipedia-for-et as our source of annotations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,Each entity mention is mapped ( via it s Freebase object type attribute ) to a pre-defined set of 120 entity types .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,Types are stored in a 2 - level hierarchical structure ( e.g. / person and / person / musician ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"The corpus consist of 3.2 M Wikipedia articles , comprising 1.3G tokens that we annotated with 157.4 M named - entity mentions and their types .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,We used this very large quantity of automatically annotated data for jointly embedding words and entity types into the same low - dimensional space .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,The key idea consists in learning an embedding for each entity type using its surrounding words .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"For instance , the embedding for / product / software will be trained using context words that surround all entities that were ( automatically ) labelled as / product / software in Wikipedia .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"In practice , we found that simply concatenating a sentence ( v1 ) with its annotated version ( v 2 ) , as illustrated in , offers a simple but efficient way of combining words and entity types so that embeddings can make good use of them .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,We use the FastText toolkit to learn the uncased embeddings for both words and entity types .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",1
6,"We train a skipgram model to learn 100 - dimensional vectors with a minimum word frequency cutoff of 5 , and a window size of 5 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",1
6,This configuration ( recommended by the authors ) performs the best in the experiments described in Section 5 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",1
6,"Since FastText learns representations of character ngrams , it has the ability to produce vectors for unknown words .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"For visualization proposes , we only plot single - word mentions that were annotated in WiFiNE with one of those 6 types .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,Words were randomly and proportionally sampled according to the frequency of each entity type .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"In addition , words have the color associated with the most frequent type they were annotated within WiFiNE .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,We observe that mentions often annotated by a given type in our resource tend to cluster around this entity type .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"For instance , "" firefox "" is close to the type / product / software , while "" enzyme "" is close to the / biology entity type .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,We also notice that words that are labelled with different types tend to appear between types they were annotated with .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"For instance , "" gpx2 "" , which is used both as a software and as a gene , has it s embedding in between / product / software and / biology .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"We inspected some of the words plotted in , and found that "" jrun "" and "" xp "" are incorrectly labelled as / product / weapon in WiFiNE .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"But since these words are seen in a software context , their embeddings are closer to the / product / software embedding than the / product / weapon one .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"We feel this tolerance to noise is a desirable feature , one that hopefully allows a more efficient use of distant supervision .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"Last , we also observe the tendency of rare words to cluster around their entity type .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"For instance , "" iota "" and "" x.org "" are embedded near their respective types , despite the fact that they appear less than 30 times in the version of Wikipedia used to compile WiFiNE .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"This joint vector space only serves the purpose of associating to each word a LS representation , that is , a 120 - dimensional vector where the ith coefficient is a value in the [ ? 1 , + 1 ] interval , equal to the cosine similarity 1 between the word embedding and the embedding of the ith entity type ( we have 120 types ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,Entity .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,shows the topmost similar entity types for proper names ( left column ) and common words ( right column ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,We observe that ambiguous mentions ( those annotated with several types ) are adequately handled .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"For instance , the LS representation of the word "" hilton "" encodes that it more often refers to a hotel or a restaurant than to an actress .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"Also , we observe that entity words that are either not or rarely annotated in WiFiNE are still adequately associated with their right type .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"For instance , "" dammstadt "" , which appears only 5 times in WiFiNE , and which refers to the Damm city in Germany , is most similar to / location / city and / location / railway .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"Interestingly , this mention does not have its page in English Wikipedia .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"Furthermore , we observe that non-entity context words have a strong similarity to types they precede or succeed .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"For instance the verb "" directed "" is very close to / person / director , an entity type that usually precedes it , and to / art / film , that usually follows it .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"Likewise , the preposition "" in "" is near / date and / location / city , which frequently follow "" in "" .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"This representation encodes the preference of an entity - mention word fora given type , an information out of reach of binary gazetteer features .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,It also lends itself nicely to the inclusion of lexical features that have been successfully used in earlier feature - based systems .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,Our representation does accommodate unfrequent words and seems tolerant to the inherent noise of distant supervision .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"In order to test the efficiency of our lexical feature representation , we implemented a state - of - the - art NER system we now describe .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"We adopt the popular Bi - LSTM - CRF architecture , a de facto baseline in many sequential tagging tasks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"In addition to the LS vector , we incorporate publicly available pre-trained embeddings , as well as character - level , and capitalization features .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,Those features have been shown to be crucial for stateof - the - art performance .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"We experimented with several publicly available word embeddings , such as Senna , Word2 Vec , GloVe , and SSKIP .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,We find that the latter performs the best in our experiments .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,SSKIP embeddings are 100 - dimensional case sensitive vectors that where trained using a n-skip - gram model on 42B tokens .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"These embeddings were previously used by , who report good performance on CONLL , and state - of - the - art results on ONTONOTES respectively .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,Note that these pre-trained embeddings are adjusted during training .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"Following , we use a forward and a backward LSTM to derive a representation of each word from its characters ( right part of .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"A character lookup table is randomly initialized , then trained at the same time as the Bi - LSTM model sketched in Section 4.1 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"Similarly to previous works , we use capitalization features for characterizing certain categories of capitalization patterns : all Upper , allLower , upperFirst , upperNotFirst , numeric or noAlphaNum .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"We define a random lookup table for these features , and learn its parameters during training .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"Contrarily to previous features , lexical vectors are computed offline and are not adjusted during training .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"We found useful in practice to apply a MinMax scaler in the range [ ? 1 , + 1 ] to each LS vector we computed ; thus , [.. , 0.095 , .. , 0.20 , .. , 0.76 , .. ] becomes [.. , ? 1 , .. , ? 0.67 , .. , 1 , ..].",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,CONLL-2003 and ONTONOTES 5.0 . provides an overview of the two datasets .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"As we can see , ONTONOTES is much larger .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"For both datasets , we convert the IOB encoding to BILOU , since found the latter to perform better .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"In keeping with others , we report mention - level F 1 score using the conlleval script 2 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,The ) is a well known collection of Reuters newswire articles that contains a large portion of sports news .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"It is annotated with four entity types : Person ( PER ) , Location ( LOC ) , Organization ( ORG ) and Miscellaneous ( MISC ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"The four entity types are fairly evenly distributed , and the train / dev / test datasets present a similar type distribution. , magazine ( 120 k ) , newswire ( 625 k ) , and web data ( 300 k ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"This dataset is annotated with 18 entity types , and is much larger than CONLL .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"Following previous researches , we use the official train / dev / test split of the CoNLL - 2012 shared task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"Also , we exclude ( both during training and testing ) the New Testaments portion as it does not contain gold NE annotations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,Training is carried out by mini-batch stochastic gradient descent ( SGD ) with a momentum of 0.9 and a gradient clipping of 5.0 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"The mini-batch is 10 for both datasets , and learning rates are 0.009 and 0.013 for CONLL and ONTONOTES respectively .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"More sophisticated optimization algorithms such as AdaDelta or Adam ( Kingma and Ba , 2014 ) converge faster , but none outperformed SGD with exponential learning rate decay in our experiments .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,Our system uses a single Bi - LSTM layer at the word level whose hidden dimensions are set to 128 and 256 for CONLL and ONTONOTES respectively .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"For both models , the character embedding size was set to 25 , and the hidden dimension of the forward and backward character LSTMs are set to 50 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"To mitigate overfitting , we apply a dropout mask with a probability of 0.5 on the input and output vectors of the Bi - LSTM layer .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"For both datasets , we set the dimension of capitalization embeddings to 25 and trained the models up to 50 epochs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"We tuned the hyper - parameters by grid search , and used early stopping based on the performance on the development set .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"We varied dropout . 65 ] ) , hidden units ) , capitalization ) and char ) embedding dimensions , learning rate ( [ 0.001 , 0.015 ] by step 0.002 ) , and optimization algorithms and fixed the other hyper - parameters .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"We implemented our system using the Tensorflow library , and ran our models on a GeForce GTX TITAN Xp GPU .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,Training requires about 2.5 hours for CONLL and 8 hours for ONTONOTES .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,shows the development set performance of our final models on each dataset compared to the work of .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"The authors use an architecture similar to ours , but use a binary gazetteer feature set , while we use our LS representation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"Since our systems involve random initialization , we report the mean as well as the standard deviation over five runs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"The improvements yielded by our model on the CONLL dataset are significant although modest , while those observed on ONTONOTES are more substantial .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,We also observe a lower variance of our system over the 5 runs .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"First , we observe that our model significantly outperforms models that use extensive sets of handcrafted features ) as well as the system of Standard deviation on the test set is reported in 2015 ) that uses NE and Entity Linking annotations to jointly optimize the performance on both tasks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"Second , our model outperforms as well other NN models that only use standard word embeddings , which indicates that our lexical feature vector is complementary to standard word embeddings .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"Third , our system matches state - of - the - art performances of models that use either more complex architectures or more elaborate features .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,use three layers of stacked residual RNN ( Bi - LSTM ) with bias decoding .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,Our model is much simpler and faster .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,They report a performance of 90.43 when using an architecture similar to ours .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,The two systems that have slightly higher F 1 scores on the CONLL dataset both use embeddings obtained from a forward and a backward Language Model trained on the One Billion Word Benchmark .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"They report gains between 0.8 and 1.2 points by using such LM embeddings , which suggests that LS vectors are indeed efficient .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"Unfortunately , due to time and resource constraints , 4 we were notable to measure whether both features complement each other .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,This is left for future investigations .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,reports the F 1 score of our system compared to the performance reported by others on the ONTONOTES test set .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"To the best of our knowledge , we surpass previously reported F 1 scores on this dataset .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"In particular , our system significantly outperforms the Bi - LSTM - CNN - CRF models of ( Chiu and Nichols , 2016 ) and by an absolute gain of 1.68 and 0.96 points respectively .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"Less surprisingly , it surpasses systems with hand - crafted features , including that use gazetteers , and the system of which uses coreference annotation in ONTONOTES to jointly model NER , entity linking , and coreference resolution tasks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,We also observe that models that use both feature sets significantly outperform other configurations .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",1
6,"To confirm that the gains came from our feature vector and not from increasing the number of hidden units , we tested several SSKIP models by increasing the LSTM hidden layer dimension so that number of parameters is the same as the model with LS vectors .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"We observed a degradation of performance on both datasets , mostly due to overfitting on the training set .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"From those results , we conclude that our lexical representation and the SSKIP one are complementary .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"In this experiment , we directly compare the LS representation with the SSKIP word - embedding feature set .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"In order to maintain a high level of performance , both character and capitalization features are used in all configurations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"We want to point out that LS vectors are not adapted during training , contrarily to the SSKIP embeddings .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"Similarly to Section 5.3 , we report in , for each feature configuration , the average F 1 score as well as the standard deviation over five runs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"We observe that on both CONLL and ONTONOTES , the SSKIP model outperforms our feature vector approach by 0.65 F1 points on average .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"The difference is not has high as we first expected , especially since the SSKIP model is adjusted during training , while our representation is not .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"Still , LS vectors seem to encode a large portion of the information needed to model the NER task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",1
6,"Also , it is worth mentioning that our embeddings are trained on 1.3B words compared to 42B for SSKIP .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",1
6,"Traditional approaches to NER , like CRF - based and Perceptron - based systems ( Ratinov and Roth , 2009 ) have dominated the field for over a decade .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",1
6,They rely heavily on hand - engineered features and external resources such as gazetteers .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",1
6,One major drawback of such an approach is its weak generalization power .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",1
6,"Current state - of - the art systems use a combination of Convolutional Neural Networks ( CNNs ) , Bi - LSTMs , along with a CRF decoder .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",1
6,"CNNs are used to encode character - level features ( prefix and suffix ) , while LSTM is used to encode word - level features .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",1
6,"Finally , a CRF is placed on top of those models in order to decode the best tag sequence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",1
6,Pre-trained embeddings obtained by unsupervised learning are core features of those models .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"In this work , we show that deep NN architectures can also benefit from lexical features , at least when encoded in the compact form we propose .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,and propose an alternative approach different from ours .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,They incorporate LM embeddings that were pre-trained on a large unlabelled corpus as features for NER .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,These embeddings allow to generate a representation fora word depending on its context .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"For instance , the LM embeddings of the word France in "" France is a developed country "" is different than that in "" Anatole France began his literary career "" .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,Such embeddings are trained on very large amount of texts .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"Our feature set is crafted from distant supervision applied to Wikipedia , a much less time - consuming process which we showed to be nevertheless adapted to rare words .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,Chiu and Nichols ( 2016 ) used gazetteer features in order to establish state - of - the - art performance on both CONLL and ONTONOTES .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,They mined DBPedia in order to compile 4 lists of named - entities that contain over 2.3 M entries .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,We show that LS representations outperform their gazetteer features .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,We have explored the idea of generating lexical features for NER out of Wikipedia data automatically annotated with fine - grained entity types .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"We used WiFiNE , a Wikipedia dump annotated with fine entity type mentions , for training a vector space that jointly embeds words and named - entities .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"This vector space is used to compute a 120 dimensional vector per word , which encodes the similarity of the word to each of the entity types .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"Our results show that our proposed lexical representation , even though it is not adjusted at training time , matches state - of - the - art results compared to more complex approaches on the well - studied CONLL dataset , and delivers anew state - of - the - art F 1 score of 87.95 on the more diversified ONTONOTES dataset .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,We further observe larger gains on collections with more unfrequent words .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"The source code and the data we used in this work are publicly available at http://rali.iro. umontreal.ca/rali/en/wikipedia-lex-sim , with the hope that other researchers will report gains , when using our lexical representation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
6,"As a future work , we want to investigate the usefulness of our LS feature representation on other NER tasks , including NER in tweets where out - of - vocabulary and low - frequency words represent a challenge ; as well as finer - grained NER which suffers from the lack of manually annotated training data .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/6/1806.03489v1-Stanza-out.txt,"Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features . While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers . In this work , we show that this is unfair : lexical features are actually quite useful . We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia . From this , we compute - offline - a feature vector representing each word . When used with a vanilla recurrent neural network model , this representation yields substantial improvements . We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset . This work is licensed under a Creative Commons Attribution 4.0 International License .",0
5,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"In order to democratize large - scale NLP and information extraction while minimizing our environmental footprint , we require fast , resource - efficient methods for sequence tagging tasks such as part - of - speech tagging and named entity recognition ( NER ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,Speed is not sufficient of course : they must also be expressive enough to tolerate the tremendous lexical variation in input data .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,The massively parallel computation facilitated by GPU hardware has led to a surge of successful neural network architectures for sequence labeling .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"While these models are expressive and accurate , they fail to fully exploit the parallelism opportunities of a GPU , and thus their speed is limited .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"Specifically , they employ either recurrent neural networks ( RNNs ) for feature extraction , or Viterbi inference in a structured output model , both of which require sequential computation across the length of the input .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"Instead , parallelized runtime independent of the length of the sequence saves time and energy costs , maximizing GPU resource usage and minimizing the amount of time it takes to train and evaluate models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,Convolutional neural networks ( CNNs ) provide exactly this property .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"Rather than composing representations incrementally over each token in a sequence , they apply filters in parallel across the entire sequence at once .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"Their computational cost grows with the number of layers , but not the input size , up to the memory and threading limitations of the hardware .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"This provides , for example , audio generation models that can be trained in parallel ( van den .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"Despite the clear computational advantages of CNNs , RNNs have become the standard method for composing deep representations of text .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"This is because a token encoded by a bidirectional RNN will incorporate evidence from the entire input sequence , but the CNN 's representation is limited by the effective input width 1 of the network : the size of the input context which is observed , directly or indirectly , by the representation of a token at a given layer in the network .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,1 ) + 1 . The number of layers required to incorporate the entire input context grows linearly with the length of the sequence .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"To avoid this scaling , one could pool representations across the sequence , but this is not appropriate for sequence labeling , since it reduces the output resolution of the representation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"In response , this paper presents an application of dilated convolutions for sequence labeling ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"For dilated convolutions , the effective input width can grow exponentially with the depth , with no loss in resolution at each layer and with a modest number of parameters to estimate .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"Like typical CNN layers , dilated convolutions operate on a sliding window of context over the sequence , but unlike conventional convolutions , the context need not be consecutive ; the dilated window skips over every dilation width d inputs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,The size of the effective input width fora token at layer l is now given by 2 l +1 ?1 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"More concretely , just four stacked dilated convolutions of width 3 produces token representations with an effective input width of 31 tokens - longer than the average sentence length ( 23 ) in the Penn TreeBank .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,Our overall iterated dilated CNN architecture ( ID - CNN ) repeatedly applies the same block of dilated convolutions to token - wise representations .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,This parameter sharing prevents overfitting and also provides opportunities to inject supervision on intermediate activations of the network .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"Similar to models that use logits produced by an RNN , the ID - CNN provides two methods for performing prediction : we can predict each token 's label independently , or by running Viterbi inference in a chain structured graphical model .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,A dilated CNN block with maximum dilation width 4 and filter width 3 .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,Neurons contributing to a single highlighted neuron in the last layer are also highlighted .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"5.0 English NER , we demonstrate significant speed gains of our ID - CNNs over various recurrent models , while maintaining similar F1 performance .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"When performing prediction using independent classification , the ID - CNN consistently outperforms a bidirectional LSTM ( Bi - LSTM ) , and performs on par with inference in a CRF with logits from a Bi - LSTM ( Bi - LSTM - CRF ) .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"As an extractor of per-token logits fora CRF , our model out - performs the Bi - LSTM - CRF .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"We also apply ID - CNNs to entire documents , where independent token classification is as accurate as the Bi - LSTM - CRF while decoding almost 8 faster .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,The clear accuracy gains resulting from incorporating broader context suggest that these models could similarly benefit many other contextsensitive NLP tasks which have until now been limited by the computational complexity of existing context - rich models .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"Let x = [ x 1 , . . . , x T ] be our input text and y = [ y 1 , . . . , y T ] be per-token output tags .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,Let D be the domain size of each y i .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"We predict the most likely y , given a conditional model P ( y|x ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,This paper considers two factorizations of the conditional distribution .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,where the tags are conditionally independent given some features for x .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"Given these features , O ( D ) prediction is simple and parallelizable across the length of the sequence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"However , feature extraction may not necessarily be parallelizable .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"For example , RNN - based features require iterative passes along the length of x .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"p is a pairwise factor that scores consecutive tags , and Z x is the partition function .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,p does not depend on the timestep tor the input x in our experiments .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,Viterbi algorithm .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"CRF prediction explicitly reasons about interactions among neighboring output tags , whereas prediction in the first model compiles this reasoning into the feature extraction step .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,The suitability of such compilation depends on the properties and quantity of the data .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"While CRF prediction requires non-trivial search in output space , it can guarantee that certain output constraints , such as for IOB tagging , will always be satisfied .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"It may also have better sample complexity , as it imposes more prior knowledge about the structure of the interactions among the tags .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"However , it has worse computational complexity than independent prediction .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"CNNs in NLP are typically one - dimensional , applied to a sequence of vectors representing tokens rather than to a two -dimensional grid of vectors representing pixels .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"In this setting , a convolutional neural network layer is equivalent to applying an affine transformation , W c to a sliding window of width r tokens on either side of each token in the sequence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"Here , and throughout the paper , we do not explicitly write the bias terms in affine transformations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,is vector concatenation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,is the dilation width .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,A dilated convolution of width 1 is equivalent to a simple convolution .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"Using the same number of parameters as a simple convolution with the same radius ( i.e. W c has the same dimensionality ) , the ? > 1 dilated convolution incorporates broader context into the representation of a token than a simple convolution .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,We can leverage the ability of dilated convolutions to incorporate global context without losing important local information by stacking dilated convolutions of increasing width .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"First described for pixel classification in computer vision , achieve state - of - the - art results on image segmentation benchmarks by stacking dilated convolutions with exponentially increasing rates of dilation , a technique they refer to as multiscale context aggregation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"By feeding the outputs of each dilated convolution as the input to the next , increasingly non-local information is incorporated into each pixel 's representation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,Performing a dilation - 1 convolution in the first layer ensures that no pixels within the effective input width of any pixel are excluded .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"By doubling the dilation width at each layer , the size of the effective input width grows exponentially while the number of parameters grows only linearly with the number of layers , so a pixel representation quickly incorporates rich global evidence from the entire image .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"Unfortunately , simply increasing the depth of stacked dilated CNNs causes considerable overfitting in our experiments .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"In response , we present Iterated Dilated CNNs ( ID - CNNs ) , which instead apply the same small stack of dilated convolutions multiple times , each iterate taking as input the result of the last application .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,Repeatedly employing the same parameters in a recurrent fashion provides both broad effective input width and desirable generalization capabilities .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"We also obtain significant accuracy gains with a training objective that strives for accurate labeling after each iterate , allowing follow - on iterations to observe and resolve dependency violations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,t of Eqn..,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,We denote the jth dilated convolutional layer of dilation width ? as D ( j ) ? .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"Next , L c layers of dilated convolutions of exponentially increasing dilation width are applied to it , folding in increasingly broader context into the embedded representation of x tat each layer .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,Let r ( ) denote the ReLU activation function .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"We refer to this stack of dilated convolutions as a block B ( ) , which has output resolution equal to its input resolution .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"To incorporate even broader context without over - fitting , we avoid making B deeper , and instead iteratively apply B L b times , introducing no extra parameters .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"Our main focus is to apply the ID - CNN an encoder to produce per-token logits for the first conditional model described in Sec. 2.1 , where tags are conditionally independent given deep features , since this will enable prediction that is parallelizable across the length of the input sequence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"We can also use the ID - CNN as logits for the CRF model ( Eqn. ( 2 ) ) , where the partition function and its gradient are computed using the forward - backward algorithm .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,We next present an alternative training method that helps bridge the gap between these two techniques .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,Sec. 2.1 identifies that the CRF has preferable sample complexity and accuracy since prediction directly reasons in the space of structured outputs .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"In response , we compile some of this reasoning in output space into ID - CNN feature extraction .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"Instead of explicit reasoning over output labels during inference , we train the network such that each block is predictive of output labels .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"Subsequent blocks learn to correct dependency violations of their predecessors , refining the final sequence prediction .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"To do so , we first define predictions of the model after each of the L b applications of the block .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"Let ht ( k ) be the result of applying the matrix W o from ( 9 ) to b t ( k ) , the output of block k.",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"By rewarding accurate predictions after each application of the block , we learn a model where later blocks are used to refine initial predictions .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,The loss also helps reduce the vanishing gradient problem for deep architectures .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"Such an approach has been applied in a variety of contexts for training very deep networks in computer vision , but not to our knowledge in NLP .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,We apply dropout to the raw inputs x t and to each block 's output b t ( b ) to help prevent overfitting .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,The version of dropout typically used in practice has the undesirable property that the randomized predictor used at train time differs from the fixed one used attest time .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"present dropout with expectationlinear regularization , which explicitly regularizes these two predictors to behave similarly .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,All of our best reported results include such regularization .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"This is the first investigation of the technique 's effectiveness for NLP , including for RNNs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,We encourage its further application .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"The state - of - the art models for sequence labeling include an inference step that searches the space of possible output sequences of a chain - structured graphical model , or approximates this search with abeam .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"These outperform similar systems that use the same features , but independent local predictions .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"On the other hand , the greedy sequential prediction ) approach of , which employs lexicalized features , gazetteers , and word clusters , outperforms CRFs with similar features .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,LSTMs were used for NER as early as the CoNLL shared task in 2003 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"More recently , a wide variety of neural network architectures for NER have been proposed .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"employ a one - layer CNN with pre-trained word embeddings , capitalization and lexicon features , and CRF - based prediction .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"achieved state - of - the - art accuracy on partof - speech , chunking and NER using a Bi - LSTM - CRF. proposed two models which incorporated Bi - LSTM - composed character embeddings alongside words : a Bi - LSTM - CRF , and a greedy stack LSTM which uses a simple shift - reduce grammar to compose words into labeled entities .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,Their Bi - LSTM - CRF obtained the state - of - the - art on four languages without word shape or lexicon features .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"use CNNs rather than LSTMs to compose characters in a Bi - LSTM - CRF , achieving state - of the - art performance on part - of - speech tagging and CoNLL NER without lexicons .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"Chiu and Nichols ( 2016 ) evaluate a similar network but propose a novel method for encoding lexicon matches , presenting results on CoNLL and OntoNotes NER .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,use GRU - CRFs with GRUcomposed character embeddings of words to train a single network on many tasks and languages .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"In general , distributed representations for text can provide useful generalization capabilities for NER systems , since they can leverage unsupervised pre-training of distributed word representations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"Though our models would also likely benefit from additional features such as character representations and lexicons , we focus on simpler models which use word - embeddings alone , leaving more elaborate input representations to future work .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"In these NER approaches , CNNs were used for low - level feature extraction that feeds into alternative architectures .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"Overall , end - to - end CNNs have mainly been used in NLP for sentence classification , where the output representation is lower resolution than that of the input Kim Our work draws on the use of dilated convolutions for image segmentation in the computer vision community .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"Similar to our block , Yu and Koltun ( 2016 ) employ a context - module of stacked dilated convolutions of exponentially increasing dilation width .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"Dilated convolutions were recently applied to the task of speech generation , and concurrent with this work , posted a pre-print describing the similar ByteNet network for machine translation that uses dilated convolutions in the encoder and decoder components .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"Our basic model architecture is similar to that of the ByteNet encoder , except that the inputs to our model are tokens and not bytes .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"Additionally , we present a novel loss and parameter sharing scheme to facilitate training models on much smaller datasets than those used by .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,We are the first to use dilated convolutions for sequence labeling .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,The broad effective input width of the ID - CNN helps aggregate document - level context .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"incorporate document context in their greedy model by adding features based on tagged entities within a large , fixed window of tokens .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,Prior work has also posed a structured model that couples predictions across the whole document .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,We describe experiments on two benchmark English named entity recognition datasets .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"On CoNLL - 2003 English NER , our ID - CNN performs on par with a Bi - LSTM not only when used to produce per-token logits for structured inference , but the ID - CNN with greedy decoding also performs on - par with the Bi - LSTM - CRF while running at more than 14 times the speed .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"We also observe a performance boost in almost all models when broadening the context to incorporate entire documents , achieving an average F1 of 90.65 on CoNLL - 2003 , out - performing the sentence - level model while still decoding at nearly 8 times the speed of the Bi - LSTM - CRF .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,We evaluate using labeled data from the CoNLL - 2003 shared task ( Tjong and OntoNotes 5.0 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"Following previous work , we use the same OntoNotes data split used for co-reference resolution in the CoNLL - 2012 shared task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"For both datasets , we convert the IOB boundary encoding to BILOU as previous work found this encoding to result in improved performance .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,As in previous work we evaluate the performance of our models using segment - level micro -averaged F1 score .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,Hyperparameters that resulted in the best performance on the validation set were selected via grid search .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"A more detailed description of the data , evaluation , optimization and data pre-processing can be found in the Appendix .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"We compare our ID - CNN against strong LSTM and CNN baselines : a Bi - LSTM with local decoding , and one with CRF decoding ( Bi - LSTM - CRF ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,We also compare against a non-dilated CNN architecture with the same number of convolutional layers as our dilated network ( 4 - layer CNN ) and one with enough layers to incorporate an effective input width of the same size as that of the dilated network ( 5 - layer CNN ) to demonstrate that the dilated convolutions more effectively aggregate contextual information than simple convolutions ( i.e. using fewer parameters ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"We also compare our document - level ID - CNNs to a baseline which does not share parameters between blocks ( noshare ) and one that computes loss only at the last block , rather than after every iterated block of dilated convolutions ( 1 - loss ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"1 ) Fast train and test performance are highly desirable for NLP practitioners , and deeper models require more computation time 2 ) more complicated models tend to over - fit on this relatively small dataset and 3 ) most accurate deep CNN architectures repeatedly up - sample and down - sample the inputs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,We do not compare to stacked LSTMs for similar reasons - a single LSTM is already slower than a 4 - layer CNN .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"Since our task is sequence labeling , we desire a model that maintains the token - level resolution of the input , making dilated convolutions an elegant solution .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,6.3 CoNLL - 2003 English NER 6.3.1 Sentence - level prediction lists F 1 scores of models predicting with sentence - level context on CoNLL - 2003 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"For models that we trained , we report F1 and standard deviation obtained by averaging over 10 random restarts .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"The Viterbi - decoding Bi - LSTM - CRF and ID - CNN - CRF and greedy ID - CNN obtain the highest average scores , with the ID - CNN - CRF outperforming the Bi - LSTM - CRF by 0.11 points of F1 on average , and the Bi - LSTM - CRF out - performing the greedy ID - CNN by 0.11 as well .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"Our greedy ID - CNN outperforms the Bi - LSTM and the 4 - layer CNN , which uses the same number of parameters as the ID - CNN , and performs similarly to the 5 - layer CNN which uses more parameters but covers the same effective input width .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"All CNN models out - perform the Bi-Model F1 86.96 90.33 Bi-LSTM 89.34 0.28 4 - layer CNN 89.97 0.20 5 - layer CNN 90.23 0.16 ID- CNN 90.32 0.26 88.67 90.05 90.20 Bi-LSTM-CRF ( re-impl ) 90.43 0.12 ID-CNN- CRF 90.54 0.18 LSTM when paired with greedy decoding , suggesting that CNNs are better token encoders than Bi - LSTMs for independent logistic regression .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"When paired with Viterbi decoding , our ID - CNN performs on par with the Bi - LSTM , showing that the ID - CNN is also an effective token encoder for structured inference .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,Our ID - CNN is not only a better token encoder than the Bi - LSTM but it is also faster .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"lists relative decoding times on the CoNLL development set , compared to the Bi - LSTM - CRF .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,We report decoding times using the fastest batch size for each method .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,The ID - CNN model decodes nearly 50 % faster than the Bi - LSTM .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"With Viterbi decoding , the gap closes somewhat but the ID - CNN - CRF still comes out ahead , about 30 % faster than the Bi - LSTM - CRF .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"The most vast speed improvements come when comparing the greedy ID - CNN to the Bi - LSTM - CRF - our ID - CNN is more than 14 times faster than the Bi - LSTM - CRF attest time , with comparable accuracy .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"The 5 - layer CNN , which observes the same effective input width as the ID - CNN but with more parameters , performs at about the same speed as the ID - CNN in our experiments .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"With a better implementation of dilated convolutions than currently included in TensorFlow , we would expect the ID - CNN to be notably faster than the 5 - layer CNN .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"We emphasize the importance of the dropout regularizer of in , where we observe increased F1 for every model trained with expectation - linear dropout regularization .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"Dropout is important for training neural network models that generalize well , especially on relatively small NLP datasets such as .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,We recommend this regularizer as a simple and helpful tool for practitioners training neural networks for NLP .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,In we show that adding document - level context improves every model on CoNLL - 2003 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"Incorporating document - level context further improves our greedy ID - CNN model , attaining 90.65 average F1 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",0
5,"We believe this model sees greater improvement with the addition of document - level context than the Bi - LSTM - CRF due to the ID - CNN learning a feature function better suited for representing broad context , in contrast with the Bi - LSTM which , though better than a simple RNN at encoding long memories of sequences , may reach its limit when provided with sequences more than 1,000 tokens long such as entire documents .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,We also note that our combination of training objective ( Eqn. 11 ) and tied parameters ( Eqn. : Comparing ID - CNNs with 1 ) backpropagating loss only from the final layer ( 1 - loss ) and 2 ) untied parameters across blocks ( noshare ) 8 ) more effectively learns to aggregate this broad context than a vanilla cross - entropy loss or deep CNN back - propagated from the final neural network layer .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,compares models trained to incorporate entire document context using the document baselines described in Section 6.2 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"In we show that , in addition to being more accurate , our ID - CNN model is also much faster than the Bi - LSTM - CRF when incorporating context from entire documents , decoding at almost 8 times the speed .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"On these long sequences , it also tags at more than 4.5 times the speed of the greedy Bi - LSTM , demonstrative of the benefit of our ID - CNNs context - aggregating computation that does not depend on the length of the sequence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,We observe similar patterns on OntoNotes as we do on CoNLL. lists overall F 1 scores of our models compared to those in the existing literature .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,Speed Bi-LSTM-CRF 1 Bi-LSTM 4.60 ID- CNN 7.96 85.76 0.13 21.19 ID-CNN 85.27 0.24 13.21 ID - CNN ( 1 block ) 84.28 0.10 26.01 : F1 score of sentence and document models on OntoNotes .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"icalized greedy model of , and our ID - CNN out - performs the Bi - LSTM as well as the more complex model of which leverages the parallel coreference annotation available in the OntoNotes corpus to predict named entities jointly with entity linking and co-reference .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"Our greedy model is out - performed by the Bi - LSTM - CRF reported in Chiu and Nichols ( 2016 ) as well as our own re-implementation , which appears to be the new state - of - the - art on this dataset .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,The gap between our greedy model and those using Viterbi decoding is wider than on CoNLL .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"We believe this is due to the more diverse set of entities in OntoNotes , which also tend to be much longer - the average length of a multi-token named entity segment in CoNLL is about one token shorter than in OntoNotes .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,These long entities benefit more from explicit structured constraints enforced in Viterbi decoding .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"Still , our ID - CNN outperforms all other greedy methods , achieving our goal of learning a better token encoder for structured prediction .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"Incorporating greater context significantly boosts the score of our greedy model on OntoNotes , whereas the Bi - LSTM - CRF performs more poorly .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"In , we also list the F1 of our ID - CNN model and the Bi - LSTM - CRF model trained on entire document context .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"For the first time , we seethe score decrease when more context is added to the Bi - LSTM - CRF model , though the ID - CNN , whose sentence model a lower score than that of the Bi - LSTM - CRF , sees an increase .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"We believe the decrease in the Bi - LSTM - CRF model occurs because of the nature of the OntoNotes dataset compared to contains a particularly high proportion of ambiguous entities , 7 perhaps leading to more benefit from document context that helps with disambiguation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"In this scenario , adding the wider context may just add noise to the high - scoring Bi - LSTM - CRF model , whereas the less accurate dilated model can still benefit from the refined predictions of the iterated dilated convolutions .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"We present iterated dilated convolutional neural networks , fast token encoders that efficiently aggregate broad context without losing resolution .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"These provide impressive speed improvements for sequence labeling , particularly when processing entire documents at a time .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
5,"In the future we hope to extend this work to NLP tasks with richer structured output , such as parsing .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/2/1702.02098v3-Stanza-out.txt,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",1
