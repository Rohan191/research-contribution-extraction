doc_num,sentence,target,doc_path,abstract,predicted
7,We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,NSE can also access 1 multiple and shared memories .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,"In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,"For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,[ 1 ] .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,"[ 2 ] have achieved a notable success in sequential tasks [ 3 , 4 ] .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,LSTM is powerful because it learns to control it s short term memories .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"However , the short term memories in LSTM area part of the training parameters .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,This imposes some practical difficulties in training and modeling long sequences with LSTM .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,Recently several studies have explored ways of extending the neural networks with an external memory [ 5 ] [ 6 ] [ 7 ] .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"Unlike LSTM , the short term memories and the training parameters of such a neural network are no longer coupled and can be adapted .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,In this paper we propose a novel class of memory augmented neural networks called Neural Semantic Encoders ( NSE ) for natural language understanding .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,NSE offers several desirable properties .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,NSE has a variable sized encoding memory which allows the model to access entire input sequence during the reading process ; therefore efficiently delivering long - term dependencies overtime .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"The encoding memory evolves overtime and maintains the memory of the input sequence through read , compose and write operations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,NSE sequentially processes the input and supports word compositionality inheriting both temporal and hierarchical nature of human language .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,NSE can read from and write to a set of relevant encoding memories simultaneously or multiple NSEs can access a shared encoding memory effectively supporting knowledge and representation sharing .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,"NSE is flexible , robust and suitable for practical NLU tasks and can be trained easily by any gradient descent optimizer .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,We evaluate NSE on five different real tasks .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"For four of them , our models set new state - of - theart results .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,Our results suggest that a NN model with the shared memory between encoder and decoder is a promising approach for sequence transduction problems such as machine translation and abstractive summarization .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,"In particular , we observe that the attention - based neural machine translation can be further improved by shared - memory models .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,We also analyze memory access pattern and compositionality in NSE and show that our model captures semantic and syntactic structures of input sentence .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,"By access we mean changing the memory states by the read , compose and write operations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,One of the pioneering work that attempts to extend deep neural networks with an external memory is Neural Turing Machines ( NTM ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,NTM implements a centralized controller and a fixed - sized random access memory .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The NTM memory is addressable by both content ( i.e. soft attention ) and location based access mechanisms .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The authors evaluated NTM on algorithmic tasks such as copying and sorting sequences .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,Comparison with Neural Turing Machines : NSE addresses certain drawbacks of NTM .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"NTM has a single centralized controller , which is usually an MLP or RNN while NSE takes a modular approach .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"The main controller in NSE is decomposed into three separate modules , each of which performs for read , compose or write operation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"In NSE , the compose module is introduced in addition to the standard memory update operations ( i.e. read - write ) in order to process the memory entries and input information .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The main advantage of NSE over NTM is in its memory update .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"Despite its sophisticated addressing mechanism , the NTM controller does not have mechanism to avoid information collision in the memory .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,Particularly the NTM controller emits two separate set of access weights ( i.e. read weight and erase and write weights ) that do not explicitly encode the knowledge about where information is read from and written to .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,Moreover the fixed - size memory in NTM has no memory allocation or de-allocation protocol .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"Therefore unless the controller is intelligent enough to track the previous read / write information , which is hard for an RNN when processing long sequences , the memory content is overlapped and information is overwritten throughout different time scales .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,We think that this is a potential reason that makes NTM hard to train and makes the training not stable .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,We also note that the effectiveness of the location based addressing introduced in NTM is unclear .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"In NSE , we introduce a novel and systematic memory update approach based on the soft attention mechanism .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,NSE writes new information to the most recently read memory locations .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,This is accomplished by sharing the same memory key vector between the read and write modules .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,The NSE memory update is scalable and potentially more robust to train .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,"NSE is provided with a variable sized memory and thus unlike NTM , the size of the NSE memory is more relaxed .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,The novel memory update mechanism and the variable sized memory together prevent NSE from the information collision issue and avoid the need of the memory allocation or de-allocation protocols .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,Each memory location of the NSE memory stores a token representation in input sequence during encoding .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"This provides NSE with an anytime - access to the entire input sequence including the tokens from the future time scales , which is not permitted in NTM , RNN and attention - based encoders .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"Lastly , NTM addresses small algorithmic problems while NSE focuses on a set of large - scale language understanding tasks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,The RNNSearch model proposed in can be seen as a variation of memory augmented networks due to its ability to read the historic output states of RNNs with soft attention .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,The work of combines the soft attention with Memory Networks ( Mem NNs ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,"Similar to RNNSearch , MemNNs are designed with non-writable memories .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,It constructs layered memory representations and showed promising results on both artificial and real question answering tasks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,We note that RNNSearch and MemNNs avoid the memory update and management overhead by simply using a non-writable memory storage .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,Another variation of MemNNs is Dynamic Memory Network that is equipped with an episodic memory and seems to be flexible in different settings .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"Although NSE differs from other memory - augumented NN models in many aspects , they all use soft attention mechanism with a type of similarity measures to retrieve relevant information from the external memory .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"For example , NTM implements cosine similarity and MemNNs use vector dot product .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,NSE uses the vector dot product for the similarity measure in NSE because it is faster to compute .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"Other related work includes Neural Program - Interpreters , which learns to run sub-programs and to compose them for high - level programs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,It uses execution traces to provide the full supervision .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,Researchers have also explored ways to add unbounded memory to LSTM using a particular data structure .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"Although this type of architecture provides a flexible capacity to store information , the memory access is constrained by the data structure used for the memory bank , such as stack and queue .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,Overall it is expensive to train and to scale the previously proposed memory - based models .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,Most models required a set of clever engineering tricks to work successfully .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,Most of the aforementioned memory augmented neural networks have been tested on synthetic tasks whereas in this paper we evaluated NSE on a wide range of real and large - scale natural language applications .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,". . , w i Ti of tokens while the output Y i can be either a single target or a sequence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,We transform each input token wt to its word embedding x t .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"R kl with a variable number of slots , where k is the embedding dimension and l is the length of the input sequence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,R k corresponds to the vector representation of information about word wt in memory .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"In particular , the memory is initialized by the embedding vectors {x t } l t=1 and is evolved overtime , through read , compose and write operations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,NSE performs three main operations in every time step .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"After initializing the memory slots with the corresponding input representations , NSE processes an embedding vector x t and retrieves a memory slot m r,t that is expected to be associatively coherent ( i.e. semantically associated ) with the current input word wt .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The slot location r ( ranging from 1 to l ) is defined by a key vector z t which the read module emits by attending over the memory slots .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The compose module implements a composition operation that combines the memory slot with the current input .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The write module then transforms the composition output to the encoding memory space and writes the resulting new representation into the slot location of the memory .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,denotes the outer product which duplicates it s left vector l or k times to form a matrix .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,Mr sequentially maps the word embeddings to the internal space of the memory M t?1 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,Then Equation 2 looks for the slots related to the input by computing association degree between each memory slot and the hidden state o t .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,We calculate the association degree by the dot product and transform this scores to the fuzzy key vector z t by normalizing with sof tmax function .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"Since our key vector is fuzzy , the slot to be composed is retrieved by taking weighted sum of the all slots as in Equation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,This process can also be seen as the soft attention mechanism .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"In Equation 4 and 5 , we compose and process the retrieved slot with the current hidden state and map the resulting vector to the encoder output space .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"Finally , we write the new representation to the memory location pointed by the key vector in where the key vector z t emitted by the read module is reused to inform the write module of the most recently read slots .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,First the slot information that was retrieved is erased and then the new representation is located .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,NSE performs this iterative process until all words in the input sequence are read .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The encoding memories { M } T t=1 and output states {h} T t=1 are further used for the tasks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"Although NSE reads a single word at a time , it has an anytime - access to the entire sequence stored in the encoding memory .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"With the encoding memory , NSE maintains a mental image of the input sequence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The memory is initialized with the raw embedding vector at time t = 0 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,We term such a freshly initialized memory a baby memory .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"As NSE reads more input content in time , the baby memory evolves and refines the encoded mental image .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,functions are neural networks and are the training parameters in our NSE .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"As the name suggests , we use LSTM and multi -layer perceptron ( MLP ) in this paper .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"Since NSE is fully differentiable , it can be trained with any gradient descent optimizer .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"For sequence to sequence transduction tasks like question answering , natural language inference and machine translation , it is beneficial to access other relevant memories in addition to its own one .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The shared or the multiple memory access allows a set of NSEs to exchange knowledge representations and to communicate with each other to accomplish a particular task throughout the encoding memory .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"NSE can be extended easily , so that it is able to read from and write to multiple memories simultaneously or multiple NSEs are able to access a shared memory .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,( b ) depicts a high - level architectural diagram of a multiple memory access - NSE ( MMA - NSE ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,The first memory ( in green ) is the shared memory accessed by more than one NSEs .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,and this is almost the same as standard NSE .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The read module now emits the additional key vector z n t for the shared memory and the composition function f M LP c combines more than one slots .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"In MMA - NSE , the different memory slots are retrieved from the shared memories depending on their encoded semantic representations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,They are then composed together with the current input and written back to their corresponding slots .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,Note that MMA - NSE is capable of accessing a variable number of relevant shared memories once a composition function that takes in dynamic inputs is chosen .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"We describe in this section experiments on five different tasks , in order to show that NSE can be effective and flexible in different settings .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"We report results on natural language inference , question answering ( QA ) , sentence classification , document sentiment analysis and machine translation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,All five tasks challenge a model in terms of language understanding and semantic reasoning .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The models are trained using Adam with hyperparameters selected on development set .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,We chose two one - layer LSTM for read / write modules on the tasks other than QA on which we used two - layer LSTM .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,The pre-trained 300 - D Glove 840B vectors and 100 - D Glove 6B vectors were obtained for the word embeddings .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,The word embeddings are fixed during training .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,The embeddings for out - of - vocabulary words were set to zero vector .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,We crop or pad the input sequence to a fixed length .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,A padding vector was inserted when padding .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The models were regularized by using dropouts and an l 2 weight decay .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The natural language inference is one of the main tasks in language understanding .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,This task tests the ability of a model to reason about the semantic relationship between two sentences .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"In order to perform well on the task , NSE should be able to capture sentence semantics and be able to reason the relation between a sentence pair , i.e. , whether a premise - hypothesis pair is entailing , contradictory or neutral .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"We conducted experiments on the Stanford Natural Language Inference ( SNLI ) dataset , which consists of 549,367/9,842/9,824 premise-hypothesis pairs for train / dev / test sets and target label indicating their relation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,h h land elementwise product hp l h h l of the two sentence representations .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"In addition , the MLP has a hidden layer with 1024 units with ReLU activation and a sof tmax layer .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"We set the batch size to 128 , the initial learning rate to 3e - 4 and l 2 regularizer strength to 3 e - 5 , and train each model for 40 epochs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The write / read neural nets and the last linear layer were regularized by using 30 % dropouts .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,We evaluated three different variations of NSE show in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The NSE model encodes each sentence simultaneously by using a separate memory for each sentence .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,The second model - MMA - NSE first encodes the premise and then the hypothesis sentence by sharing the premise encoded memory in addition to the hypothesis memory .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,"For the third model , we use inter-sentence attention which selectively reconstructs the premise representation .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,shows the results of our models along with the results of published methods for the task .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The classifier with handcrafted features extracts a set of lexical features .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The next group of models are based on sentence encoding .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"While most of the sentence encoder models rely solely on word embeddings , the dependency tree CNN and the SPINN - PI models make use of sentence parser output .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,The SPINN - PI model is similar to NSE in spirit that it also explicitly computes word composition .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,"However , the composition in the SPINN - PI is guided by supervisions from a dependency parser .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,NSE outperformed the previous sentence encoders on this task .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"The MMA - SNE further slightly improved the result , indicating that reading the premise memory is helpful while encoding the hypothesis .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The last set of methods designs inter-sentence relation with parameterized soft attention .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,Our MMA - NSE attention model is similar to the LSTM attention model .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"Particularly , it attends over the premise encoder outputs {h p } T t= 1 in respect to the final hypothesis representation h h land constructs an attentively blended vector of the premise .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,This model obtained 85.4 % accuracy score .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The best performing model for this task performs tree matching with attention mechanism and LSTM .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,Answer sentence selection is an integral part of the open - domain question answering .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"For this task , a model is trained to identify the correct sentences that answer a factual question , from a set of candidate sentences .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,We experiment on WikiQA dataset constructed from Wikipedia .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"The dataset contains 20,360/2,733/6,165 QA pairs for train / dev / test sets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"The MLP setup used in the language inference task is kept same , except that we now replace the sof tmax layer with a sigmoid layer and model the following conditional probability distribution .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,where h q land ha l are the question and the answer encoded vectors and o QA denotes the output of the hidden layer of the MLP .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,We trained the MMA - NSE attention model to minimize the sigmoid cross entropy loss .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,MMA - NSE first encodes the answers and then the questions by accessing its own and the answer encoding memories .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"In our preliminary experiment , we found that the multiple memory access and the attention over answer encoder outputs {h a } T t= 1 are crucial to this problem .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"Following previous work , we adopt MAP and MRR as the evaluation metrics for this task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"We set the batch size to 4 and the initial learning rate to 1 e - 5 , and train the model for 10 epochs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,We used 40 % dropouts afterword embeddings and no l 2 weight decay .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The word embeddings are pre-trained 300 - D Glove 840B vectors .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"For this task , a linear mapping layer transforms the 300 - D word embeddings to the 512- D LSTM inputs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,presents the results of our model and the previous models for the task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The classifier with handcrafted features is a SVM model trained with a set of features .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The Bigram - CNN model is a simple convolutional neural net .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"While the LSTM and LSTM attention models outperform the previous best result by nearly 5 - 6 % by implementing deep LSTM with three hidden layers , NASM improves it further and sets a strong baseline by combining variational auto - encoder with the soft attention .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,Our MMA - NSE attention model exceeds the NASM by approximately 1 % on MAP and 0.8 % on MRR for this task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,Bigram- CNN 0.6190 0.6281 3 - layer LSTM 0.6552 0.6747 3 - layer LSTM attention 0.6639 0.6828 NASM 0.6705 0.6914 MMA - NSE attention 0.6811 0.6993 88.1 47.4 DRNN 86.6 49.8 2 - layer LSTM 86.3 46.0 Bi-LSTM 87.5 49.1 CT- LSTM 88.0 51.0 DMN 88.6 52.1 NSE 89.7 52.8 : Test accuracy for sentence classification .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"Binary , FG : fine - grained 5 classes .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,We evaluated NSE on the Stanford Sentiment Treebank ( SST ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,This dataset comes with standard train / dev / test sets and two subtasks : binary sentence classification or fine - grained classification of five classes .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,We trained our model on the text spans corresponding to labeled phrases in the training set and evaluated the model on the full sentences .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The sentence representations were passed to a two - layer MLP for classification .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The first layer of the MLP has ReLU activation and 1024 or 300 units for binary or fine - grained setting .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The second layer is a sof tmax layer .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The read / write modules are two one - layer LSTM with 300 hidden units and the word embeddings are the pre-trained 300 - D Glove 840B vectors .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,"We set the batch size to 64 , the initial learning rate to 3e - 4 and l 2 regularizer strength to 3 e - 5 , and train each model for 25 epochs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,The write / read neural nets and the last linear layer were regularized by 50 % dropouts .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,compares the result of our model with the state - of - the - art methods on the two subtasks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,Most best performing methods exploited the parse tree provided in the treebank on this task with the exception of the DMN .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The Dynamic Memory Network ( DMN ) model is a memory - augmented network .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,Our model outperformed the DMN and set the state - of - the - art results on both subtasks .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"We evaluated our models for document - level sentiment analysis on two publically available largescale datasets : the IMDB consisting of 335,018 movie reviews and 10 different classes and Yelp 13 consisting of 348,415 restaurant reviews and 5 different classes .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,Each document in the datasets is associated with human ratings and we used these ratings as gold labels for sentiment classification .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,"Particularly , we used the pre-split datasets of .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,We stack a NSE or LSTM on the top of another NSE for document modeling .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,The first NSE encodes the sentences and the second NSE or LSTM takes sentence encoded outputs and constructs document representations .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,The document representation is given to a output sof tmax layer .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The whole network is trained jointly by backpropagating the cross entropy loss .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,We used one - layer LSTM with 100 hidden units for the read / write modules and the pre-trained 100 - D Glove 6B vectors for this task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"We set the batch size to 32 , the initial learning rate to 3e - 4 and l 2 regularizer strength to 1 e - 5 , and trained each model for 50 epochs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The write / read neural nets and the document - level NSE / LSTM were regularized by 15 % dropouts and the softmax layer by 20 % dropouts .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"In order to speedup the training , we created document buckets by considering the number of sentences per document , i.e. , documents with the same number of sentences were put together in the same bucket .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The buckets were shuffled and updated per epoch .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,"We did not use curriculum scheduling , although it is observed to help sequence training .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,shows our results .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,We report two performance metrics : accuracy and MSE .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"The best results on the task were previously obtained by Conv - GRNN and LSTM - GRNN , which are also stacked models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,These models first learn the sentence representations with a CNN or LSTM and then combine them for document representation using a gated recurrent neural network ( GRNN : BLEU scores for English - German translation task .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,Yelp 13 dataset has five classes to distinguish .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The stacked NSEs ( NSE - NSE ) performed slightly better than the NSE - LSTM on the IMDB dataset .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,This is possibly due to the encoding memory of the document level NSE that preserves the long dependency in documents with a large number of sentences .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"Lastly , we conducted an experiment on neural machine translation ( NMT ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The NMT problem is mostly defined within the encoder - decoder framework .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The encoder provides the semantic and syntactic information about the source sentences to the decoder and the decoder generates the target sentences by conditioning on this information and its partially produced translation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"For an efficient encoding , the attention - based NTM was introduced .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"For NTM , we implemented three different models .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The first model is a baseline model and is similar to the one proposed in ( RNNSearch ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"This model ( LSTM - LSTM ) has two LSTM for the encoder / decoder and has the soft attention neural net , which attends over the source sentence and constructs a focused encoding vector for each target word .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The second model is an NSE - LSTM encoder - decoder which encodes the source sentence with NSE and generates the targets with the LSTM network by using the NSE output states and the attention network .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"The last model is an NSE - NSE setup , where the encoding part is the same as the NSE - LSTM while the decoder NSE now uses the output state and has an access to the encoder memory , i.e. , the encoder and the decoder NSEs access a shared memory .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The memory is encoded by the first NSEs and then read / written by the decoder NSEs .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,We used the English - German translation corpus from the IWSLT 2014 evaluation campaign .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,The corpus consists of sentence - aligned translation of TED talks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,The data was pre-processed and lowercased with the Moses toolkit .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,Word association or composition graphs produced by NSE memory access .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The directed arcs connect the words that are composed via compose module .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The source nodes are input words and the destination nodes ( pointed by the arrows ) correspond to the accessed memory slots .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,< S > denotes the beginning of sequence .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,the number of parameters of the models is roughly the equal .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The models were trained to minimize word - level cross entropy loss and were regularized by 20 % input dropouts and the 30 % output dropouts .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,"We set the batch size to 128 , the initial learning rate to 1e - 3 for LSTM - LSTM and 3e - 4 for the other models and l 2 regularizer strength to 3 e - 5 , and train each model for 40 epochs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,We report BLEU score for each models .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,NSE is capabable of performing multiscale composition by retrieving associative slots fora particular input at a time step .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,We analyzed the memory access order and the compositionality of memory slot and the input word in the NSE model trained on the SNLI data .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,shows the word association graphs for the two sentence picked from SNLI test set .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,The association graph was constructed by inspecting the key vector z .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,"For an input word , we connect it to the most active slot pointed by z 12 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"Note the graph components clustered around the semantically rich words : "" sits "" , "" wall "" and "" autumn "" ( a ) and "" Three "" , "" puppies "" , "" tub "" and "" vet "" ( b ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The memory slots corresponding to words that are semantically rich in the current context are the most frequently accessed .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"The graph is able to capture certain syntactic structures including phrases ( e.g. , "" hand built rock wall "" ) and modifier relations ( between "" sits "" and "" quietly "" and between "" tub "" and "" sprayed with water "" ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,Another interesting property is that the model tends to perform sensible compositions while processing the input sentence .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"For example , NSE retrieved the memory slot corresponding to "" wall "" or "" Three "" when reading the input "" rock "" or "" are "" .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"A , we show a step - by - step visualization of NSE memory states for the first sentence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,Note how the encoding memory is evolved overtime .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"In time step four ( t = 4 ) , the memory slot for "" quietly "" encodes information about "" quiet ( ly ) little child "" .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"When t = 6 , the model forms another composition involving "" quietly "" , "" quietly sits "" .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"In the last time step , we are able to find the most or the least frequently accessed slots in the memory .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,The least accessed slots correspond to function words while the frequently accessed slots are content words and tend to carryout rich semantics and intrinsic compositions found in the input sentence .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,Overall the model is less constrained and is able to compose multiword expressions .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,Our proposed memory augmented neural networks have achieved the state - of - the - art results when evaluated on five representative NLP tasks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"NSE is capable of building an efficient architecture of the single , shared and multiple memory accesses fora specific NLP task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"For example , for the NLI task NSE accesses premise encoded memory when processing hypothesis .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"For the QA task , NSE accesses answer encoded memory when reading question for QA .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"In machine translation , NSE shares a single encoded memory between encoder and decoder .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,Such flexibility in the architectural choice of the NSE memory access allows for the robust models fora better performance .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,The initial state of the NSE memory stores information about each word in the input sequence .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",1
7,We in this paper used word embeddings to represent the words in the memory .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,Different variations of word representations such as character - based models are left to be evaluated for memory initialization in the future .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,We plan to extend NSE so that it learns to select and access a relevant subset from a memory set .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
7,"One could also explore unsupervised variations of NSE , for example , to train them to produce encoding memory and representation vector of entire sentences or documents using either new or existing models such as the skip - gram model .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/7/1607.04315v3-Stanza-out.txt,"We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders . NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations . NSE can also access 1 multiple and shared memories . In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks . For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",0
1,Understanding unstructured text is a major goal within natural language processing .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,Comprehension tests pose questions based on short text passages to evaluate such understanding .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,"In this work , we investigate machine comprehension on the challenging MCTest benchmark .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,"Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,"We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,"The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Humans learn in a variety of ways - by communication with each other , and by study , the reading of text .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Comprehension of unstructured text by machines , at a near- human level , is a major goal for natural language processing .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,It has garnered significant attention from the machine learning research community in recent years .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,Machine comprehension ( MC ) is evaluated by posing a set of questions based on a text passage ( akin to the reading tests we all took in school ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,"Such tests are objectively gradable and can be used to assess a range of abilities , from basic understanding to causal reasoning to inference .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,"Given a text passage and a question about its content , a system is tested on its ability to determine the correct answer .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,"In this work , we focus on MCTest , a complex but data - limited comprehension benchmark , whose multiple - choice questions require not only extraction but also inference and limited reasoning .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Inference and reasoning are important human skills that apply broadly , beyond language .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,We present a parallel - hierarchical approach to machine comprehension designed to work well in a data - limited setting .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"There are many use-cases in which comprehension over limited data would be handy : for example , user manuals , internal documentation , legal contracts , and soon .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Moreover , work towards more efficient learning from any quantity of data is important in its own right , for bringing machines more inline with the way humans learn .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Typically , artificial neural networks require numerous parameters to capture complex patterns , and the more parameters , the more training data is required to tune them .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Likewise , deep models learn to extract their own features , but this is a data - intensive process .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,Our model learns to comprehend at a high level even when data is sparse .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,The key to our model is that it compares the question and answer candidates to the text using several distinct perspectives .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,We refer to a question combined with one of its answer candidates as a hypothesis ( to be detailed below ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"The semantic perspective compares the hypothesis to sentences in the text viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"The word - by - word perspective focuses on similarity matches between individual words from hypothesis and text , at various scales .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"As in the semantic perspective , we consider matches over complete sentences .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"We also use a sliding window acting on a subsentential scale ( inspired by the work of ) , which implicitly considers the linear distance between matched words .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Finally , this word - level sliding window operates on two different views of text sentences : the sequential view , where words appear in their natural order , and the dependency view , where words are reordered based on a linearization of the sentence 's dependency graph .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,Words are represented throughout by embedding vectors .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,These distinct perspectives naturally form a hierarchy that we depict in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Language is hierarchical , so it makes sense that comprehension relies on hierarchical levels of understanding .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,The perspectives of our model can be considered a type of feature .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"However , they are implemented by parametric differentiable functions .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,"This is in contrast to most previous efforts on MCTest , whose numerous hand - engineered features can not be trained .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,"Our model , significantly , can be trained end - to - end with backpropagation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,"To facilitate learning with limited data , we also develop a unique training scheme .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,We initialize the model 's neural networks to perform specific heuristic functions that yield decent ( thought not impressive ) performance on the dataset .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Thus , the training scheme gives the model a safe , reasonable baseline from which to start learning .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,We call this technique training wheels .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,Computational models that comprehend ( insofar as they perform well on MC datasets ) have developed contemporaneously in several research groups .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Models designed specifically for MCTest include those of , and more recently , , and .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"In experiments , our Parallel - Hierarchical model achieves state - of - the - art accuracy on MCTest , outperforming these existing methods .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Below we describe related work , the mathematical details of our model , and our experiments , then analyze our results .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"In this section we borrow from , who laid out the MC problem nicely .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,Machine comprehension requires machines to answer questions based on unstructured text .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,This can be viewed as selecting the best answer from a set of candidates .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"In the multiple - choice case , candidate answers are predefined , but candidate answers may also be undefined yet restricted ( e.g. , to yes , no , or any noun phrase in the text ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"For each question q , let T be the unstructured text and A = {a i } the set of candidate answers to q .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,The machine comprehension task reduces to selecting the answer that has the highest evidence given T .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"As in , we combine an answer and a question into a hypothesis , hi = f ( q , a i ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"To facilitate comparisons of the text with the hypotheses , we also breakdown the passage into sentences t j , T = {t j }.",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"In our setting , q , a i , and t j each represent a sequence of embedding vectors , one for each word and punctuation mark in the respective item .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,Machine comprehension is currently a hot topic within the machine learning community .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"In this section we will focus on the best - performing models applied specifically to MCTest , since it is somewhat unique among MC datasets ( see Section 5 ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Generally , models can be divided into two categories : those that use fixed , engineered features , and neural models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,The bulk of the work on MCTest falls into the former category .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,"Manually engineered features often require significant effort on the part of a designer , and / or various auxiliary tools to extract them , and they can not be modified by training .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,"On the other hand , neural models can be trained end - to - end and typically harness only a single feature : vectorrepresentations of words .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,Word embeddings are fed into a complex and possibly deep neural network which processes and compares text to question and answer .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Among deep models , mechanisms of attention and working memory are common , as in and .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"3.1 Feature - engineering models treated MCTest as a structured prediction problem , searching fora latent answerentailing structure connecting question , answer , and text .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,This structure corresponds to the best latent alignment of a hypothesis with appropriate snippets of the text .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,The process of ( latently ) selecting text snippets is related to the attention mechanisms typically used in deep networks designed for MC and machine translation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,The model uses event and entity coreference links across sentences along with a host of other features .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,These include specifically trained word vectors for synonymy ; antonymy and class - inclusion relations from external database sources ; dependencies and semantic role labels .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"The model is trained using a latent structural SVM extended to a multitask setting , so that questions are first classified using a pretrained top - level classifier .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,This enables the system to use different processing strategies for different question categories .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,The model also combines question and answer into a well - formed statement using the rules of Cucerzan and Agichtein ( 2005 ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Our model is simpler than that of in terms of the features it takes in , the training procedure ( stochastic gradient descent vs. alternating minimization ) , question classification ( we use none ) , and question - answer combination ( simple concatenation or mean vs. a set of rules ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"augmented the baseline feature set from with features for syntax , frame semantics , coreference chains , and word embeddings .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,They combined features using a linear latent - variable classifier trained to minimize a max - margin loss function .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"As in , questions and answers are combined using a set of manually written rules .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"The method of achieved the previous state of the art , but has significant complexity in terms of the feature set .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Space does not permit a full description of all models in this category , but see also and .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Despite its relative lack of features , the Parallel - Hierarchical model improves upon the featureengineered state of the art for MCTest by a small amount ( about 1 % absolute ) as detailed in Section 5 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Neural models have , to date , performed relatively poorly on MCTest .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,This is because the dataset is sparse and complex .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,investigated deep - learning approaches concurrently with the present work .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"They measured the performance of the Attentive Reader and the Neural Reasoner , both deep , end - to - end recurrent models with attention mechanisms , and also developed an attention - based convolutional network , the HABCNN .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Their network operates on a hierarchy similar to our own , providing further evidence of the promise of hierarchical perspectives .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Specifically , the HABCNN processes text at the sentence level and the snippet level , where the latter combines adjacent sentences ( as we do through an n-gram input ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,Embedding vectors for the question and the answer candidates are combined and encoded by a convolutional network .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"This encoding modulates attention over sentence and snippet encodings , followed by maxpooling to determine the best matches between question , answer , and text .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"As in the present work , matching scores are given by cosine similarity .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,The HABCNN also makes use of a question classifier .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Despite the shared concepts between the HABCNN and our approach , the Parallel - Hierarchical model performs significantly better on MCTest ( more than 15 % absolute ) as detailed in Section 5 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,Other neural models tested in fare even worse .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,Let us now define our machine comprehension model in full .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"We first describe each of the perspectives separately , then describe how they are combined .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Below , we use subscripts to index elements of sequences , like word vectors , and superscripts to indicate whether elements come from the text , question , or answer .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"In particular , we use the subscripts k , m , n , p to index sequences from the text , question , answer , and hypothesis , respectively , and superscripts t , q , a , h. We depict the model schematically in .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,The semantic perspective is similar to the Memory Networks approach for embedding inputs into memory space .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,"The matrix At ? R Dd , the bias vector b t A ? RD , and for f we use the leaky ReLU function .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,k is a trainable weight associated to each word in the vocabulary .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,These scalar weights implement a kind of exogenous or bottomup attention that depends only on the input stimulus .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"They can , for example , learn to perform the function of stopword lists in a soft , trainable way , to nullify the contribution of unimportant filler words .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"The semantic representation of a hypothesis is formed analogously , except that we combine the question word vectors q m and answer word vectors an as a single sequence {h p } = {q m , an }.",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,RD .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,These transformations map a text sentence and a hypothesis into a common space where they can be compared .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"We compute the semantic match be-tween text sentence and hypothesis using the cosine similarity , M sem = cos ( s t , sh ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"The first step in building the word - by - word perspective is to transform word vectors from a text sentence , question , and answer through respective neural functions .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,RD and f is again the leaky ReLU .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,We transform the question and the answer toq m and n analogously using distinct matrices and bias vectors .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"In contrast with the semantic perspective , we keep the question and answer candidates separate in the wordby - word perspective .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"This is because matches to answer words are inherently more important than matches to question words , and we want our model to learn to use this property .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Inspired by the work of in paraphrase detection , we compute matches between hypotheses and text sentences at the word level .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"ca kn = cos (t k , n ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,m is the word weight for the question word and Z normalizes these weights to sum to one over the question .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"We define the match between a sentence and answer candidate , M a , analogously .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,are trainable parameters that control the relative importance of the terms .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,The sequential sliding window is related to the original MCTest baseline by .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Our sliding window decays from its focus word according to a Gaussian distribution , which we extend by assigning a trainable weight to each location in the window .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,This modification enables the window to use information about the distance between word matches ; the original baseline used distance information through a predefined function .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"The sliding window scans over the words of the text as one continuous sequence , without sentence breaks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Each window is treated like a sentence in the previous subsection , but we include a location - based weight ?( k ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"This weight is based on a word 's position in the window , which , given a window , depends on its global position k.",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,for the question and analogously for the answer .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,We initialize the location weights with a Gaussian and fine - tune them during training .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"The final matching score , denoted as M sws , is computed as in and with sq km replacing c q km .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"The dependency sliding window operates identically to the linear sliding window , but on a different view of the text passage .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,The output of this component is M swd and is formed analogously to M sws .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,The dependency perspective uses the Stanford Dependency Parser as an auxiliary tool .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Thus , the dependency graph can be considered a fixed feature .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Moreover , linearization of the dependency graph , because it relies on an eigendecomposition , is not differentiable .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,"However , we handle the linearization in data preprocessing so that the model sees only reordered word - vector inputs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,"Specifically , we run the Stanford Dependency Parser on each text sentence to build a dependency graph .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,"This graph has n w vertices , one for each word in the sentence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,R nwnw and determine its eigenvectors .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,The second eigenvector u 2 of the Laplacian is known as the Fiedler vector .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,ij is the weight of the edge from vertex i to vertex j.,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"The Fiedler vector maps a weighted graph onto a line such that connected nodes stay close , modulated by the connection weights .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,This enables us to reorder the words of a sentence based on their proximity in the dependency graph .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,The reordering of the words is given by the ordered index set I = arg sort ( u 2 ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Jenny , Mrs. Mustard 's helper , called the police .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"the police , called Jenny helper , Mrs. 's Mustard .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,Who called the police ? with Mrs. Mustard .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"The dependency reordering enables the window to determine the correct answer , Jenny .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,It is important in comprehension to synthesize information found throughout a document .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"MCTest was explicitly designed to ensure that it could not be solved by lexical techniques alone , but would instead require some form of inference or limited reasoning .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,It therefore includes questions where the evidence for an answer spans several sentences .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"To perform synthesis , our model also takes in ngrams of sentences , i.e. , sentence pairs and triples strung together .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"The model treats these exactly as it does single sentences , applying all functions detailed above .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,A later pooling operation combines scores across all n-grams ( including the singlesentence input ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,This is described in the next subsection .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"With n-grams , the model can combine information distributed across contiguous sentences .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"In some cases , however , the required evidence is spread across distant sentences .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"To give our model some capacity to deal with this scenario , we take the top N sentences as scored by all the preceding functions , and then repeat the scoring computations viewing these top N as a single sentence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,The reasoning behind these approaches can be explained well in a probabilistic setting .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"If we consider our similarity scores to model the likelihood of a text sentence given a hypothesis , p (t j |h i ) , then the n-gram and top N approaches model a joint probability p (t j 1 , t j 2 , . . . , t j k |h i ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,We can not model the joint probability as a product of individual terms ( score values ) because distributed pieces of evidence are likely not independent .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,"We use a multilayer perceptron to combine M sem , M word , M swd , and M sws as a final matching score M i for each answer candidate .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"This network also pools and combines the separate n-gram scores , and uses a linear activation function .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"where is a constant margin , i * indexes the correct answer , and we take the maximum over i so that we are ranking the correct answer over the best - ranked incorrect answer ( of which there are three ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,This approach worked better than comparing the correct answer to the incorrect answers individually as in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Our implementation of the Parallel - Hierarchical model , using the Keras framework , is available on Github .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Before training , we initialized the neural - network components of our model to perform sensible heuristic functions .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,Training did not converge on the small MCTest without this vital approach .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Empirically , we found that we could achieve above 50 % accuracy on MCTest using a simple sum of word vectors followed by a dot product between the question sum and the hypothesis sum .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Therefore , we initialized the network for the semantic perspective to perform this sum , by initializing A x as the identity matrix and bx A as the zero vector , x ? {t , h} .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,Recall that the activation function is a ReLU so that positive outputs are unchanged .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"We also found basic word - matching scores to be helpful , so we initialized the word - by - word networks likewise .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"The network for perspectivecombination was initialized to perform a sum of individual scores , using a zero bias - vector and a weight matrix of ones , since we found that each perspective contributed positively to the overall result .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,This training wheels approach is related to other techniques from the literature .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"For instance , proposed the identity - matrix initialization in the context of recurrent neural networks in order to preserve the error signal through backpropagation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"In residual networks , shortcut connections bypass certain layers in the network so that a simpler function can be trained in conjunction with the full model .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"MCTest is a collection of 660 elementary - level children 's stories and associated questions , written by human subjects .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"The stories are fictional , ensuring that the answer must be found in the text itself , and carefully limited to what a young child can understand .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,The more challenging variant consists of 500 stories with four multiple - choice questions each .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Despite the elementary level , stories and questions are more natural and more complex than those found in synthetic MC datasets like bAb I and CNN .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,MCTest is challenging because it is both complicated and small .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,"It s size limits the number of parameters that can be trained , and prevents learning any complex language modeling simultaneously with the capacity to answer questions .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,In this section we describe important details of the training procedure and model setup .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"For a complete list of hyperparameter settings , our stopword list , and other minutiae , we refer interested readers to our Github repository .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"For word vectors we use Google 's publicly available embeddings , trained with word2vec on the 100 - billion - word News corpus .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"These vectors are kept fixed throughout training , since we found that training them was not helpful ( likely because of MCTest 's size ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,The vectors are 300 - dimensional ( d = 300 ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"We do not use a stopword list for the text passage , instead relying on the trainable word weights to ascribe global importance ratings to words .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,These weights are initialized with the inverse document frequency ( IDF ) statistic computed over the MCTest corpus .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,"However , we douse a short stopword list for questions .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"This list nullifies query words such as { Who , what , when , where , how} , along with conjugations of the verbs to do and to be .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Following earlier methods , we use a heuristic to improve performance on negation questions .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"When a question contains the words which and not , we negate the hypothesis ranking scores so that the minimum becomes the maximum .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,The most important technique for training the model was the training wheels approach .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Without this , training was not effective at all .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,The identity initialization requires that the network weight matrices are square ( d = D ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"We found dropout to be particularly effective at improving generalization from the training to the test set , and used 0.5 as the dropout probability .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Dropout occurs after all neural - network transformations , if those transformations are allowed to change with training .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,Our best performing model held networks at the wordby - word level fixed .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"For combining distributed evidence , we used up to trigrams over sentences and our bestperforming model reiterated over the top two sentences ( N = 2 ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"We used the Adam optimizer with the standard settings ( Kingma and Ba , 2014 ) and a learning rate of 0.003 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,To determine the best hyperparameters we performed a grid search over 150 settings based on validation - set accuracy .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"MCTest 's original validation set is too small for reliable hyperparameter tuning , so , following , we merged the training and validation sets of MCTest - 160 and MCTest - 500 , then split them randomly into a 250 - story training set and a 200 - story validation set .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,presents the performance of featureengineered and neural methods on the MCTest test set .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Accuracy scores are divided among questions whose evidence lies in a single sentence ( single ) and across multiple sentences ( multi ) , and among the two variants .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Clearly , MCTest - 160 is easier .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,The first three rows represent featureengineered methods .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,RTE is the best - performing variant of the original baseline published along with MCTest .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"It uses a lexical sliding window and distance - based measure , augmented with rules for recognizing textual entailment .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,We described the methods of and in Section 3 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"On MCTest - 500 , the Parallel Hierarchical model significantly outperforms these methods on single questions ( > 2 % ) and slightly outperforms the latter two on multi questions ( ? 0.3 % ) and overall ( ? 1 % ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,The method of achieves the best overall result on MCTest - 160 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,We suspect this is because our neural method suffered from the relative lack of training data .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,The last four rows in are neural methods that we discussed in Section 3 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,Performance measures are taken from .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,Here we see our model outperforming the alternatives by a large margin across the board ( > 15 % ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"The Neural Reasoner and the Attentive Reader are large , deep models with hundreds of thousands of parameters , so it is unsurprising that they performed poorly on MCTest .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"The specificallydesigned HABCNN fared better , its convolutional architecture cutting down on the parameter count .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Because there are similarities between our model and the HABCNN , we hypothesize that much of the performance difference is attributable to our training wheels methodology .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,We measure the contribution of each component of the model by ablating it .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,Results are given in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Not surprisingly , the n-gram functionality is important , contributing almost 5 % accuracy improvement .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,MCTest - 500 accuracy ( % ) Single means for synthesizing distributed evidence .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"The top N function contributes very little to the overall performance , suggesting that most multi questions have their evidence distributed across contiguous sentences .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Ablating the sentential component made the most significant difference , reducing performance by more than 5 % .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,Simple word - by - word matching is obviously useful on MCTest .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,"The sequential sliding window makes a 3 % contribution , highlighting the importance of word - distance measures .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,"On the other hand , the dependency - based sliding window makes only a minor contribution .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,We found this surprising .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,It maybe that linearization of the dependency graph removes too much of its information .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Finally , the exogenous word weights make a significant contribution of almost 5 % .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Analysis reveals that most of our system 's test failures occur on questions about quantity ( e.g. , How many ...? ) and temporal order ( e.g. , Who was invited last ? ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Quantity questions makeup 9.5 % of our errors on the validation set , while order questions makeup 10.3 % .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,"This weakness is not unexpected , since our architecture lacks any capacity for counting or tracking temporal order .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Incorporating mechanisms for these forms of reasoning is a priority for future work ( in contrast , the Memory Network model is quite good at temporal reasoning ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,The Parallel - Hierarchical model is simple .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,It does no complex language or sequence modeling .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,It s simplicity is a response to the limited data of MCTest .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Nevertheless , the model achieves stateof - the - art results on the multi questions , which ( putatively ) require some limited reasoning .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,Our model is able to handle them reasonably well just by stringing important sentences together .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Thus , the model imitates reasoning with a heuristic .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,"This suggests that , to learn true reasoning abilities , MCTest is too simple a dataset - and it is almost certainly too small for this goal .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,"However , it maybe that human language processing can be factored into separate processes of comprehension and reasoning .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",1
1,"If so , the Parallel - Hierarchical model is a good start on the former .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Indeed , if we train the method exclusively on single questions then its results become even more impressive : we can achieve a test accuracy of 79.1 % on MCTest - 500 .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"We have presented the novel Parallel - Hierarchical model for machine comprehension , and evaluated it on the small but complex MCTest .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Our model achieves state - of - the - art results , outperforming several feature - engineered and neural approaches .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Working with our model has emphasized to us the following ( not necessarily novel ) concepts , which we record hereto promote further empirical validation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,Good comprehension of language is supported by hierarchical levels of understanding ( Cf. ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,Exogenous attention ( the trainable word weights ) maybe broadly helpful for NLP .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"The training wheels approach , that is , initializing neural networks to perform sensible heuristics , appears helpful for small datasets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
1,"Reasoning over language is challenging , but easily simulated in some cases .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/question-answering/4/1603.08884v1-Stanza-out.txt,"Understanding unstructured text is a major goal within natural language processing . Comprehension tests pose questions based on short text passages to evaluate such understanding . In this work , we investigate machine comprehension on the challenging MCTest benchmark . Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features . We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy . The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set . Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text . When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",0
4,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",1
4,"Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",1
4,"Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",1
4,We opensource our pretrained models and code 1 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",1
4,Inductive transfer learning has had a large impact on computer vision ( CV ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"Applied CV models ( including object detection , classification , and segmentation ) are rarely trained from scratch , but instead are fine - tuned from models that have been pretrained on ImageNet , MS - COCO , and other datasets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"Text classification is a category of Natural Language Processing ( NLP ) tasks with real - world applications such as spam , fraud , and bot detection , emergency response , and commercial document classification , such as for legal discovery .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,1 http://nlp.fast.ai/ulmfit.,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,Equal contribution .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"Jeremy focused on the algorithm development and implementation , Sebastian focused on the experiments and writing .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"While Deep Learning models have achieved state - of - the - art on many NLP tasks , these models are trained from scratch , requiring large datasets , and days to converge .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,Research in NLP focused mostly on transductive transfer .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"For inductive transfer , fine - tuning pretrained word embeddings , a simple transfer technique that only targets a model 's first layer , has had a large impact in practice and is used inmost state - of - the - art models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"Recent approaches that concatenate embeddings derived from other tasks with the input at different layers ) still train the main task model from scratch and treat pretrained embeddings as fixed parameters , limiting their usefulness .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"In light of the benefits of pretraining , we should be able to do better than randomly initializing the remaining parameters of our models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"However , inductive transfer via finetuning has been unsuccessful for NLP .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"first proposed finetuning a language model ( LM ) but require millions of in - domain documents to achieve good performance , which severely limits its applicability .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,We show that not the idea of LM fine - tuning but our lack of knowledge of how to train them effectively has been hindering wider adoption .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,LMs overfit to small datasets and suffered catastrophic forgetting when fine - tuned with a classifier .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"Compared to CV , NLP models are typically more shallow and thus require different fine - tuning methods .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,The same 3 - layer LSTM architecturewith the same hyperparameters and no additions other than tuned dropout hyperparametersoutperforms highly engineered models and trans - fer learning approaches on six widely studied text classification tasks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"On IMDb , with 100 labeled examples , ULMFiT matches the performance of training from scratch with 10 and - given 50 k unlabeled examples - with 100 more data .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"1 ) We propose Universal Language Model Fine - tuning ( ULMFiT ) , a method that can be used to achieve CV - like transfer learning for any task for NLP .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"2 ) We propose discriminative fine - tuning , slanted triangular learning rates , and gradual unfreezing , novel techniques to retain previous knowledge and avoid catastrophic forgetting during fine - tuning .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"3 ) We significantly outperform the state - of - the - art on six representative text classification datasets , with an error reduction of 18 - 24 % on the majority of datasets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,4 ) We show that our method enables extremely sample - efficient transfer learning and perform an extensive ablation analysis .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,5 ) We make the pretrained models and our code available to enable wider adoption .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,Transfer learning in CV Features in deep neural networks in CV have been observed to transition from general to task - specific from the first to the last layer .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"For this reason , most work in CV focuses on transferring the first layers of the model .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,Sharif achieve state - of - theart results using features of an Image Net model as input to a simple classifier .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"In recent years , this approach has been superseded by fine - tuning either the last or several of the last layers of a pretrained model and leaving the remaining layers frozen .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"In NLP , only recently have methods been proposed that go beyond transferring word embeddings .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,The prevailing approach is to pretrain embeddings that capture additional context via other tasks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"Embeddings at different levels are then used as features , concatenated either with the word embeddings or with the inputs at intermediate layers .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"This method is known as hypercolumns in CV 2 and is used by , who use language modeling , paraphrasing , entailment , and Machine Translation ( MT ) respectively for pretraining .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",1
4,"require engineered custom architectures , while we show state - of - the - art performance with the same basic architecture across a range of tasks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",1
4,"In CV , hypercolumns have been nearly entirely superseded by end - to - end fine - tuning .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,A related direction is multi-task learning ( MTL ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,This is the approach taken by and who add a language modeling objective to the model that is trained jointly with the main task model .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"MTL requires the tasks to be trained from scratch every time , which makes it inefficient and often requires careful weighting of the taskspecific objective functions .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"Fine- tuning has been used successfully to transfer between similar tasks , e.g. in QA , for distantly supervised sentiment analysis , or MT domains but has been shown to fail between unrelated ones .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"also fine - tune a language model , but overfit with 10 k labeled examples and require millions of in - domain documents for good performance .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"In contrast , ULMFiT leverages general - domain pretraining and novel finetuning techniques to prevent overfitting even with only 100 labeled examples and achieves state - of the - art results also on small datasets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",1
4,"Given a static source task T Sand any target task T T with T S = T T , we would like to improve performance on T T .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",1
4,"It captures many facets of language relevant for downstream tasks , such as long - term dependencies , hierarchical relations , and sentiment .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"In contrast to tasks like and entailment , it provides data in near- unlimited quantities for most domains and languages .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,The full LM is fine - tuned on target task data using discriminative fine - tuning ( ' Discr ' ) and slanted triangular learning rates ( STLR ) to learn task - specific features .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"The classifier is fine - tuned on the target task using gradual unfreezing , ' Discr ' , and STLR to preserve low - level representations and adapt high - level ones ( shaded : unfreezing stages ; black : frozen ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"task , which we show significantly improves performance ( see Section 5 ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"Moreover , language modeling already is a key component of existing tasks such as MT and dialogue modeling .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"Formally , language modeling induces a hypothesis space H that should be useful for many other NLP tasks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"We propose Universal Language Model Finetuning ( ULMFiT ) , which pretrains a language model ( LM ) on a large general - domain corpus and fine - tunes it on the target task using novel techniques .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"1 ) It works across tasks varying in document size , number , and label type ; 2 ) it uses a single architecture and training process ; 3 ) it requires no custom feature engineering or preprocessing ; and 4 ) it does not require additional in - domain documents or labels .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"In our experiments , we use the state - of - theart language model AWD - LSTM , a regular LSTM ( with no attention , short - cut connections , or other sophisticated additions ) with various tuned dropout hyperparameters .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"Analogous to CV , we expect that downstream performance can be improved by using higherperformance language models in the future .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"ULMFiT consists of the following steps , which we show in : a) General - domain LM pretraining ( 3.1 ) ; b ) target task LM fine - tuning ( 3.2 ) ; and c ) target task classifier fine - tuning ( 3.3 ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,We discuss these in the following sections .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,An Image Net - like corpus for language should be large and capture general properties of language .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"We pretrain the language model on Wikitext - 103 consisting of 28,595 preprocessed Wikipedia articles and 103 million words .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,Pretraining is most beneficial for tasks with small datasets and enables generalization even with 100 labeled examples .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"We leave the exploration of more diverse pretraining corpora to future work , but expect that they would boost performance .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"While this stage is the most expensive , it only needs to be performed once and improves performance and convergence of downstream models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"No matter how diverse the general - domain data used for pretraining is , the data of the target task will likely come from a different distribution .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,We thus fine - tune the LM on data of the target task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",1
4,"Given a pretrained general - domain LM , this stage converges faster as it only needs to adapt to the idiosyncrasies of the target data , and it allows us to train a robust LM even for small datasets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",1
4,"We propose discriminative fine - tuning and slanted triangular learning rates for fine - tuning the LM , which we introduce in the following .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",1
4,"As different layers capture different types of information , they should be fine - tuned to different extents .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"To this end , we propose a novel fine - tuning method , discriminative fine - tuning 3 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"Instead of using the same learning rate for all layers of the model , discriminative fine - tuning allows us to tune each layer with different learning rates .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,J ( ? ) is the gradient with regard to the model 's objective function .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,l contains the parameters of the model at the l - th layer and L is the number of layers of the model .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,l is the learning rate of the l - th layer .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,l?1 = ? l / 2.6 as the learning rate for lower layers .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"For adapting its parameters to task - specific features , we would like the model to quickly converge to a suitable region of the parameter space in the beginning of training and then refine its parameters .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,Using the same learning rate ( LR ) or an annealed learning rate throughout training is not the best way to achieve this behaviour .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"In other words , the number of epochs times the number of updates per epoch .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,t is the learning rate at iteration t.,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"We generally use cut f rac = 0.1 , ratio = 32 and ? max = 0.01 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"STLR modifies triangular learning rates ( Smith , 2017 ) with a short increase and along decay period , which we found key for good performance .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"In Section 5 , we compare against aggressive cosine annealing , a similar schedule that has recently been used to achieve state - of - the - art performance in CV .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,6 : The slanted triangular learning rate schedule used for ULMFiT as a function of the number of training iterations .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"Finally , for fine - tuning the classifier , we augment the pretrained language model with two additional linear blocks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"Following standard practice for CV classifiers , each block uses batch normalization and dropout , with ReLU activations for the intermediate layer and a softmax activation that outputs a probability distribution over target classes at the last layer .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,Note that the parameters in these task - specific classifier layers are the only ones that are learned from scratch .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,The first linear layer takes as the input the pooled last hidden layer states .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"The signal in text classification tasks is often contained in a few words , which may occur anywhere in the document .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"As input documents can consist of hundreds of words , information may get lost if we only consider the last hidden state of the model .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,where [ ] is concatenation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,Fine - tuning the target classifier is the most critical part of the transfer learning method .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"Overly aggressive fine - tuning will cause catastrophic forgetting , eliminating the benefit of the information captured through language modeling ; too cautious fine - tuning will lead to slow convergence ( and resultant overfitting ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"Besides discriminative finetuning and triangular learning rates , we propose gradual unfreezing for fine - tuning the classifier .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,We first unfreeze the last layer and fine - tune all unfrozen layers for one epoch .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"We then unfreeze the next lower frozen layer and repeat , until we finetune all layers until convergence at the last iteration .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"This is similar to ' chain - thaw ' , except that we add a layer at a time to the set of ' thawed ' layers , rather than only training a single layer at a time .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"While discriminative fine - tuning , slanted triangular learning rates , and gradual unfreezing all are beneficial on their own , we show in Section 5 that they complement each other and enable our method to perform well across diverse datasets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,Language models are trained with backpropagation through time ( BPTT ) to enable gradient propagation for large input sequences .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,We divide the document into fixedlength batches of size b.,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"At the beginning of each batch , the model is initialized with the final state of the previous batch ; we keep track of the hidden states for mean and max - pooling ; gradients are back - propagated to the batches whose hidden states contributed to the final prediction .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"In practice , we use variable length backpropagation sequences .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"Bidirectional language model Similar to existing work ( Peters et al. , 2017 , 2018 ) , we are not limited to fine - tuning a unidirectional language model .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"For all our experiments , we pretrain both a forward and a backward LM .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,We fine - tune a classifier for each LM independently using BPT3C and average the classifier predictions .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"While our approach is equally applicable to sequence labeling tasks , we focus on text classification tasks in this work due to their important realworld applications .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"We evaluate our method on six widely - studied datasets , with varying numbers of documents and varying document length , used by state - of - the - art text classification and transfer learning approaches as instances of three common text classification tasks : sentiment analysis , question classification , and topic classification .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,We show the statistics for each dataset and task in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,TBCNN 4.0 Virtual 5.9 LSTM- CNN 3.9 ULMFiT ( ours ) 4.6 ULMFiT ( ours ) 3.6 : Test error rates ( % ) on text classification datasets used by .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"For topic classification , we evaluate on the large - scale AG news and DBpedia ontology datasets created by .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,We use the same pre-processing as in earlier work .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"In addition , to allow the language model to capture aspects that might be relevant for classification , we add special tokens for upper-case words , elongation , and repetition .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,We are interested in a model that performs robustly across a diverse set of tasks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"To this end , if not mentioned otherwise , we use the same set of hyperparameters across tasks , which we tune on the IMDb validation set .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"We use the AWD - LSTM language model with an embedding size of 400 , 3 layers , 1150 hidden activations per layer , and a BPTT batch size of 70 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"We apply dropout of 0.4 to layers , 0.3 to RNN layers , 0.4 to input embedding layers , 0.05 to embedding layers , and weight dropout of 0.5 to the RNN hidden - to - hidden matrix .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,The classifier has a hidden layer of size 50 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"We use Adam with ? 1 = 0.7 instead of the default ? 1 = 0.9 and ? 2 = 0.99 , similar to .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"We use a batch size of 64 , abase learning rate of 0.004 and 0.01 for finetuning the LM and the classifier respectively , and tune the number of epochs on the validation set of each task 7 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,We otherwise use the same practices used in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"For each task , we compare against the current state - of - theart .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"For the IMDb and TREC - 6 datasets , we compare against CoVe , a stateof - the - art transfer learning method for NLP .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"For the AG , Yelp , and DBpedia datasets , we compare against the state - of - the - art text categorization method by .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"For consistency , we report all results as error rates ( lower is better ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,We show the test error rates on the IMDb and TREC - 6 datasets used by in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"Our method outperforms both CoVe , a state - of - the - art transfer learning method based on hypercolumns , as well as the state - of - the - art on both datasets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"On IMDb , we reduce the error dramatically by 43.9 % and 22 % with regard to CoVe and the state - of - the - art respectively .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"This is promising as the existing stateof - the - art requires complex architectures , multiple forms of attention and sophisticated embedding schemes , while our method employs a regular LSTM with dropout .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"We note that the language model fine - tuning approach of only achieves an error of 7.64 vs. 4.6 for our method on IMDb , demonstrating the benefit of transferring knowledge from a large Image Net - like corpus using our fine - tuning techniques .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"It s documents are generally a few paragraphs long - similar to emails ( e.g for legal discovery ) and online comments ( e.g for community management ) ; and sentiment analysis is similar to many commercial applications , e.g. product response tracking and support email routing .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"On TREC - 6 , our improvement - similar as the improvements of state - of - the - art approaches - is not statistically significant , due to the small size of the 500 - examples test set .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"Nevertheless , the competitive performance on TREC - 6 demonstrates that our model performs well across different dataset sizes and can deal with examples that range from single sentences - in the case of TREC - 6to several paragraphs for IMDb .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"Note that despite pretraining on more than two orders of magnitude less data than the 7 million sentence pairs used by , we consistently outperform their approach on both datasets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"We show the test error rates on the larger AG , DBpedia , Yelp - bi , and Yelp - full datasets in .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,Our method again outperforms the state - of the - art significantly .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"On AG , we observe a similarly dramatic error reduction by 23.7 % compared to the state - of - the - art .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"On DBpedia , Yelp - bi , and Yelp - full , we reduce the error by 4.8 % , 18.2 % , 2.0 % respectively .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"In order to assess the impact of each contribution , we perform a series of analyses and ablations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"We run experiments on three corpora , IMDb , TREC - 6 , and AG that are representative of different tasks , genres , and sizes .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"For all experiments , we split off 10 % of the training set and report error rates on this validation set with unidirectional LMs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,We fine - tune the classifier for 50 epochs and train all methods but ULMFiT with early stopping .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,IMDb TREC - 6 AG Without pretraining 5.63 10.67 5.52 With pretraining 5.00 5.69 5.38 : Validation error rates for ULMFiT with and without pretraining .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,a task with a small number of labels .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,We evaluate ULMFiT on different numbers of labeled examples in two settings : only labeled examples are used for LM fine - tuning ( 'supervised ' ) ; and all task data is available and can be used to fine - tune the LM ( ' semi-supervised ' ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,We compare ULM - FiT to training from scratch - which is necessary for hypercolumn - based approaches .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"We split off balanced fractions of the training data , keep the validation set fixed , and use the same hyperparameters as before .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,We show the results in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"On IMDb and AG , supervised ULMFiT with only 100 labeled examples matches the performance of training from scratch with 10 and 20 more data respectively , clearly demonstrating the benefit of general - domain LM pretraining .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"If we allow ULMFiT to also utilize unlabeled examples ( 50 k for IMDb , 100 k for AG ) , at 100 labeled examples , we match the performance of training from scratch with 50 and 100 more data on AG and IMDb respectively .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"On TREC - 6 , ULMFiT significantly improves upon training from scratch ; as examples are shorter and fewer , supervised and semi-supervised ULMFiT achieve similar results .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,We compare using no pretraining with pretraining on WikiText - 103 in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",1
4,"Pretraining is most useful for small and medium - sized datasets , which are most common in commercial applications .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",1
4,"However , even for large datasets , pretraining improves performance .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"In order to gauge the importance of choosing an appropriate LM , we compare a vanilla LM with the same hyperparameters without any dropout 8 with the AWD - LSTM LM with tuned dropout parameters in .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"Using our fine - tuning techniques , even a regular LM reaches surprisingly good performance on the larger datasets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"On the smaller TREC - 6 , a vanilla LM without dropout runs the risk of overfitting , which decreases performance .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"We compare no finetuning against fine - tuning the full model ( ' Full ' ) , the most commonly used fine - tuning method , with and without discriminative fine - tuning ( ' Discr ' ) and slanted triangular learning rates ( ' Stlr ' ) in .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,Fine - tuning the LM is most beneficial for larger datasets .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"' Discr ' and ' Stlr ' improve performance across all three datasets and are necessary on the smaller TREC - 6 , where regular fine - tuning is not beneficial .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"We compare training from scratch , fine - tuning the full model ( ' Full ' ) , only fine - tuning the last layer ( ' Last ' ) , ' Chain - thaw ' , and gradual unfreezing ( ' Freez ' ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,We furthermore assess the importance of discriminative fine - tuning ( ' Discr ' ) and slanted triangular learning rates ( ' Stlr ' ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"We compare the latter to an alternative , aggressive cosine annealing schedule ( ' Cos ' ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"To avoid overfitting , we only train the vanilla LM classifier for 5 epochs and keep dropout of 0.4 in the classifier . of 0.001 and 0.0001 for the last and all other layers respectively for ' Chain - thaw ' as in , and a learning rate of 0.001 otherwise .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,We show the results in .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"Fine - tuning the classifier significantly improves over training from scratch , particularly on the small TREC - 6 . ' Last ' , the standard fine - tuning method in CV , severely underfits and is never able to lower the training error to 0 . ' Chainthaw ' achieves competitive performance on the smaller datasets , but is outperformed significantly on the large AG .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,' Freez ' provides similar performance as ' Full ' .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"' Discr ' consistently boosts the performance of ' Full ' and ' Freez ' , except for the large AG .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"Cosine annealing is competitive with slanted triangular learning rates on large data , but under-performs on smaller datasets .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"Finally , full ULMFiT classifier fine - tuning ( bottom row ) achieves the best performance on IMDB and TREC - 6 and competitive performance on AG .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"Importantly , ULMFiT is the only method that shows excellent performance across the board - and is therefore the only universal method .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"While our results demonstrate that how we fine - tune the classifier makes a significant difference , fine - tuning for inductive transfer is currently under-explored in NLP as it mostly has been thought to be unhelpful .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"To better understand the fine - tuning behavior of our model , we compare the validation error of the classifier fine - tuned with ULMFiT and ' Full ' during training in .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"On all datasets , fine - tuning the full model leads to the lowest error comparatively early in training , e.g. already after the first epoch on IMDb .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,The error then increases as the model starts to overfit and knowledge captured through pretraining is lost .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"In contrast , ULMFiT is more stable and suffers from no such catastrophic forgetting ; performance remains similar or improves until late epochs , which shows the positive effect of the learning rate schedule .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"At the cost of training a second model , ensembling the predictions of a forward and backwards LM - classifier brings a performance boost of around 0.5 - 0.7 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,On IMD b we lower the test error from 5.30 of a single model to 4.58 for the bidirectional model .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"While we have shown that ULMFiT can achieve state - of - the - art performance on widely used text classification tasks , we believe that language model fine - tuning will be particularly useful in the following settings compared to existing transfer learning approaches : a) NLP for non-English languages , where training data for supervised pretraining tasks is scarce ; b ) new NLP tasks where no state - of - the - art architecture exists ; and c) tasks with limited amounts of labeled data ( and some amounts of unlabeled data ) .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"Given that transfer learning and particularly fine - tuning for NLP is under - explored , many future directions are possible .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"One possible direction is to improve language model pretraining and fine - tuning and make them more scalable : for Image Net , predicting far fewer classes only incurs a small performance drop , while recent work shows that an alignment between source and target task label sets is important ) - focusing on predicting a subset of words such as the most frequent ones might retain most of the performance while speeding up training .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",1
4,"Language modeling can also be augmented with additional tasks in a multi-task learning fashion or enriched with additional supervision , e.g. syntax - sensitive dependencies to create a model that is more general or better suited for certain downstream tasks , ideally in a weakly - supervised manner to retain its universal properties .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",1
4,Another direction is to apply the method to novel tasks and models .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"While an extension to sequence labeling is straightforward , other tasks with more complex interactions such as entailment or question answering may require novel ways to pretrain and fine - tune .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"Finally , while we have provided a series of analyses and ablations , more studies are required to better understand what knowledge a pretrained language model captures , how this changes during fine - tuning , and what information different tasks require .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,"We have proposed ULMFiT , an effective and extremely sample - efficient transfer learning method that can be applied to any NLP task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,We have also proposed several novel fine - tuning techniques that in conjunction prevent catastrophic forgetting and enable robust learning across a diverse range of tasks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,Our method significantly outperformed existing transfer learning techniques and the stateof - the - art on six representative text classification tasks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
4,We hope that our results will catalyze new developments in transfer learning for NLP .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/text-classification/5/1801.06146v5-Stanza-out.txt,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch . We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model . Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets . Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data . We opensource our pretrained models and code 1 .",0
9,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,"One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,"Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,"Deep neural networks have shown great success in various applications such as objection recognition ( see , e.g. , ) and speech recognition ( see , e.g. , ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,"Furthermore , many recent works showed that neural networks can be successfully used in a number of tasks in natural language processing ( NLP ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"These include , but are not limited to , language modeling , paraphrase detection and word embedding extraction .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"In the field of statistical machine translation ( SMT ) , deep neural networks have begun to show promising results .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,summarizes a successful usage of feedforward neural networks in the framework of phrase - based SMT system .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"Along this line of research on using neural networks for SMT , this paper focuses on a novel neural network architecture that can be used as apart of the conventional phrase - based SMT system .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"The proposed neural network architecture , which we will refer to as an RNN Encoder - Decoder , consists of two recurrent neural networks ( RNN ) that act as an encoder and a decoder pair .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"The encoder maps a variable - length source sequence to a fixed - length vector , and the decoder maps the vector representation back to a variable - length target sequence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,The two networks are trained jointly to maximize the conditional probability of the target sequence given a source sequence .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"Additionally , we propose to use a rather sophisticated hidden unit in order to improve both the memory capacity and the ease of training .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,The proposed RNN Encoder - Decoder with a novel hidden unit is empirically evaluated on the task of translating from English to French .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,We train the model to learn the translation probability of an English phrase to a corresponding French phrase .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,The model is then used as apart of a standard phrase - based SMT system by scoring each phrase pair in the phrase table .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,The empirical evaluation reveals that this approach of scoring phrase pairs with an RNN Encoder - Decoder improves the translation performance .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,We qualitatively analyze the trained RNN Encoder - Decoder by comparing its phrase scores with those given by the existing translation model .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"The qualitative analysis shows that the RNN Encoder - Decoder is better at capturing the linguistic regularities in the phrase table , indirectly explaining the quantitative improvements in the overall translation performance .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,The further analysis of the model reveals that the RNN Encoder - Decoder learns a continuous space representation of a phrase that preserves both the semantic and syntactic structure of the phrase .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"A recurrent neural network ( RNN ) is a neural network that consists of a hidden state hand an optional output y which operates on a variablelength sequence x = ( x 1 , . . . , x T ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,where f is a non-linear activation function .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,f maybe as simple as an elementwise logistic sigmoid function and as complex as along short - term memory ( LSTM ) unit ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,An RNN can learn a probability distribution over a sequence by being trained to predict the next symbol in a sequence .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"In that case , the output at each timestep t is the conditional distribution p ( x t | x t?1 , . . . , x 1 ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"From this learned distribution , it is straightforward to sample anew sequence by iteratively sampling a symbol at each time step .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,The RNN Encoder - Decoder used in the experiment had 1000 hidden units with the proposed gates at the encoder and at the decoder .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"The input matrix between each input symbol x t and the hidden unit is approximated with two lower - rank matrices , and the output matrix is approximated similarly .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"We used rank - 100 matrices , equivalent to learning an embedding of dimension 100 for each word .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,The activation function used forh in Eq. ( 8 ) is a hyperbolic tangent function .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"The computation from the hidden state in the decoder to the output is implemented as a deep neural network ( Pascanu et al. , 2014 ) with a single intermediate layer having 500 maxout units each pooling 2 inputs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"All the weight parameters in the RNN Encoder - Decoder were initialized by sampling from an isotropic zero-mean ( white ) Gaussian distribution with its standard deviation fixed to 0.01 , except for the recurrent weight parameters .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"For the recurrent weight matrices , we first sampled from a white Gaussian distribution and used its left singular vectors matrix , following .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"We used Adadelta and stochastic gradient descent to train the RNN Encoder - Decoder with hyperparameters = 10 ?6 and ? = 0.95 ( Zeiler , 2012 ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"At each update , we used 64 randomly selected phrase pairs from a phrase table ( which was created from 348 M words ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,The model was trained for approximately three days .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,Details of the architecture used in the experiments are explained in more depth in the supplementary material .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"In addition to a novel model architecture , we also propose anew type of hidden unit ( f in Eq .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,( 1 ) ) that has been motivated by the LSTM unit but is much simpler to compute and implement .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,1 shows the graphical depiction of the proposed hidden unit .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,Let us describe how the activation of the j - th hidden unit is computed .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"is the logistic sigmoid function , and [. ] j denotes the j - th element of a vector .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"x and h t?1 are the input and the previous hidden state , respectively .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,W rand Ur are weight matrices which are learned .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"In this formulation , when the reset gate is close to 0 , the hidden state is forced to ignore the previous hidden state and reset with the current input only .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"This effectively allows the hidden state to drop any information that is found to be irrelevant later in the future , thus , allowing a more compact representation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"On the other hand , the update gate controls how much information from the previous hidden state will carryover to the current hidden state .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,This acts similarly to the memory cell in the LSTM network and helps the RNN to remember longterm information .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"Furthermore , this maybe considered an adaptive variant of a leaky - integration unit .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"As each hidden unit has separate reset and update gates , each hidden unit will learn to capture dependencies over different time scales .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"Those units that learn to capture short - term dependencies will tend to have reset gates that are frequently active , but those that capture longer - term dependencies will have update gates that are mostly active .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"In our preliminary experiments , we found that it is crucial to use this new unit with gating units .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,We were notable to get meaningful result with an oft - used tanh unit without any gating .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"where the first term at the right hand side is called translation model and the latter language model ( see , e.g. , ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"In practice , however , most SMT systems model log p ( f | e ) as a loglinear model with additional features and corre - sponding weights : where f n and w n are the n - th feature and weight , respectively .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,Z ( e ) is a normalization constant that does not depend on the weights .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,The weights are often optimized to maximize the BLEU score on a development set .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"In the phrase - based SMT framework introduced in and , the translation model log p ( e | f ) is factorized into the translation probabilities of matching phrases in the source and target sentences .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,These probabilities are once again considered additional features in the log - linear model ( see Eq. ) and are weighted accordingly to maximize the BLEU score .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"Since the neural net language model was proposed in , neural networks have been used widely in SMT systems .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"In many cases , neural networks have been used to rescore translation hypotheses ( n- best lists ) ( see , e.g. , ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"Recently , however , there has been interest in training neural networks to score the translated sentence ( or phrase pairs ) using a representation of the source sentence as an additional input .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"See , e.g. , , and .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,Here we propose to train the RNN Encoder - Decoder ( see Sec. 2.2 ) on a table of phrase pairs and use its scores as additional features in the loglinear model in Eq. ( 9 ) when tuning the SMT decoder .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"When we train the RNN Encoder - Decoder , we ignore the ( normalized ) frequencies of each phrase pair in the original corpora .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,This measure was taken in order ( 1 ) to reduce the computational expense of randomly selecting phrase pairs from a large phrase table according to the normalized frequencies and ( 2 ) to ensure that the RNN Encoder - Decoder does not simply learn to rank the phrase pairs according to their numbers of occurrences .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,One underlying reason for this choice was that the existing translation probability in the phrase table already reflects the frequencies of the phrase pairs in the original corpus .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"With a fixed capacity of the RNN Encoder - Decoder , we try to ensure that most of the capacity of the model is focused toward learning linguistic regularities , i.e. , distinguishing between plausible and implausible translations , or learning the "" manifold "" ( region of probability concentration ) of plausible translations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"Once the RNN Encoder - Decoder is trained , we add anew score for each phrase pair to the existing phrase table .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,This allows the new scores to enter into the existing tuning algorithm with minimal additional overhead in computation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"As Schwenk pointed out in , it is possible to completely replace the existing phrase table with the proposed RNN Encoder - Decoder .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"In that case , fora given source phrase , the RNN Encoder - Decoder will need to generate a list of ( good ) target phrases .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"This requires , however , an expensive sampling procedure to be performed repeatedly .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"In this paper , thus , we only consider rescoring the phrase pairs in the phrase table .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"Before presenting the empirical results , we discuss a number of recent works that have proposed to use neural networks in the context of SMT .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,Schwenk in proposed a similar approach of scoring phrase pairs .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"Instead of the RNN - based neural network , he used a feedforward neural network that has fixed - size inputs ( 7 words in his case , with zero - padding for shorter phrases ) and fixed - size outputs ( 7 words in the target language ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"When it is used specifically for scoring phrases for the SMT system , the maximum phrase length is often chosen to be small .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"However , as the length of phrases increases or as we apply neural networks to other variable - length sequence data , it is important that the neural network can handle variable - length input and output .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,The proposed RNN Encoder - Decoder is well - suited for these applications .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"Similar to , Devlin et al. proposed to use a feedforward neural network to model a translation model , however , by predicting one word in a target phrase at a time .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,"They reported an impressive improvement , but their approach still requires the maximum length of the input phrase ( or context words ) to be fixed a priori .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,"Although it is not exactly a neural network they train , the authors of proposed to learn a bilingual embedding of words / phrases .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,They use the learned embedding to compute the distance between a pair of phrases which is used as an additional score of the phrase pair in an SMT system .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"In , a feedforward neural network was trained to learn a mapping from a bag - of - words representation of an input phrase to an output phrase .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"This is closely related to both the proposed RNN Encoder - Decoder and the model proposed in , except that their input representation of a phrase is a bag - of - words .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,A similar approach of using bag - of - words representations was proposed in as well .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"Earlier , a similar encoder - decoder model using two recursive neural networks was proposed in ) , but their model was restricted to a monolingual setting , i.e. the model reconstructs an input sentence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"More recently , another encoder - decoder model using an RNN was proposed in , where the decoder is conditioned on a representation of either a source sentence or a source context .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,One important difference between the proposed RNN Encoder - Decoder and the approaches in and is that the order of the words in source and target phrases is taken into account .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"The RNN Encoder - Decoder naturally distinguishes between sequences that have the same words but in a different order , whereas the aforementioned approaches effectively ignore order information .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,The closest approach related to the proposed RNN Encoder - Decoder is the Recurrent Continuous Translation Model ( Model 2 ) proposed in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,"In their paper , they proposed a similar model that consists of an encoder and decoder .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,The difference with our model is that they used a convolutional n-gram model ( CGM ) for the encoder and the hybrid of an inverse CGM and a recurrent neural network for the decoder .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,"They , however , evaluated their model on rescoring the n-best list proposed by the conventional SMT system and computing the perplexity of the gold standard translations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,We evaluate our approach on the English / French translation task of the WMT ' 14 workshop .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,Large amounts of resources are available to build an English / French SMT system in the framework of the WMT ' 14 translation task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"The bilingual corpora include Europarl ( 61M words ) , news commentary ( 5.5 M ) , UN ( 421 M ) , and two crawled corpora of 90 M and 780M words respectively .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,The last two corpora are quite noisy .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"To train the French language model , about 712M words of crawled newspaper material is available in addition to the target side of the bitexts .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,All the word counts refer to French words after tokenization .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"It is commonly acknowledged that training statistical models on the concatenation of all this data does not necessarily lead to optimal performance , and results in extremely large models which are difficult to handle .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"Instead , one should focus on the most relevant subset of the data fora given task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,"We have done so by applying the data selection method proposed in , and its extension to bitexts .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,By these means we selected a subset of 418 M words out of more than 2G words for language modeling and a subset of 348 M out of 850 M words for training the RNN Encoder - Decoder .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,"We used the test set newstest2012 and 2013 for data selection and weight tuning with MERT , and newstest2014 as our test set .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,Each set has more than 70 thousand words and a single reference translation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,"For training the neural networks , including the proposed RNN Encoder - Decoder , we limited the source and target vocabulary to the most frequent 15,000 words for both English and French .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,This covers approximately 93 % of the dataset .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,All the out - of - vocabulary words were mapped to a special token ( [ UNK ] ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,The baseline phrase - based SMT system was built using Moses with default settings .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"This system achieves a BLEU score of 30.64 and 33.3 on the development and test sets , respectively ( see Table 1 ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"In order to assess the effectiveness of scoring phrase pairs with the proposed RNN Encoder - Decoder , we also tried a more traditional approach of using a neural network for learning a target language model ( CSLM ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"Especially , the comparison between the SMT system using CSLM and that using the proposed approach of phrase scoring by RNN Encoder - Decoder will clarify whether the contributions from multiple neural networks in different parts of the SMT sys - tem add up or are redundant .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,We trained the CSLM model on 7 - grams from the target corpus .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"Each input word was projected into the embedding space R 512 , and they were concatenated to form a 3072 dimensional vector .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,The concatenated vector was fed through two rectified layers ( of size 1536 and 1024 ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,The output layer was a simple softmax layer ( see Eq. ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"All the weight parameters were initialized uniformly between ? 0.01 and 0.01 , and the model was trained until the validation perplexity did not improve for 10 epochs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"After training , the language model achieved a perplexity of 45.80 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,The validation set was a random selection of 0.1 % of the corpus .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"The model was used to score partial translations during the decoding process , which generally leads to higher gains in BLEU score than n-best list rescoring .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,To address the computational complexity of using a CSLM in the decoder a buffer was used to aggregate n-grams during the stacksearch performed by the decoder .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"Only when the buffer is full , or a stack is about to be pruned , the n-grams are scored by the CSLM .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,This allows us to perform fast matrixmatrix multiplication on GPU using Theano .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,The top scoring target phrases fora small set of source phrases according to the translation model ( direct translation probability ) and by the RNN Encoder - Decoder .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,Source phrases were randomly selected from phrases with 4 or more words .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,denotes an incomplete ( partial ) character .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,r is a Cyrillic letter ghe .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,The results are presented in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,"As expected , adding features computed by neural networks consistently improves the performance over the baseline performance .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,The best performance was achieved when we used both CSLM and the phrase scores from the RNN Encoder - Decoder .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,This suggests that the contributions of the CSLM and the RNN Encoder - Decoder are not too correlated and that one can expect better results by improving each method independently .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,"Furthermore , we tried penalizing the number of words that are unknown to the neural networks ( i.e. words which are not in the shortlist ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,We do so by simply adding the number of unknown words as an additional feature the loglinear model in Eq. ( 9 ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"However , in this case we 3 To understand the effect of the penalty , consider the set of all words in the 15,000 large shortlist , SL .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,SL are replaced by a special token [ UNK ] before being scored by the neural networks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"where x <t is a shorthand notation for xt ? 1 , . . . , x 1 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"were notable to achieve better performance on the test set , but only on the development set .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"In order to understand where the performance improvement comes from , we analyze the phrase pair scores computed by the RNN Encoder - Decoder against the corresponding p ( f | e ) from the translation model .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"Since the existing translation model relies solely on the statistics of the phrase pairs in the corpus , we expect its scores to be better estimated for the frequent phrases but badly estimated for rare phrases .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"Also , as we mentioned earlier in Sec. 3.1 , we further expect the RNN Encoder - Decoder which was trained without any frequency information to score the phrase pairs based rather on the linguistic regularities than on the statistics of their occurrences in the corpus .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"As a result , the probability of words not in the shortlist is always overestimated .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"It is possible to address this issue by backing off to an existing model that contain non-shortlisted words ( see ) In this paper , however , we opt for introducing a word penalty instead , which counteracts the word probability overestimation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,frequent .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"For each such source phrase , we look at the target phrases that have been scored high either by the translation probability p ( f | e ) or by the RNN Encoder - Decoder .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"Similarly , we perform the same procedure with those pairs whose source phrase is long but rare in the corpus .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,lists the top - 3 target phrases per source phrase favored either by the translation model or by the RNN Encoder - Decoder .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,The source phrases were randomly chosen among long ones having more than 4 or 5 words .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"In most cases , the choices of the target phrases by the RNN Encoder - Decoder are closer to actual or literal translations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,We can observe that the RNN Encoder - Decoder prefers shorter phrases in general .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"Interestingly , many phrase pairs were scored similarly by both the translation model and the RNN Encoder - Decoder , but there were as many other phrase pairs that were scored radically different ( see ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"This could arise from the proposed approach of training the RNN Encoder - Decoder on a set of unique phrase pairs , discouraging the RNN Encoder - Decoder from learning simply the frequencies of the phrase pairs from the corpus , as explained earlier . , we show for each of the source phrases in , the generated samples from the RNN Encoder - Decoder .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,"For each source phrase , we generated 50 samples and show the top - five phrases accordingly to their scores .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,We can see that the RNN Encoder - Decoder is able to propose well - formed target phrases without looking at the actual phrase table .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"Importantly , the generated phrases do not overlap completely with the target phrases from the phrase table .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,This encourages us to further investigate the possibility of replacing the whole or apart of the phrase table with the proposed RNN Encoder - Decoder in the future .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"Since the proposed RNN Encoder - Decoder is not specifically designed only for the task of machine translation , here we briefly look at the properties of the trained model .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"It has been known for sometime that continuous space language models using neural networks are able to learn semantically meaningful embeddings ( See , e.g. , ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"Since the proposed RNN Encoder - Decoder also projects to and maps back from a sequence of words into a continuous space vector , we expect to see a similar property with the proposed model as well .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,The left plot in shows the 2 - D embedding of the words using the word embedding matrix learned by the RNN Encoder - Decoder .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,The projection was done by the recently proposed Barnes - Hut - SNE .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,We can clearly see that semantically similar words are clustered with each other ( see the zoomed - in plots in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,The proposed RNN Encoder - Decoder naturally generates a continuous - space representation of a phrase .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,The representation ( c in ) in this case is a 1000 - dimensional vector .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"Similarly to the word representations , we visualize the representations of the phrases that consists of four or more words using the Barnes - Hut - SNE in .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"From the visualization , it is clear that the RNN Encoder - Decoder captures both semantic and syntactic structures of the phrases .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"For instance , in the bottom - left plot , most of the phrases are about the duration of time , while those phrases that are syntactically similar are clustered together .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,The bottom - right plot shows the cluster of phrases that are semantically similar ( countries or regions ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"On the other hand , the top - right plot shows the phrases that are syntactically similar .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"In this paper , we proposed anew neural network architecture , called an RNN Encoder - Decoder that is able to learn the mapping from a sequence of an arbitrary length to another sequence , possibly from a different set , of an arbitrary length .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,The proposed RNN Encoder - Decoder is able to either score a pair of sequences ( in terms of a conditional probability ) or generate a target sequence given a source sequence .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"Along with the new architecture , we proposed a novel hidden unit that includes a reset gate and an update gate that adaptively control how much each hidden unit remembers or forgets while reading / generating a sequence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"We evaluated the proposed model with the task of statistical machine translation , where we used the RNN Encoder - Decoder to score each phrase pair in the phrase table .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,"Qualitatively , we were able to show that the new model is able to capture linguistic regularities in the phrase pairs well and also that the RNN Encoder - Decoder is able to propose well - formed target phrases .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,The scores by the RNN Encoder - Decoder were found to improve the overall translation performance in terms of BLEU scores .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,"Also , we found that the contribution by the RNN Encoder - Decoder is rather orthogonal to the existing approach of using neural networks in the SMT system , so that we can improve further the performance by using , for instance , the RNN Encoder - Decoder and the neural net language model together .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,Our qualitative analysis of the trained model shows that it indeed captures the linguistic regularities in multiple levels i.e. at the word level as well as phrase level .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",1
9,This suggests that there maybe more natural language related applications that may benefit from the proposed RNN Encoder - Decoder .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,The proposed architecture has large potential for further improvement and analysis .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"One approach that was not investigated here is to replace the whole , or apart of the phrase table by letting the RNN Encoder - Decoder propose target phrases .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
9,"Also , noting that the proposed model is not limited to being used with written language , it will bean important future research to apply the proposed architecture to other applications such as speech transcription .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/0/1406.1078v3-Stanza-out.txt,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) . One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model . Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",0
6,Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,"However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,"In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,"On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We can still achieve BLEU = 36.3 even without using an attention mechanism .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,Our models are also validated on the more difficult WMT ' 14 English - to - German task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,Neural machine translation ( NMT ) has attracted a lot of interest in solving the machine translation ( MT ) problem in recent years .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,"Unlike conventional statistical machine translation ( SMT ) systems which consist of multiple separately tuned components , NMT models encode the source sequence into continuous representation space and generate the target sequence in an end - to - end fashon .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"Moreover , NMT models can also be easily adapted to other tasks such as dialog systems , question answering systems and image caption generation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"In general , there are two types of NMT topologies : the encoder - decoder network and the attention network .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,The encoder - decoder network represents the source sequence with a fixed dimensional vector and the target sequence is generated from this vector word byword .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,The attention network uses the representations from all time steps of the input sequence to build a detailed relationship between the target words and the input words .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,Recent results show that the systems based on these models can achieve similar performance to conventional SMT systems .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"However , a single neural model of either of the above types has not been competitive with the best conventional system when evaluated on the WMT ' 14 English - to - French task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,The best BLEU score from a single model with six layers is only 31.5 while the conventional method of achieves 37.0 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We focus on improving the single model perfor - mance by increasing the model depth .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,Deep topology has been proven to outperform the shallow architecture in computer vision .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,In the past two years the top positions of the ImageNet contest have always been occupied by systems with tensor even hundreds of layers .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"But in NMT , the biggest depth used successfully is only six .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,We attribute this problem to the properties of the Long Short - Term Memory ( LSTM ) which is widely used in NMT .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,"In the LSTM , there are more non-linear activations than in convolution layers .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,"These activations significantly decrease the magnitude of the gradient in the deep topology , especially when the gradient propagates in recurrent form .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,"There are also many efforts to increase the depth of the LSTM such as the work by , where the shortcuts do not avoid the nonlinear and recurrent computation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"In this work , we introduce anew type of linear connections for multi - layer recurrent networks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"These connections , which are called fast - forward connections , play an essential role in building a deep topology with depth of 16 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"In addition , we introduce an interleaved bi-directional architecture to stack LSTM layers in the encoder .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,This topology can be used for both the encoder - decoder network and the attention network .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"On the WMT ' 14 Englishto - French task , this is the deepest NMT topology that has ever been investigated .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,"With our deep attention model , the BLEU score can be improved to 37.7 outperforming the shallow model which has six layers by 6.2 BLEU points .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,This is also the first time on this task that a single NMT model achieves state - of - the - art performance and outperforms the best conventional SMT system with an improvement of 0.7 .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"Even without using the attention mechanism , we can still achieve 36.3 with a single model .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"After model ensembling and unknown word processing , the BLEU score can be further improved to 40.4 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"When evaluated on the subset of the test corpus without unknown words , our model achieves 41.4 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"As a reference , previous work showed that oracle rescoring of the 1000 - best sequences generated by the SMT model can achieve the BLEU score of about 45 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,Our models are also validated on the more difficult WMT ' 14 English - to - German task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"Neural machine translation aims at generating the target word sequence y = {y 1 , . . . , y n } given the source word sequence x = {x 1 , . . . , x m } with neural models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,where y 0:j?1 is the sub sequence from y 0 toy j?1 . y 0 and y m + 1 denote the start mark and end mark of target sequence respectively .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"The process can be explicitly split into an encoding part , a decoding part and the interface between these two parts .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"In the encoding part , the source sequence is processed and transformed into a group of vectors e = {e 1 , , em } for each time step .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,Further operations will be used at the interface part to extract the final representation c of the source sequence from e.,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"At the decoding step , the target sequence is generated from the representation c.",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"Recently , there have been two types of NMT models which are different in the interface part .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"In the encoder - decoder model , a single vector extracted from e is used as the representation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"In the attention model , c is dynamically obtained according to the relationship between the target sequence and the source sequence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"The recurrent neural network ( RNN ) , or its specific form the LSTM , is generally used as the basic unit of the encoding and decoding part .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"However , the topology of most of the existing models is shallow .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"In the attention network , the encoding part and the decoding part have only one LSTM layer respectively .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"In the encoder - decoder network , researchers have used at most six LSTM layers .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,"Because machine translation is a difficult problem , we believe more complex encoding and decoding architecture is needed for modeling the relationship between the source sequence and the target sequence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,"In this work , we focus on enhancing the complexity of the encoding / decoding architecture by increasing the model depth .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,Deep neural models have been studied in a wide range of problems .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,"In computer vision , models with more than ten convolution layers outperform shallow ones on a series of image tasks in recent years .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,Different kinds of shortcut connections are proposed to decrease the length of the gradient propagation path .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"Training networks based on LSTM layers , which are widely used in language problems , is a much more challenging task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"Because of the existence of many more nonlinear activations and the recurrent computation , gradient values are not stable and are generally smaller .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"Following the same spirit for convolutional networks , a lot of effort has also been spent on training deep LSTM networks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,"introduced depth - gated shortcuts , connecting LSTM cells at adjacent layers , to provide a fast way to propagate the gradients .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,They validated the modification of these shortcuts on an MT task and a language modeling task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,"However , the best score was obtained using models with three layers .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"Similarly , proposed a two dimensional structure for the LSTM .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,Their structure decreases the number of nonlinear activations and path length .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"However , the gradient propagation still relies on the recurrent computation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"The investigations were also made on question - answering to encode the questions , whereat most two LSTM layers were stacked .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"Based on the above considerations , we propose new connections to facilitate gradient propagation in the following section .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We build the deep LSTM network with the new proposed linear connections .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,The shortest paths through the proposed connections do not include any nonlinear transformations and do not rely on any recurrent computation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We call these connections fastforward connections .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"Within the deep topology , we also introduce an interleaved bi-directional architecture to stack the LSTM layers .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,Our entire deep neural network is shown in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"This topology can be divided into three parts : the encoder part ( P -E ) on the left , the decoder part ( P - D ) on the right and the interface between these two parts ( P - I ) which extracts the representation of the source sequence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"We have two instantiations of this topology : Deep - ED and Deep - Att , which correspond to the extension of the encoder - decoder network and the attention network respectively .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,Our main innovation is the novel scheme for connecting adjacent recurrent layers .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We will start with the basic RNN model for the sake of clarity .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,where the bias parameter is not included for simplicity .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We use a red circle and a blue empty square to denote an input and a hidden state .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"A blue square with a "" - "" denotes the previous hidden state .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,A dotted line means that the hidden state is used recurrently .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"Feed-Forward computation : ft = W f x t . Left part in ) . "" f "" block .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"Recurrent computation : RNN ( f t , h t?1 ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,Right part and the sum operation ( + ) followed by activation in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,""" r "" block .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,1 ( denoted by h k?1 ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"In our work , we add fast - forward connections ( F - F connections ) which connect two feed - forward computation blocks "" f "" of adjacent recurrent layers .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"It means that each block "" f "" at recurrent layer k takes both the outputs of block "" f "" and block "" r "" at its previous layer as input ( ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,F - F connections are denoted by dashed red lines in and .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,The path of F - F connections contains neither nonlinear activations nor recurrent computation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"It provides a fast path for information to propagate , so we call this path fast - forward connections .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"Additionally , in order to learn more temporal dependencies , the sequences can be processed in different directions at each pair of adjacent recurrent layers .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,The opposite directions are marked by the direction term ( ? 1 ) k .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"At the first recurrent layer , the block "" f "" takes x t as the input .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"[ , ] denotes the concatenation of vectors .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,This is shown in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We add a connection between f kt and f k ?1 t .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"Without f k ?1 t , our model will be reduced to the traditional stacked model .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We alternate the RNN direction at different layers k with the direction term ( ? 1 ) k .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"If we fix the direction term to ? 1 , all layers work in the forward direction .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"In our experiments , instead of an RNN , a specific type of recurrent layer called LSTM ) is used .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,The LSTM is structurally more complex than the basic RNN in Eq .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"2 . We define the computation of the LSTM as a function which maps the input f and its state - output pair ( h , s ) at the previous time step to the current stateoutput pair .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,are the parameters of the LSTM .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,It is slightly different from the standard notation in that we do not have a matrix to multiply with the input fin our notation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,where x t is the input to the deep bi-directional LSTM model .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"For the encoder , x t is the embedding of the t th word in the source sentence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,For the decoder x t is the concatenation of the embedding of the t th word in the target sentence and the encoder representation for step t.,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"In our final model two additional operations are used with Eq. 5 , which is shown in Eq .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"6 . Half ( f ) denotes the first half of the elements off , and Dr ( h ) is the dropout operation which randomly sets an element of h to zero with a certain probability .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,The use of Half ( ) is to reduce the parameter size and does not affect the performance .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"We observed noticeable performance degradation when using only the first third of the elements of "" f "" .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"With the F - F connections , we build a fast channel to propagate the gradients in the deep topology .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,F - F connections can accelerate the model convergence and while improving the performance .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,A similar idea was also used in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,Encoder : The LSTM layers are stacked following Eq. 5 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We call this type of encoder interleaved bidirectional encoder .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"In addition , there are two similar columns ( a 1 and a 2 ) in the encoder part .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,Each column consists of n e stacked LSTM layers .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,There is no connection between the two columns .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,The first layers of the two columns process the word representations of the source sequence in different directions .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"At the last LSTM layers , there are two groups of vectors representing the source sequence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,The group size is the same as the length of the input sequence .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,Interface : Prior encoder - decoder models and attention models are different in their method of extracting the representations of the source sequences .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"In our work , as a consequence of the introduced F - F connections , we have 4 output vectors ( h For Deep - Att , we do not need the above two operations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,"We only concatenate the 4 output vectors at each time step to obtain e t , and a soft attention mechanism is used to calculate the final representation ct from e t .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,Note that the vector dimensionality off is four times larger than that of h ( see Eq. 4 ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,"the concatenated vector e t to a vector with 1 / 4 dimension size , denoted by the ( fully connected ) block "" fc "" in .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,The decoder follows Eq. 5 and Eq. 6 with fixed direction term ? 1 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,y t?1 is the target word embedding at the previous time step and y 0 is zero .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,There is a single column of n d stacked LSTM layers .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We also use the F - F connections like those in the encoder and all layers are in the forward direction .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"Note that at the last LSTM layer , we only use ht to make the prediction with a softmax layer .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"Although the network is deep , the training technique is straightforward .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We will describe this in the next part .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We take the parallel data as the only input without using any monolingual data for either word representation pre-training or language modeling .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"Because of the deep bi-directional structure , we do not need to reverse the sequence order as .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"The deep topology brings difficulties for the model training , especially when first order methods such as stochastic gradient descent ( SGD ) are used .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,The parameters should be properly initialized and the converging process can be slow .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"We tried several optimization techniques such as AdaDelta ( Zeiler , 2012 ) , RMSProp ( Tieleman and and .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We found that all of them were able to speedup the process a lot compared to simple SGD while no significant performance difference was observed among them .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"In this work , we chose Adam for model training and do not present a detailed comparison with other optimization methods .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,Dropout is also used to avoid over-fitting .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,It is utilized on the LSTM nodes h kt ( See Eq. 5 ) with a ratio of pd for both the encoder and decoder .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"During the whole model training process , we keep all hyper parameters fixed without any intermediate interruption .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,The hyper parameters are selected according to the performance on the development set .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"For such a deep and large network , it is not easy to determine the tuning strategy and this will be considered in future work .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We use the common left - to - right beam - search method for sequence generation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"1 . We keep n b best candidates according to Eq. 11 at each time step , until the end of sentence mark is generated .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"The hypotheses are ranked by the total likelihood of the generated sequence , although normalized likelihood is used in some works .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We evaluate our method mainly on the widely used WMT ' 14 English - to - French translation task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"In order to validate our model on more difficult language pairs , we also provide results on the WMT ' 14 English - to - German translation task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,Our models are implemented in the PADDLE ( PArallel Distributed Deep LEarning ) platform .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"For both tasks , we use the full WMT ' 14 parallel corpus as our training data .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"In total , the English - to - French corpus includes 36 million sentence pairs , and the English - to - German corpus includes 4.5 million sentence pairs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"The news - test - 2012 and news - test - 2013 are concatenated as our development set , and the news - test - 2014 is the test set .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,Our data partition is consistent with previous works on NMT to ensure fair comparison .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"For the source language , we select the most frequent 200K words as the input vocabulary .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,For the target language we select the most frequent 80 K French words and the most frequent 160K German words as the output vocabulary .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"The full vocabulary of the German corpus is larger , so we select more German words to build the target vocabulary .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,Out - of - vocabulary words are replaced with the unknown symbol unk .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"For complete comparison to previous work on the Englishto - French task , we also show the results with a smaller vocabulary of 30K input words and 30 K output words on the sub train set with selected 12M parallel sequences .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"We have two models as described above , named Deep - ED and Deep - Att.",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,Both models have exactly the same configuration and layer size except the interface part P - I.,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We use 256 dimensional word embeddings for both the source and target languages .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"All LSTM layers , including the 2n e layers in the encoder and then d layers in the decoder , have 512 memory cells .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,The output layer size is the same as the size of the target vocabulary .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,The dimension of ct is 5120 and 1280 for Deep - ED and Deep - Att respectively .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"For each LSTM layer , the activation functions for gates , inputs and outputs are sigmoid , tanh , and tanh respectively .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,Our network is narrow on word embeddings and LSTM layers .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"Note that in previous work , 1000 dimensional word embeddings and 1000 dimensional LSTM layers are used .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We also tried larger scale models but did not obtain further improvements .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"Note that each LSTM layer includes two parts as described in Eq. 3 , feed - forward computation and recurrent computation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"Since there are non-linear activations in the recurrent computation , a larger learning rate l r = 5 10 ? 4 is used , while for the feed - forward computation a smaller learning rate l f = 4 10 ? 5 is used .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,Word embeddings and the softmax layer also use this learning rate l f .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We refer all the parameters not used for recurrent computation as non-recurrent part of the model .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"Here r is the regularization strength , l is the corresponding learning rate , g stands for the gradients of v.",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,The two embedding layers are not regularized .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,All the other layers have the same r = 2 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,The parameters of the recurrent computation part are initialized to zero .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,All non-recurrent parts are randomly initialized with zero mean and standard deviation of 0.07 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,A detailed guide for setting hyperparameters can be found in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,The dropout ratio pd is 0.1 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"In each batch , there are 500 ? 800 sequences in our work .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,The exact number depends on the sequence lengths and model size .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We also find that larger batch size results in better convergence although the improvement is not large .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"However , the largest batch size is constrained by the GPU memory .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We use 4 ? 8 GPU machines ( each has 4 K40 GPU cards ) running for 10 days to train the full model with parallelization at the data batch level .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,It takes nearly 1.5 days for each pass .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,One thing we want to emphasize here is that our deep model is not sensitive to these settings .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,Small variation does not affect the final performance .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We evaluate the same way as previous NMT works .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,All reported BLEU scores are computed with the multi-bleu. perl 1 script which is also used in the above works .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,The results are for tokenized and case sensitive evaluation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,English - to - French : First we list our single model results on the English - to - French task in Tab .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,1 . In the first block we show the results with the full corpus .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,The previous best single NMT encoderdecoder model ( Enc - Dec ) with six layers achieves BLEU = 31.5 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"From Deep - ED , we obtain the BLEU score of 36.3 , which outperforms Enc - Dec model by 4.8 BLEU points .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"This result is even better than the ensemble result of eight Enc - Dec models , which is 35.6 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"This shows that , in addition to the convolutional layers for computer vision , deep topologies can also work for LSTM layers .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"For Deep - Att , the performance is further improved to 37.7 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We also list the previous state - of - the - art performance from a conventional SMT system with the BLEU of 37.0 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,This is the first time that a single NMT model trained in an end - to - end form beats the best conventional system on this task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We also show the results on the smaller data set with 12M sentence pairs and 30 K vocabulary in the second block .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"The two attention models , RNNsearch and RNNsearch - LV , achieve BLEU scores of 28.5 and 32.7 respectively .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,Note that RNNsearch - LV uses a large output vocabulary of 500K words based on the standard attention model RNNsearch .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We obtain BLEU = 35.9 which outperforms its corresponding shallow model RNNsearch by 7.4 BLEU points .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,The SMT result from is also listed and falls behind our model by 2.6 BLEU points .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,Data Voc BLEU RNNsearch 4.5M 50K 16.5 RNNsearch-LV 4.5M 500K 16.9 SMT 4.5 M Full 20.7 Deep - Att ( Ours ) 4.5M 160K 20.6 : English - to - German task : BLEU scores of single neural models .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We also list the conventional SMT system for comparison .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,Two post processing techniques are used to improve the performance further on the English - to - French task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"First , three Deep - Att models are built for ensemble results .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"They are initialized with different random parameters ; in addition , the training corpus for these models is shuffled with different random seeds .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We sum over the predicted probabilities of the target words and normalize the final distribution to generate the next word .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,It is shown in Tab .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,8 that the model ensemble can improve the performance further to 38.9 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"In and there are eight models for the best scores , but we only use three models and we do not obtain further gain from more models . : BLEU scores of different models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,The first two blocks are our results of two single models and models with post processing .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,In the last block we list two baselines of the best conventional SMT system and NMT system .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,"Second , we recover the unknown words in the generated sequences with the Positional Unknown ( Pos Unk ) model introduced in .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,The full parallel corpus is used to obtain the word mappings .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,"We find this method provides an additional 1.5 BLEU points , which is consistent with the conclusion in .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We obtain the new BLEU score of 39.2 with a single Deep - Att model .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"For the ensemble models of Deep - Att , the BLEU score rises to 40.4 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"In the last two lines , we list the conventional SMT model and the previous best neural models based system Enc - Dec for comparison .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We find our best score outperforms the previous best score by nearly 3 points .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"On the English - to - French task , we analyze the effect of the source sentence length on our models as shown in .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"Here we show five curves : our Deep - Att single model , our Deep - Att ensemble model , our Deep - ED model , a previously proposed Enc - Dec model with four layers and an SMT model .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We find our Deep - Att model works better than the previous two models ( Enc - Dec and SMT ) on nearly all sentence lengths .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"It is also shown that for very long sequences with length over 70 words , the performance of our Deep - Att does not degrade , when compared to another NMT model Enc - Dec.",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"Our Deep - ED also has much better performance than the shallow Enc - Dec model on nearly all lengths , although for long sequences it degrades and starts to fall behind Deep - Att .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,Next we look into the detail of the effect of unknown words on the English - to - French task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,We select the subset without unknown words on target sentences from the original test set .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,There are 1705 such sentences ( 56.8 % ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We compute the BLEU scores on this subset and the results are shown in Tab .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,9 . We also list the results from SMT model the score 37.7 on the full test set .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"On this subset , the SMT model achieves 37.5 , which is similar to its score 37.0 on the full test set .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,This suggests that the difficulty on this subset is not much different from that on the full set .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We therefore attribute the larger gap for Deep - att to the existence of unknown words .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We also compute the BLEU score on the subset of the ensemble model and obtain 41.4 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"As a reference related to human performance , in , it has been tested that the BLEU score of oracle re-scoring the LIUM 1000 - best results is 45 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"Deep models have more parameters , and thus have a stronger ability to fit the large data set .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"However , our experimental results suggest that deep models are less prone to the problem of over-fitting .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"In , we show three results from models with a different depth on the English - to - French task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"These three models are evaluated by token error rate , which is defined as the ratio of incorrectly predicted words in the whole target sequence with correct historical input .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,The curve with square marks corresponds to Deep - Att with n e = 9 and n d = 7 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,The curve with circle marks corresponds ton e = 5 and n d = 3 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,The curve with triangle marks corresponds ton e = 1 and n d = 1 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We find that the deep model has better performance on the test set when the token error rate is the same as that of the shallow models on the training set .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"This shows that , with decreased token error rate , the deep model is more advantageous in avoiding the over - fitting phenomenon .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"We only plot the early training stage curves because , during the late training stage , the curves are not smooth .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"With the introduction of fast - forward connections to the deep LSTM network , we build a fast path with neither non-linear transformations nor recurrent computation to propagate the gradients from the top to the deep bottom .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,"On this path , gradients decay much slower compared to the standard deep network .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,This enables us to build the deep topology of NMT models .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,We trained NMT models with depth of 16 including 25 LSTM layers and evaluated them mainly on the WMT ' 14 English - to - French translation task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,This is the deepest topology that has been investigated in the NMT area on this task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,"We showed that our Deep - Att exhibits 6.2 BLEU points improvement over the previous best single model , achieving a 37.7 BLEU score .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,This single end - toend NMT model outperforms the best conventional SMT system and achieves a state - of - the - art performance .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,"After utilizing unknown word processing and model ensemble of three models , we obtained a BLEU score of 40.4 , an improvement of 2.9 BLEU points over the previous best result .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",1
6,"When evaluated on the subset of the test corpus without unknown words , our model achieves 41.4 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,Our model is also validated on the more difficult English - to - German task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
6,Our model is also efficient in sequence generation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/machine-translation/3/1606.04199v3-Stanza-out.txt,"Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years . However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system . In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers . Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 . On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points . This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points . We can still achieve BLEU = 36.3 even without using an attention mechanism . After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 . Our models are also validated on the more difficult WMT ' 14 English - to - German task .",0
3,Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,We show how to use AT for the tasks of entity recognition and relation extraction .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",1
3,"In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",1
3,"Many neural network methods have recently been exploited in various natural language processing ( NLP ) tasks , such as parsing , POS tagging , relation extraction , translation , and joint tasks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",1
3,"However , observed that intentional small scale perturbations ( i.e. , adversarial examples ) to the input of such models may lead to incorrect decisions ( with high confidence ) .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",1
3,proposed adversarial training ( AT ) ( for image recognition ) as a regularization method which uses a mixture of clean and adversarial examples to enhance the robustness of the model .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",1
3,"Although AT has recently been applied in NLP tasks ( e.g. , text classification ) , this paper - to the best of our knowledge - is the first attempt investigating regularization effects of AT in a joint setting for two related tasks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,We start from a baseline joint model that performs the tasks of named entity recognition and relation extraction at once .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"Previously proposed models ( summarized in Section 2 ) exhibit several issues that the neural network - based baseline approach ( detailed in Section 3.1 ) overcomes : ( i ) our model uses automatically extracted features without the need of external parsers nor manually extracted features ( see ; ; ) , ( ii ) all entities and the corresponding relations within the sentence are extracted at once , instead of examining one pair of entities at a time ( see ) , and ( iii ) we model relation extraction in a multi-label setting , allowing multiple relations per entity ( see ; ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,The core contribution of the paper is the use of AT as an extension in the training procedure for the joint extraction task ( Section 3.2 ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"To evaluate the proposed AT method , we perform a large scale experimental study in this joint task ( see Section 4 ) , using datasets from different contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"We use a strong baseline that outperforms all previous models that rely on automatically extracted features , achieving state - of - the - art performance ( Section 5 ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"Compared to the baseline model , applying AT during training leads to a consistent additional increase in joint extraction effectiveness .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,Joint models that are based on manually extracted features have been proposed for performing both the named entity recognition ( NER ) and relation extraction subtasks at once .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"These methods rely on the availability of NLP tools ( e.g. , POS taggers ) or manually designed features leading to additional complexity .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,Neural network methods have been exploited to overcome this feature design issue and usually involve RNNs and CNNs .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"Specifically , as well as apply bidirectional tree - structured RNNs for different contexts ( i.e. , news , biomedical ) to capture syntactic information ( using external dependency parsers ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,propose the use of various manually extracted features along with RNNs .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"solve the simpler problem of entity classification ( EC , assuming entity boundaries are given ) , instead of NER , and they replicate the context around the entities , feeding entity pairs to the relation extraction layer .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,investigate RNNs with attention without taking into account that relation labels are not mutually exclusive .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"Finally , use LSTMs in a joint model for extracting just one relation at a time , but increase the complexity of the NER part .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,Our baseline model enables simultaneous extraction of multiple relations from the same input .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"Then , we further extend this strong baseline using adversarial training .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,Adversarial training ( AT ) has been proposed to make classifiers more robust to input perturbations in the context of image recognition .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"In the context of NLP , several variants have been proposed for different tasks such as text classification , relation extraction and POS tagging .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,AT is considered as a regularization method .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"Unlike other regularization methods ( i.e. , dropout , word dropout ) that introduce random noise , AT generates perturbations that are variations of examples easily misclassified by the model .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"The baseline model , described in detail in , is illustrated in .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,It aims to detect ( i ) the type and the boundaries of the entities and ( ii ) the relations between them .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"The input is a sequence of tokens ( i.e. , sentence ) w = w 1 , ... , w n .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"We use character level embeddings to implicitly capture morphological features ( e.g. , prefixes and suffixes ) , representing each character by a vector ( embedding ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,The character embeddings are fed to a bidirectional LSTM ( BiLSTM ) to obtain the character - based representation of the word .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,We also use pre-trained word embeddings .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"Word and character embeddings are concatenated to form the final token representation , which is then fed to a BiLSTM layer to extract sequential information .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"For the NER task , we adopt the BIO ( Beginning , Inside , Outside ) encoding scheme .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"In , the B - PER tag is assigned to the beginning token of a ' person ' ( PER ) entity .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"For the prediction of the entity tags , we use : ( i ) a softmax approach for the entity classification ( EC ) task ( assuming entity boundaries given ) or ( ii ) a CRF approach where we identify both the type and the boundaries for each entity .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"During decoding , in the softmax setting , we greedily detect the entity types of the tokens ( i.e. , independent prediction ) .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"Although independent distribution of types is reasonable for EC tasks , this is not the case when there are strong correlations between neighboring tags .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"For instance , the BIO encoding scheme imposes several constraints in the NER task ( e.g. , the B - PER and I - LOC tags can not be sequential ) .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"Motivated by this intuition , we use a linear - chain CRF for the NER task .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"For decoding , in the CRF setting , we use the Viterbi algorithm .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"During training , for both EC ( softmax ) and NER tasks ( CRF ) , we minimize the cross - entropy loss L NER .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"The entity tags are later fed into the relation extraction layer as label embeddings ( see ) , assuming that knowledge of the entity types is beneficial in predicting the relations between the involved entities .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,We model the relation extraction task as a multi-label head selection problem .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"In our model , each word w i can be involved in multiple relations with other words .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"For instance , in the example illustrated in , "" Smith "" could be involved not only in a Lives in relation with the token "" California "" ( head ) but also in other relations simultaneously ( e.g. , Works for , Born In with some corresponding tokens ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,i and the vector of corresponding relationsr i .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"We compute the score s ( w j , w i , r k ) of word w j to be the head of w i given a relation label r k using a single layer neural network .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"The corresponding probability is defined as : P ( w j , r k | w i ; ? ) = ? ( s ( w j , w i , r k ) ) , where ?( . ) is the sigmoid function .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,where m is the number of associated heads ( and thus relations ) per word w i .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"During decoding , the most probable heads and relations are selected using threshold - based prediction .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,is a set of parameters .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"In the case of multi-token entities , only the last token of the entity can serve as head of another token , to eliminate redundant relations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"If an entity is not involved in any relation , we predict the auxiliary "" N "" relation label and the token itself as head .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,We exploit the idea of AT as a regularization method to make our model robust to input perturbations .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"Specifically , we generate examples which are variations of the original ones by adding some noise at the level of the concatenated word representation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,This is similar to the concept introduced by to improve the robustness of image recognition classifiers .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,is a copy of the current model parameters .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,Since Eq.,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"( 2 ) is intractable in neural networks , we use the approximation proposed in defined as : ? adv = g/ g , with g = ? w L JOINT ( w ; ? ) , where is a small bounded norm treated as a hyperparameter .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,D ( where Dis the dimension of the embeddings ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"We train on the mixture of original and adversarial examples , so the final loss is computed as : L JOINT ( w ; ? ) + L JOINT ( w + ? adv ;? ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"We evaluate our models on four datasets , using the code as available from our github codebase .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"Specifically , we follow the 5 - fold crossvalidation defined by for the ACE04 dataset .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"For the CoNLL04 ) EC task ( assuming boundaries are given ) , we use the same splits as in ; .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,We also evaluate our models on the NER task similar to in the same dataset using 10 - fold cross validation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"For the Dutch Real Estate Classifieds , DREC ( Bekoulis et al. , 2017 ) dataset , we use train - test splits as in .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"For the Adverse Drug Events , ADE , we perform 10 - fold cross -validation similar to .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"To obtain comparable results that are not affected by the input embeddings , we use the embeddings of the previous works .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,We employ early stopping in all of the experiments .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"We use the Adam optimizer and we fix the hyperparameters ( i.e. , ? , dropout values , best epoch , learning rate ) on the validation sets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"is selected from { 5 e?2 , 1 e ? 2 , 1 e ? 3 , 1e?4 } .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"Larger values of ? ( i.e. , larger perturbations ) lead to consistent performance decrease in our early experiments .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,This can be explained from the fact that adding more noise can change the content of the sentence as also reported by .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"We use three types of evaluation , namely : ( i ) S( trict ) : we score an entity as correct if both the entity boundaries and the entity type are correct ( ACE04 , ADE , CoNLL04 , DREC ) , ( ii ) B ( oundaries ) : we score an entity as correct if only the entity boundaries are correct while the entity type is not taken into account ( DREC ) and ( iii ) R( elaxed ) : a multi-token entity is considered correct if at least one correct type is assigned to the tokens comprising the entity , assuming that the : Comparison of our method with the stateof - the - art in terms of F 1 score .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"The proposed models are : ( i ) baseline , ( ii ) baseline EC ( predicts only entity classes ) and ( iii ) baseline ( EC ) + AT ( regularized by AT ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,The and symbols indicate whether the models rely on external NLP tools .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"We include different evaluation types ( S , Rand B ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"boundaries are known ( CoNLL04 ) , to compare to previous works .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"In all cases , a relation is considered as correct when both the relation type and the argument entities are correct .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,shows our experimental results .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,The name of the dataset is presented in the first column while the models are listed in the second column .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"The proposed models are the following : ( i ) baseline : the baseline model shown in with the CRF layer and the sigmoid loss , ( ii ) baseline EC : the proposed model with the softmax layer for EC , ( iii ) baseline ( EC ) + AT : the baseline regularized using AT .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,The final three columns present the F 1 results for the two subtasks and their average performance .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,Bold values indicate the best results among models that use only automatically extracted features .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"For ACE04 , the baseline outperforms by ? 2 % in both tasks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"This improvement can be explained by the use of : ( i ) multi-label head selection , ( ii ) CRF - layer and ( iii ) character level embeddings .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",1
3,"Compared to , who rely on NLP tools , the baseline performs within a reasonable margin ( less than 1 % ) on the joint task .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",1
3,"On the other hand , use the same model for the ADE biomedical dataset , where we report a 2.5 % overall improvement .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",1
3,This indicates that NLP tools are not always accurate for various contexts .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",1
3,"For the CoNLL04 dataset , we use two evaluation settings .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",1
3,We use the relaxed evaluation similar to ; on the EC task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"The baseline model outperforms the state - of - the - art models that do not rely on manually extracted features ( > 4 % improvement for both tasks ) , since we directly model the whole sentence , instead of just considering pairs of entities .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"Moreover , compared to the model of that relies on complex features , the baseline model performs within a margin of 1 % in terms of overall F 1 score .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"We also report NER results on the same dataset and improve overall F 1 score with ? 1 % compared to , indicating that our automatically extracted features are more informative than the hand - crafted ones .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,These automatically extracted features exhibit their performance improvement mainly due to the shared LSTM layer that learns to automatically generate feature representations of entities and their corresponding relations within a single model .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"For the DREC dataset , we use two evaluation methods .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"In the boundaries evaluation , the baseline has an improvement of ? 3 % on both tasks compared to , whose quadratic scoring layer complicates NER .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,and show the effectiveness of the adversarial training on top of the baseline model .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"In all of the experiments , AT improves the predictive performance of the baseline model in the joint setting .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"Moreover , as seen in , the performance of the models using AT is closer to maximum even from the early training epochs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"Specifically , for ACE04 , there is an improvement in both tasks as well as in the overall F 1 performance ( 0.4 % ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"For CoNLL04 , we note an improvement in the overall F 1 of 0.4 % for the EC and 0.8 % for the NER tasks , respectively .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"For the DREC dataset , in both settings , there is an overall improvement of ? 1 % .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"shows that from the first epochs , the model obtains its maximum performance on the DREC validation set .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"Finally , for ADE , our AT model beats the baseline F 1 by 0.7 % .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
3,"Our results demonstrate that AT outperforms the neural baseline model consistently , considering our experiments across multiple and more diverse datasets than typical related works .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",1
3,The im - provement of AT over our baseline ( depending on the dataset ) ranges from ? 0.4 % to ? 0.9 % in terms of overall F 1 score .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",1
3,"This seemingly small performance increase is mainly due to the limited performance benefit for the NER component , which is in accordance with the recent advances in NER using neural networks that report similarly small gains ( e.g. , the performance improvement in and on the CoNLL - 2003 test set is 0.01 % and 0.17 % F 1 percentage points , while in the work of , a 0.07 % F 1 improvement on CoNLL - 2000 using AT for NER is reported ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",1
3,"However , the relation extraction performance increases by ? 1 % F 1 scoring points , except for the ACE04 dataset .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",1
3,"Further , as seen in , the improvement for CoNLL04 is particularly small on the evaluation set .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",1
3,"This may indicate a correlation between the dataset size and the benefit of adversarial training in the context of joint models , but this needs further investigation in future work .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",1
3,We proposed to use adversarial training ( AT ) for the joint task of entity recognition and relation extraction .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",1
3,"The contribution of this study is twofold : ( i ) investigation of the consistent effectiveness of AT as a regularization method over a multi-context baseline joint model , with ( ii ) a large scale experimental evaluation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",1
3,"Experiments show that AT improves the results for each task separately , as well as the overall performance of the baseline joint model , while reaching high performance already during the first epochs of the training procedure .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/3/1808.06876v3-Stanza-out.txt,"Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data . We show how to use AT for the tasks of entity recognition and relation extraction . In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",0
8,State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,"However , these features are not always accurate for various languages and contexts .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,"In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,"Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,"We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,The goal of the entity recognition and relation extraction is to discover relational structures of entity mentions from unstructured texts .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,It is a central problem in information extraction since it is critical for tasks such as knowledge base population and question answering .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"The problem is traditionally approached as two separate subtasks , namely ( i ) named entity recognition ( NER ) and ( ii ) relation extraction ( RE ) , in a pipeline setting .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"The main limitations of the pipeline models are : ( i ) error propagation between the components ( i.e. , NER and RE ) and ( ii ) possible useful information from the one task is not exploited by the other ( e.g. , identifying a Works for relation might be helpful for the NER module in detecting the type of the two entities , i.e. , PER , ORG and vice versa ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"On the other hand , more recent studies propose to use joint models to detect entities and their relations overcoming the aforementioned issues and achieving state - of - the - art performance .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,The previous joint models heavily rely on hand - crafted features .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,"Recent advances in neural networks alleviate the issue of manual feature engineering , but some of them still depend on NLP tools ( e.g. , POS taggers , dependency parsers ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,propose a Recurrent Neural Network ( RNN ) - based joint model that uses a bidirectional sequential LSTM ( Long Short Term Memory ) to model the entities and a tree - LSTM that takes into account dependency tree information to model the relations between the entities .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,The dependency information is extracted using an external dependency parser .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Similarly , in the work of for entity and relation extraction from biomedical text , a model which also uses tree - LSTMs is applied to extract dependency information .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"propose a method that relies on RNNs but uses a lot of hand - crafted features and additional NLP tools to extract features such as POS - tags , etc. replicate the context around the entities with Convolutional Neural Networks ( CNNs ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Note that the aforementioned works examine pairs of entities for relation extraction , rather than modeling the whole sentence directly .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,This means that relations of other pairs of entities in the same sentence - which could be helpful in deciding on the relation type fora particular pair - are not taken into account .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"propose a neural joint model based on LSTMs where they model the whole sentence at once , but still they do not have a principled way to deal with multiple relations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,introduce a quadratic scoring layer to model the two tasks simultaneously .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"The limitation of this approach is that only a single relation can be assigned to a token , while the time complexity for the entity recognition task is increased compared to the standard approaches with linear complexity .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"In this work , we focus on anew general purpose joint model that performs the two tasks of entity recognition and relation extraction simultaneously , and that can handle multiple relations together .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Our model achieves state - of - the - art performance in a number of different contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) without relying on any manually engineered features nor additional NLP tools .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"In summary , our proposed model ( which will be detailed next in Section 3 ) solves several shortcomings that we identified in related works ( Section 2 ) for joint entity recognition and relation extraction : ( i ) our model does not rely on external NLP tools nor hand - crafted features , ( ii ) entities and relations within the same text fragment ( typically a sentence ) are extracted simultaneously , where ( iii ) an entity can be involved in multiple relations at once .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Specifically , the model of depends on dependency parsers , which perform particularly well on specific languages ( i.e. , English ) and contexts ( i.e. , news ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Yet , our ambition is to develop a model that generalizes well in various setups , therefore using only automatically extracted features that are learned during training .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,"For instance , and use exactly the same model in different contexts , i.e. , news ( ACE04 ) and biomedical data ( ADE ) , respectively .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,"Comparing our results to the ADE dataset , we obtain a 1.8 % improvement on the NER task and ? 3 % on the RE task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,"On the other hand , our model performs within a reasonable margin ( ? 0.6 % in the NER task and ? 1 % on the RE task ) on the ACE04 dataset without the use of pre-calculated features .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,This shows that the model of strongly relies on the features extracted by the dependency parsers and can not generalize well into different contexts where dependency parser features are weak .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Comparing to , we train our model by modeling all the entities and the relations of the sentence at once .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,This type of inference is beneficial in obtaining information about neighboring entities and relations instead of just examining a pair of entities each time .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Finally , we solve the underlying problem of the models proposed by and , who essentially assume classes ( i.e. , relations ) to be mutually exclusive : we solve this by phrasing the relation extraction component as a multi-label prediction problem .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"To demonstrate the effectiveness of the proposed method , we conduct the largest experimental evaluation to date ( to the best of our knowledge ) in jointly performing both entity recognition and relation extraction ( see Section 4 and Section 5 ) , using different datasets from various domains ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Specifically , we apply our method to four datasets , namely ACE04 ( news ) , Adverse Drug Events ( ADE ) , Dutch Real Estate Classifieds ( DREC ) and CoNLL'04 ( news ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Our method outperforms all state - of - the - art methods that do not rely on any additional features or tools , while performance is very close ( or even better in the biomedical dataset ) compared to methods that do exploit hand - engineered features or NLP tools .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,The tasks of entity recognition and relation extraction can be applied either one by one in a pipeline setting or in a joint model .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"In this section , we present related work for each task ( i.e. , named entity recognition and relation extraction ) as well as prior work into joint entity and relation extraction .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Note that another difference is that we use a CRF layer for the NER part , while Katiyar & Cardie ( 2017 ) uses a softmax and uses a quadratic scoring layer ; see further , when we discuss performance comparison results in Section 5 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"We formulate the entity identification task as a sequence labeling problem , similar to previous work on joint learning models and named entity recognition using the BIO ( Beginning , Inside , Outside ) encoding scheme .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,Each entity consists of multiple sequential tokens within the sentence and we should assign a tag for every token in the sentence .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"That way we are able to identify the entity arguments ( start and end position ) and its type ( e.g. , ORG ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"To do so , we assign the B - type ( beginning ) to the first token of the entity , the I - type ( inside ) to every other token within the entity and the O tag ( outside ) if a token is not part of an entity .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,shows an example of the BIO encoding tags assigned to the tokens of the sentence .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"In the CRF layer , one can observe that we assign the B - ORG and I - ORG tags to indicate the beginning and the inside tokens of the entity "" Disease Control Center "" , respectively .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"On top of the BiLSTM layer , we employ either a softmax or a CRF layer to calculate the most probable entity tag for each token .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,l the layer width .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"we employ the softmax approach only for the entity classification ( EC ) task ( which is similar to NER ) where we need to predict only the entity types ( e.g. , PER ) for each token assuming boundaries are given .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,The CRF approach is used for the NER task which includes both entity type and boundaries recognition .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"In the softmax approach , we assign entity types to tokens in a greedy way at prediction time ( i.e. , the selected tag is just the highest scoring tag overall possible set of tags ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,"Although assuming an independent tag distribution is beneficial for entity classification tasks ( e.g. , POS tagging ) , this is not the case when there are strong de-pendencies between the tags .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,"Specifically , in NER , the BIO tagging scheme forces several restrictions ( e.g. , B - LOC can not be followed by I - PER ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"The softmax method allows local decisions ( i.e. , for the tag of each token w i ) even though the BiLSTM captures information about the neighboring words .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Still , the neighboring tags are not taken into account for the tag decision of a specific token .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"For example , in the entity "" John Smith "" , tagging "" Smith "" as PER is useful for deciding that "" John "" is B - PER .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"To this end , for NER , we use a linear - chain CRF , similar to Lample et al .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,where an improvement of ? 1 % F 1 NER points is reported when using CRF .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"is the score of the predicted tag for token w i , T is a square transition matrix in which each entry represents transition scores from one tag to another .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,We apply Viterbi to obtain the tag sequence ? ( e ) with the highest score .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,We train both the softmax ( for the EC task ) and the CRF layer ( for NER ) by minimizing the cross - entropy loss L NER .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"We also use the entity tags as input to our relation extraction layer by learning label embeddings , motivated by where an improvement of 2 % F 1 is reported ( with the use of label embeddings ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"In our case , label embeddings lead to an increase of 1 % F 1 score as reported in ( see Section 5.2 ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"The input to the next layer is twofold : the output states of the LSTM and the learned label embedding representation , encoding the intuition that knowledge of named enti-ties can be useful for relation extraction .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"During training , we use the gold entity tags , while at prediction time we use the predicted entity tags as input to the next layer .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,We consider relation extraction as the second task of our joint model .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,The main approaches for relation extraction rely either on hand - crafted features or neural networks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Feature - based methods focus on obtaining effective hand - crafted features , for instance defining kernel functions and designing lexical , syntactic , semantic features , etc . .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,Neural network models have been proposed to overcome the issue of manually designing hand - crafted features leading to improved performance .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,CNN - and models have been introduced to automatically extract lexical and sentence level features leading to a deeper language understanding .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,combine CNNs and RNNs using an ensemble scheme to achieve state - of - the - art results .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,Entity and relation extraction includes the task of ( i ) identifying the entities ( described in Section 2.1 ) and ( ii ) extracting the relations among them ( described in Section 2.2 ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,Feature - based joint models have been proposed to simultaneously solve the entity recognition and relation extraction ( RE ) subtasks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"These methods rely on the availability of NLP tools ( e.g. , POS taggers ) or manually designed features and thus ( i ) require additional effort for the data preprocessing , ( ii ) perform poorly in different application and language settings where the NLP tools are not reliable , and ( iii ) increase the computational complexity .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"In this paper , we introduce a joint neural network model to overcome the aforementioned issues and to automatically perform end - to - end relation extraction without the need of any manual feature engineering or the use of additional NLP components .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,Neural network approaches have been considered to address the problem in a joint setting ( end - to - end relation extraction ) and typically include the use of RNNs and CNNs .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Specifically , propose the use of bidirectional tree - structured RNNs to capture dependency tree information ( where parse trees are extracted using state - of - the - art dependency parsers ) which has been proven beneficial for relation extraction .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"apply the work of to biomedical text , reporting state - of - the - art performance for two biomedical datasets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,propose the use of a lot of hand - crafted features along with RNNs .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"solve the entity classification task ( which is different from NER since in entity classification the boundaries of the entities are known and only the type of the entity should be predicted ) and relation extraction problems using an approximation of a global normalization objective ( i.e. , CRF ) : they replicate the context of the sentence ( left and right part of the entities ) to feed one entity pair at a time to a CNN for relation extraction .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Thus , they do not simultaneously infer other potential entities and relations within the same sentence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"and The input of our model is the words of the sentence which are then represented as word vectors ( i.e. , embeddings ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,The BiLSTM layer extracts a more complex representation for each word .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,Then the CRF and the sigmoid layers are able to produce the outputs for the two tasks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"The outputs for each token ( e.g. , Smith ) are : ( i ) an entity recognition label ( e.g. , I - PER ) and ( ii ) a set of tuples comprising the head tokens of the entity and the types of relations between them ( e.g. , {( Center , Works for ) , ( Atlanta , Lives in ) } ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"tional complexity described by , by dividing the loss functions into a NER and a relation extraction component .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Moreover , we are able to handle multiple relations instead of just predicting single ones , as was described for the application of structured real estate advertisements of .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"In this section , we present our multi-head joint model illustrated in .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"The model is able to simultaneously identify the entities ( i.e. , types and boundaries ) and all the possible relations between them at once .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,We formulate the problem as a multi-head selection problem extending previous work as described in Section 2.3 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"By multi-head , we mean that any particular entity maybe the CoNLL04 dataset is presented .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"The input of our model is a sequence of tokens ( i.e. , words of the sentence ) which are then represented as word vectors ( i.e. , word embeddings ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,The BiLSTM layer is able to extract a more complex representation for each word that incorporates the context via the RNN structure .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,Then the CRF and the sigmoid layers are able to produce the outputs for the two tasks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"The outputs for each token ( e.g. , Smith ) are twofold : ( i ) an entity recognition label ( e.g. , I - PER , denoting the token is inside a named entity of type PER ) and ( ii ) a set of tuples comprising the head tokens of the entity and the types of relations between them ( e.g. , {( Center , Works for ) , ( Atlanta , Lives in ) } ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Since we assume token - based encoding , we consider only the last token of the entity as head of another token , eliminating redundant relations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"For instance , there is a Works for relation between entities "" John Smith "" and "" Disease Control Center "" .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Instead of connecting all tokens of the entities , we connect only "" Smith "" with "" Center "" .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Also , for the case of no relation , we introduce the "" N "" label and we predict the token itself as the head .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Given a sentence w = w 1 , ... , w n as a sequence of tokens , the word embedding layer is responsible to map each token to a word vector ( w word2vec ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,We use pre-trained word embeddings using the Skip - Gram word2vec model .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"In this work , we also use character embeddings since they are commonly applied to neural NER .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,This type of embeddings is able to capture morphological features such as prefixes and suffixes .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,"For instance , in the Adverse Drug Events ( ADE ) dataset , the suffix "" toxicity "" can specify an adverse drug event entity such as "" neurotoxicity "" or "" hepatotoxicity "" and thus it is very informative .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,"Another example might be the Dutch suffix "" kamer "" ( "" room "" in English ) in the Dutch Real Estate Classifieds ( DREC ) dataset which is used to specify the space entities "" badkamer "" ( "" bathroom "" in English ) and "" slaapkamer "" ( "" bedroom "" in English ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Character - level embeddings are learned during training , similar to Ma & Hovy and .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"In the work of Lample et al. , character embeddings lead to a performance improvement of up to 3 % in terms of NER F 1 score .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,overall F 1 scoring points .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"For more details , see Section 5.2 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,RNNs are commonly used in modeling sequential data and have been successfully applied in various NLP tasks .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"In this work , we use multi - layer LSTMs , a specific kind of RNNs which are able to capture long term dependencies well .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,We employ a BiLSTM which is able to encode information from left to right ( past to future ) and right to left ( future to past ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"This way , we can combine bidirectional information for each word by concatenating the forward ( hi ) and the backward ( hi ) output at timestep i .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"In this subsection , we describe the relation extraction task , formulated as a multi-head selection problem .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"In the general formulation of our method , each token w i can have multiple heads ( i.e. , multiple relations with other tokens ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,i is the vector of the corresponding relations for each token w i .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"This is different for the previous standard head selection for dependency parsing method since ( i ) it is extended to predict multiple heads and ( ii ) the decisions for the heads and the relations are jointly taken ( i.e. , instead of first predicting the heads and then in a next step the relations by using an additional classifier ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,R .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"is the hidden size of the LSTM , b is the size of the label embeddings and l the layer width .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"to be the probability of token w j to be selected as the head of token w i with the relation label r k between them , where ?( . ) stands for the sigmoid function .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,R are the ground truth vectors of heads and associated relation labels of w i and m is the number of relations ( heads ) for w i .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,i and relation labelsr i exceeding a threshold based on the estimated joint probability as defined in Eq..,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Unlike previous work on joint models ( Katiyar & Cardie , 2017 ) , we are able to predict multiple relations considering the classes as independent and not mutually exclusive ( the probabilities do not necessarily sum to 1 for different classes ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"For the joint entity and relation extraction task , we calculate the final objective as L NER + L rel .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,Our model is able to simultaneously extract entity mentions and the relations between them .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,"To demonstrate the effectiveness and the general purpose nature of our model , we also test it on the recently introduced Dutch real estate classifieds ( DREC ) dataset where the entities need to form a tree structure .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,"By using thresholded inference , a tree structure of relations is not guaranteed .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,Thus we should enforce tree structure constraints to our model .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"To this end , we post-process the output of our system with Edmonds ' maximum spanning tree algorithm for directed graphs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,V represent the last tokens of the identified entities ( as predicted by NER ) and the edges E represent the highest scoring relations with their scores as weights .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,Edmonds ' algorithm is applied in cases a tree is not already formed by thresholded inference .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"We conduct experiments on four datasets : ( i ) Automatic Content Extraction , ACE04 ( Dod - We follow the cross -validation setting of Li & Ji and .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,We removed DISC and did 5 - fold cross -validation on the bnews and nwire subsets ( 348 documents ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,We obtained the preprocessing script from Miwa 's github codebase .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"We measure the performance of our system using micro F 1 scores , Precision and Recall on both entities and relations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,We treat an entity as correct when the entity type and the region of its head are correct .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"We treat a relation as correct when it s type and argument entities are correct , similar to and .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,We refer to this type of evaluation as strict .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"We select the best hyperparameter values on a randomly selected validation set for each fold , selected from the training set ( 15 % of the data ) since there are no official train and validation splits in the work of .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"and Other ) and five relation types ( Kill , Live in , Located in , OrgBased in and Work for ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,We use the splits defined by Gupta et al. and .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"The dataset consists of 910 training instances , 243 for validation and 288 for testing .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,We measure the performance by computing the F 1 score on the test set .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,We adopt two evaluation settings to compare to previous work .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Specifically , we perform an EC task assuming the entity boundaries are given similar to Gupta et al. and .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"To obtain comparable results , we omit the entity class "" Other "" when computing the EC score .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,We score a multi-token entity as correct if at least one of its comprising token types is correct assuming that the boundaries are given ; a relation is correct when the type of the relation and the argument entities are both correct .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,We report macro-average F 1 scores for EC and RE to obtain comparable results to previous studies .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Moreover , we perform actual NER evaluation instead of just EC , reporting results using the strict evaluation metric .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,We measure the performance by computing the F 1 score on the test set .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"To compare our results with previous work , we use the boundaries evaluation setting .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"In this setting , we count an entity as correct if the boundaries of the entity are correct .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,A relation is correct when the relation is correct and the argument entities are both correct .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Also , we report results using the strict evaluation for future reference .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,ADE : There are two types of entities ( drugs and diseases ) in this dataset and the aim of the task is to identify the types of entities and relate each drug with a disease ( adverse drug events ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"There are 6,821 sentences in total and similar to previous work , we remove ? 130 relations with overlapping entities ( e.g. , "" lithium "" is a drug which is related to "" lithium intoxication "" ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Since there are no official sets , we evaluate our model using 10 - fold cross- validation where 10 % of the data was used as validation and 10 % for test set similar to .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,The final results are displayed in F 1 metric as a macro -average across the folds .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"The dataset consists of 10,652 entities and 6,682 relations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,We report results similar to previous work on this dataset using the strict evaluation metric .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"We use pre-trained word2vec embeddings used in previous work , so as to retain the same inputs for our model and to obtain comparable results that are not affected by the input embeddings .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Specifically , we use the 200 - dimensional word embeddings used in the work of for the ACE04 dataset 6 trained on Wikipedia .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,We have developed our joint model by using Python with the TensorFlow machine learning library .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,"Training is performed using the Adam optimizer ( Kingma & Ba , 2015 ) with a learning rate of 10 ?3 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,We fix the size of the LSTM to d = 64 and the layer width of the neural network to l = 64 ( both for the entity and the relation scoring layers ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,We use dropout to regularize our network .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,Dropout is applied in the input embeddings and in between the hidden layers for both tasks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,Different dropout rates have been applied but the best dropout values ( 0.2 to 0.4 ) for each dataset have been used .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,The hidden dimension for the characterbased LSTMs is 25 ( for each direction ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,We also fixed our label embeddings to be of size b = 25 for all the datasets except for CoNLL04 where the label embeddings were not beneficial and thus were not used .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,We experimented with tanh and relu activation functions ( recall that this is the function f ( ) from the model description relu activation only in the ACE04 and tanh in all other datasets .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,We employ the technique of early stopping based on the validation set .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"In all the datasets examined in this study , we obtain the best hyperparameters after 60 to 200 epochs depending on the size of the dataset .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,We select the best epoch according to the results in the validation set .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,For more details about the effect of each hyperparameter to the model performance seethe Appendix .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"In , we present the results of our analysis .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,The first column indicates the considered dataset .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"In the second column , we denote the model which is applied ( i.e. , previous work and the proposed models ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"E is the proposed model with addition of Edmonds ' algorithm to guarantee a tree - structured output for the DREC dataset , ( iii ) single - head is the proposed method but it predicts only one head per token using a softmax loss instead of a sigmoid , and ( iv ) multi-head EC is the proposed method with a softmax to predict the entity classes assuming that the boundaries are given , and the sigmoid loss for multiple head selection . ( iii ) Relaxed : we score a multi-token entity as correct if at least one of its comprising token types is correct assuming that the boundaries are given ; a relation is correct when the type of the relation and the argument entities are both correct .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"In the next three columns , we present the results for the entity identification task ( Precision , Recall , F 1 ) and then ( in the subsequent three columns ) the results of the relation extraction task ( Precision , Recall , F 1 ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Finally , in the last column , we report an additional F 1 measure which is the average F 1 performance of the two subtasks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"We mark with bold font in , the class probabilities do not necessarily sum up to one since the classes are considered independent .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Moreover , we use a CRF - layer to model the NER task to capture dependencies between sequential tokens .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Finally , we obtain more effective word representations by using character - level embeddings .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"for the RE task ) compared to For the CoNLL04 dataset , there are two different evaluation settings , namely relaxed and strict .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"In the relaxed setting , we perform an EC task instead of NER assuming that the boundaries of the entities are given .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,We adopt this setting to produce comparable results with previous studies ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Similar to , we present results of single models and no ensembles .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,We observe that our model outperforms all previous models that do not rely on complex hand - crafted features by a large margin ( > 4 % for both tasks ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Unlike these previous studies that consider pairs of entities to obtain the entity types and the corresponding relations , we model the whole sentence at once .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"That way , our method is able to directly infer all entities and relations of a sentence and benefit from their possible interactions that can not be modeled when training is performed for each entity pair individually , one at a time .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"In the same setting , we also report the results of Gupta et al. in which they use multiple complicated hand - crafted features coming from NLP tools .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,Our model performs slightly better for the EC task and within a margin of 1 % in terms of overall F 1 score .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,The difference in the overall performance is due to the fact that our model uses only automatically generated features .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"We also report re-sults on the same dataset conducting NER ( i.e. , predicting entity types and boundaries ) and evaluating using the strict evaluation measure , similar to .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,Our results are not directly comparable to the work of because we use the splits provided by .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"However , in this setting we present the results from as reference .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"We report an improvement of ? 2 % overall F 1 score , which suggests that our neural model is able to extract more informative representations compared to feature - based approaches .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"We also report results for the DREC dataset , with two different evaluation settings .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Specifically , we use the boundaries and the strict settings .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"We transform the previous results from to the boundaries setting to make them comparable to our model since in their work , they report token - based F 1 score , which is not a common evaluation metric in relation extraction problems .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Also , in their work , they focus on identifying only the boundaries of the entities and not the types ( e.g. , Floor , Space ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,"In the boundaries evaluation , we achieve ? 3 % improvement for both tasks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,"This is due to the fact that their quadratic scoring layer is beneficial for the RE task , yet complicates NER , which is usually modeled as a sequence labeling task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,"Moreover , we report results using the strict evaluation which is used inmost related works .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Using the prior knowledge that each entity has only one head , we can simplify our model and predict only one head each time ( i.e. , using a softmax loss ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,The difference between the single and the multi-head models is marginal ( < 0.1 % for both tasks ) .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"This shows that our model ( multi-head ) can adapt to various environments , even if the setting is single head ( in terms of the application , and thus also in both training and test data ) .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,We conduct ablation tests on the ACE04 dataset reported in to analyze the effectiveness of the various parts of our joint model .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,The performance of the RE task decreases ( ? 1 % in terms of F 1 score ) when we remove the label embeddings layer and only use the LSTM hidden states as inputs for the RE task .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"This shows that the NER labels , as expected , provide meaningful information for the RE component .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,Removing character embeddings also degrades the performance of both NER ( ? 1 % ) and RE ( ? 2 % ) tasks by a relatively large margin .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"This illustrates that composing words by the representation of characters is effective , and our method benefits from additional information such as capital letters , suffixes and prefixes within the token ( i.e. , its character sequences ) .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,"Finally , we conduct experiments for the NER task by removing the CRF loss layer and substituting it with a softmax .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,"Assuming independent distribution of labels ( i.e. , softmax ) leads to a slight decrease in the F 1 performance of the NER module and a ? 2 % decrease in the performance of the RE task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,"This happens because the CRF loss is able to capture the strong tag dependencies ( e.g. , I - LOC can not follow B - PER ) that are present in the dataset instead of just assuming that the tag decision for each token is independent from tag decisions of neighboring tokens .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"In this work , we present a joint neural model to simultaneously extract entities and relations from textual data .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,Our model comprises a CRF layer for the entity recognition task and a sigmoid layer for the relation extraction task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Specifically , we model the relation extraction task as a multi-head selection problem since one entity can have multiple relations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Previous models on this task rely heavily on external NLP tools ( i.e. , POS taggers , dependency parsers ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Thus , the performance of these models is affected by the accuracy of the extracted features .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Unlike previous studies , our model produces automatically generated features rather than relying on hand - crafted ones , or existing NLP tools .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,"Given its independence from such NLP or other feature generating tools , our approach can be easily adopted for any language and context .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",1
8,We demonstrate the effectiveness of our approach by conducting a large scale experimental study .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,Our model is able to outperform neural methods that automatically generate features while the results are marginally similar ( or sometimes better ) compared to feature - based neural network approaches .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"As future work , we aim to explore the effectiveness of entity pre-training for the entity recognition module .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,This approach has been proven beneficial in the work of Miwa & Bansal ( 2016 ) for both the entity and the relation extraction modules .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,we are planning to explore away to reduce the calculations in the quadratic relation scoring layer .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"For instance , a straightforward way to do so is to use in the sigmoid layer only the tokens that have been identified as entities .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Gupta , P. , . optimize only over the NER task ) , ( ii ) explore several hyperparameters of the network ( e.g. , dropout , LSTM size , character embeddings size ) , and ( iii ) report F 1 score using different word embeddings compared to the embeddings used in previous works .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"In of the main paper , we focused on comparing our model against other joint models that are able to solve the two tasks ( i.e. , NER and relation extraction ) simultaneously , mainly demonstrating superiority of phrasing the relation extraction as a multi-head selection problem ( enabling the extraction of multiple relations at once ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Here , in and vice versa ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Note that improving NER in isolation was not the objective of our multi-head model , but we rather aimed to compare our model against other joint models that solve the task of entity recognition and relation identification simultaneously .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,We thus did not envision to claim or achieve state - of - the - art performance in each of the individual building blocks of our joint model .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"and A4 show the performance of our model on the test set for different values of the embedding dropout , LSTM layer dropout and the LSTM output dropout hyperparameters , respectively .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Note that the hyperparameter values used for the results in Section 5 were obtained by tuning over the development set , and these are indicated in boldface in the tables below .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,We vary one hyperparameter at a time in order to assess the effect of a particular hyperparameter .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"The main outcomes from these tables are twofold : ( i ) low dropout values ( e.g. , 0 , 0.1 ) lead to a performance decrease in the overall F 1 score ( see where a ? 3 % F 1 decrease is reported on the ACE04 dataset ) and ( ii ) average dropout values ( i.e. , 0.2 - 0.4 ) lead to consistently similar results .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"In In the main results ( see Section 5 ) , to guarantee a fair comparison to previous work and to obtain comparable results that are not affected by the input embeddings , we use embeddings used also in prior studies .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"To assess the performance of our system to input variations , we also report results using different word embeddings ( see ) ( i.e. , ; Li et al. ) on the ACE04 dataset .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
8,"Our results showcase that our model , even when using different word embeddings , is still performing better compared to other works that , like ours , do not rely on additional NLP tools .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/relation-classification/2/1804.07847v3-Stanza-out.txt,"State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers . Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools . However , these features are not always accurate for various languages and contexts . In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool . Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) . We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) . Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",0
2,Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,"With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,"However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,"In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,"We introduce BioBERT ( Bidirectional Encoder Representations from Transformers for Biomedical Text Mining ) , which is a domain - specific language representation model pre-trained on large - scale biomedical corpora .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,"With almost the same architecture across tasks , BioBERT largely outperforms BERT and previous state - of - the - art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,"While BERT obtains performance comparable to that of previous state - of - the - art models , BioBERT significantly outperforms them on the following three representative biomedical text mining tasks : biomedical named entity recognition ( 0.62 % F1 score improvement ) , biomedical relation extraction ( 2.80 % F1 score improvement ) and biomedical question answering ( 12.24 % MRR improvement ) .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,The volume of biomedical literature continues to rapidly increase .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,"On average , more than 3000 new articles are published everyday in peer-reviewed journals , excluding pre-prints and technical reports such as clinical trial reports in various archives .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,PubMed alone has a total of 29M articles as of January 2019 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,Reports containing valuable information about new discoveries and new insights are continuously added to the already overwhelming amount of literature .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"Consequently , there is increasingly more demand for accurate biomedical text mining tools for extracting information from the literature .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,Recent progress of biomedical text mining models was made possible by the advancements of deep learning techniques used in natural language processing ( NLP ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,"For instance , Long Short - Term Memory ( LSTM ) and Conditional Random Field ( CRF ) have greatly improved performance in biomedical named entity recognition ( NER ) over the last few years .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,Other deep learning based models have made improvements in biomedical text mining tasks such as relation extraction ( RE ) and question answering ( QA ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"However , directly applying state - of - the - art NLP methodologies to biomedical text mining has limitations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"First , as recent word representation models such as Word2 Vec , ELMo and BERT are trained and tested mainly on datasets containing general domain texts ( e.g. Wikipedia ) , it is difficult to estimate their performance on datasets containing biomedical texts .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"Also , the word distributions of general and biomedical corpora are quite different , which can often be a problem for biomedical text mining models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"As a result , recent models in biomedical text mining rely largely on adapted versions of word representations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"In this study , we hypothesize that current state - of - the - art word representation models such as BERT need to be trained on biomedical corpora to be effective in biomedical text mining tasks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"Previously , Word2 Vec , which is one of the most widely known context independent word representation models , was trained on biomedical corpora which contain terms and expressions that are usually not included in a general domain corpus .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"While ELMo and BERT have proven the effectiveness of contextualized word representations , they can not obtain high performance on biomedical corpora because they are pre-trained on only general domain corpora .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"As BERT achieves very strong results on various NLP tasks while using almost the same structure across the tasks , adapting BERT for the biomedical domain could potentially benefit numerous biomedical NLP researches .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"In this article , we introduce BioBERT , which is a pre-trained language representation model for the biomedical domain .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,The overall process of pre-training and fine - tuning BioBERT is illustrated in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"First , we initialize BioBERT with weights from BERT , which was pretrained on general domain corpora ( English Wikipedia and Books Corpus ) .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,"Then , BioBERT is pre-trained on biomedical domain corpora ( PubMed abstracts and PMC full - text articles ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"To show the effectiveness of our approach in biomedical text mining , BioBERT is fine - tuned and evaluated on three popular biomedical text mining tasks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"We test various pre-training strategies with different combinations and sizes of general domain corpora and biomedical corpora , and analyze the effect of each corpus on pre-training .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,We also provide in - depth analyses of BERT and BioBERT to show the necessity of our pre-training strategies .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,BioBERT is the first domain - specific BERT based model pretrained on biomedical corpora for 23 days on eight NVIDIA V100 GPUs .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,We show that pre-training BERT on biomedical corpora largely improves its performance .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"BioBERT obtains higher F 1 scores in biomedical NER ( 0.62 ) and biomedical RE ( 2.80 ) , and a higher MRR score ( 12.24 ) in biomedical QA than the current state - of the - art models .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,"Compared with most previous biomedical text mining models that are mainly focused on a single task such as NER or QA , our model BioBERT achieves state - of - the - art performance on various biomedical text mining tasks , while requiring only minimal architectural modifications .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,"We make our pre-processed datasets , the pre-trained weights of BioBERT and the source code for fine - tuning BioBERT publicly available .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,BioBERT basically has the same structure as BERT .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,"We briefly discuss the recently proposed BERT , and then we describe in detail the pre-training and fine - tuning process of BioBERT .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,Learning word representations from a large amount of unannotated text is a long - established method .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"While previous models ( e.g. Word2 Vec , GloVe ) focused on learning context independent word representations , recent works have focused on learning context dependent word representations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"For instance , ELMo uses a bidirectional language model , while uses machine translation to embed context information into word representations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,BERT is a contextualized word representation model that is based on a masked language model and pretrained using bidirectional transformers .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"Due to the nature of language modeling where future words can not be seen , previous language models were limited to a combination of two unidirectional language models ( i.e. left - to - right and right - toleft ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"BERT uses a masked language model that predicts randomly masked words in a sequence , and hence can be used for learning bidirectional representations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"Also , it obtains state - of - the - art performance on most NLP tasks , while requiring minimal task - specific architectural modification .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"According to the authors of BERT , incorporating information from bidirectional representations , rather than unidirectional representations , is crucial for representing words in natural language .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,We hypothesize that such bidirectional representations are also critical in biomedical text mining as complex relationships between biomedical terms often exist in a biomedical corpus .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"Due to the space limitations , we refer readers to fora more detailed description of BERT .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"As a general purpose language representation model , BERT was pretrained on English Wikipedia and Books Corpus .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"However , biomedical domain texts contain a considerable number of domain - specific .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"Overview of the pre-training and fine - tuning of BioBERT proper nouns ( e.g. BRCA1 , c.248T > C ) and terms ( e.g. transcriptional , antimicrobial ) , which are understood mostly by biomedical researchers .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"As a result , NLP models designed for general purpose language understanding often obtains poor performance in biomedical text mining tasks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"In this work , we pre-train BioBERT on PubMed abstracts ( PubMed ) and PubMed Central full - text articles ( PMC ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"The text corpora used for pre-training of BioBERT are listed in , and the tested combinations of text corpora are listed in .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"For computational efficiency , whenever the Wiki Books corpora were used for pre-training , we initialized BioBERT with the pre-trained BERT model provided by .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,We define BioBERT as a language representation model whose pre-training corpora includes biomedical corpora ( e.g. BioBERT ( PubMed ) ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"For tokenization , BioBERT uses WordPiece tokenization , which mitigates the out - of - vocabulary issue .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,I ##mm ##uno ##g ##lo # #bul # #in ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,We found that using cased vocabulary ( not lowercasing ) results in slightly better performances in downstream tasks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"Although we could have constructed new WordPiece vocabulary based on biomedical corpora , we used the original vocabulary of BERT BASE for the following reasons : ( i ) compatibility of BioBERT with BERT , which allows BERT pre-trained on general domain corpora to be re-used , and makes it easier to interchangeably use existing models based on BERT and BioBERT and ( ii ) any new words may still be represented and fine - tuned for the biomedical domain using the original WordPiece vocabulary of BERT .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"With minimal architectural modification , BioBERT can be applied to various downstream text mining tasks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"We fine - tune BioBERT on the following three representative biomedical text mining tasks : NER , RE and QA .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"Named entity recognition is one of the most fundamental biomedical text mining tasks , which involves recognizing numerous domain - specific proper nouns in a biomedical corpus .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"While most previous works were built upon different combinations of LSTMs and CRFs , BERT has a simple architecture based on bidirectional transformers .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,BERT uses a single output layer based on the representations from its last layer to compute only token level BIO2 probabilities .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"Note that while previous works in biomedical NER often used word embeddings trained on PubMed or PMC corpora , BioBERT directly learns WordPiece embeddings during pre-training and fine - tuning .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"For the evaluation metrics of NER , we used entity level precision , recall and F1 score .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,Relation extraction is a task of classifying relations of named entities in a biomedical corpus .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"We utilized the sentence classifier of the original version of BERT , which uses a [ CLS ] token for the classification of relations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,Sentence classification is performed using a single output layer based on a [ CLS ] token representation from BERT .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,We anonymized target named entities in a sentence using pre-defined tags such as @ GENE $ or @DISEASE $ .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"The precision , recall and F 1 scores on the RE task are reported .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,Question answering is a task of answering questions posed in natural language given related passages .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"To fine - tune BioBERT for QA , we used the same BERT architecture used for SQuAD .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,We used the BioASQ factoid datasets because their format is similar to that of SQuAD .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,Token level probabilities for the start / end location of answer phrases are computed using a single output layer .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"However , we observed that about 30 % of the BioASQ factoid questions were unanswerable in an extractive QA setting as the exact answers did not appear in the given passages .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"Like , we excluded the samples with unanswerable questions from the training sets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"Also , we used the same pre-training process of , which uses SQuAD , and it largely improved the performance of both BERT and BioBERT .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"We used the following evaluation metrics from BioASQ : strict accuracy , lenient accuracy and mean reciprocal rank .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,The statistics of biomedical NER datasets are listed in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"We used the pre-processed versions of all the NER datasets provided by except the 2010 i 2 b2 / VA , JNLPBA and Species - 800 datasets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,The pre-processed NCBI Disease dataset has fewer annotations than the original dataset due to the removal of duplicate articles from its training set .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,We used the CoNLL format ( https :// github.com/spyysalo/standoff2conll ) for pre-processing the 2010 i 2b2 / VA and JNLPBA datasets .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,The Species - 800 dataset was preprocessed and split based on the dataset of Pyysalo ( https://github. com/spyysalo/s800 ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"We did not use alternate annotations for the BC2 GM dataset , and all NER evaluations are based on entity - level exact matches .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"Note that although there are several other recently introduced high quality biomedical NER datasets , we use datasets that are frequently used by many biomedical NLP researchers , which makes it much easier to compare our work with theirs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,The RE datasets contain gene - disease relations and protein - chemical relations ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,Pre-processed GAD and EU - ADR datasets are available with our provided codes .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"For the CHEMPROT dataset , we used the same pre-processing procedure described in .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"We used the BioASQ factoid datasets , which can be converted into the same format as the SQuAD dataset ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,We used full abstracts ( PMIDs ) and related questions and answers provided by the BioASQ organizers .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,We have made the pre-processed BioASQ datasets publicly available .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"For all the datasets , we used the same dataset splits used in previous works ) fora fair evaluation ; however , the splits of LINAAEUS and Species - 800 could not be found from and maybe different .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"Like previous work , we reported the performance of 10 - fold cross-validation on datasets that do not have separate test sets ( e.g. GAD , EU - ADR ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,We compare BERT and BioBERT with the current state - of - theart models and report their scores .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,Note that the state - of - the - art models each have a different architecture and training procedure .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"For instance , the state - of - the - art model by trained on the JNLPBA dataset is based on multiple Bi - LSTM CRF models with character level CNNs , while the state - of - the - art model by trained on the LINNAEUS dataset uses a Bi - LSTM CRF model with character level LSTMs and is additionally trained on silver - standard datasets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"On the other hand , BERT and BioBERT have exactly the same structure , and use only the gold standard datasets and not any additional datasets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,We used the BERT BASE model pre-trained on English Wikipedia and Books Corpus for 1 M steps .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,BioBERT v1.0 ( PubMed PMC ) is the version of BioBERT ( PubMed PMC ) trained for 470 K steps .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"When using both the PubMed and PMC corpora , we found that 200K and 270K pre-training steps were optimal for PubMed and PMC , respectively .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"We also used the ablated versions of BioBERT v1.0 , which were pre-trained on only PubMed for 200 K steps ( Bio BERT v1.0 ( PubMed ) ) and PMC for 270K steps ( Bio BERT v1.0 ( PMC ) ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"After our initial release of BioBERT v 1.0 , we pre-trained BioBERT on PubMed for 1 M steps , and we refer to this version as BioBERT v 1.1 ( PubMed ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,Other hyper - parameters such as batch size and learning rate scheduling for pre-training BioBERT are the same as those for pre-training BERT unless stated otherwise .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,"We pre-trained BioBERT using Naver Smart Machine Learning ( NSML ) , which is utilized for large - scale experiments that need to be run on several GPUs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,We used eight NVIDIA V100 ( 32GB ) GPUs for the pre-training .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"The maximum sequence length was fixed to 512 and the mini-batch size was set to 192 , resulting in 98 304 words per iteration .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,It takes more than 10 days to pre-train BioBERT v 1.0 ( PubMed PMC ) nearly 23 days for BioBERT v 1.1 ( PubMed ) in this setting .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"Despite our best efforts to use BERT LARGE , we used only BERT BASE due to the computational complexity of BERT LARGE .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,We used a single NVIDIA Titan Xp ( 12GB ) GPU to fine - tune BioBERT on each task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,Note that the fine - tuning process is more computationally efficient than pre-training BioBERT .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"For finetuning , a batch size of 10 , 16 , 32 or 64 was selected , and a learning rate of 5e5 , 3e5 or 1e5 was selected .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,Fine - tuning BioBERT on QA and RE tasks took less than an hour as the size of the training data is much smaller than that of the training data used by .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"On the other hand , it takes more than 20 epochs for BioBERT to reach its highest performance on the NER datasets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,The results of NER are shown in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"First , we observe that BERT , which was pre-trained on only the general domain corpus is quite effective , but the micro averaged F 1 score of BERT was lower ( 2.01 lower ) than that of the state - of - the - art models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"On the other hand , BioBERT achieves higher scores than BERT on all the datasets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,"BioBERT outperformed the state - of - the - art models on six out of nine datasets , and BioBERT v 1.1 ( PubMed ) outperformed the state - of - the - art models by 0.62 in terms of micro averaged F1 score .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,"The relatively low scores on the LINNAEUS dataset can be attributed to the following : ( i ) the lack of a silver - standard dataset for training previous state - of - the - art models and ( ii ) different training / test set splits used in previous work , which were unavailable .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,The RE results of each model are shown in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"BERT achieved better performance than the state - of - the - art model on the CHEMPROT dataset , which demonstrates its effectiveness in RE .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"On average ( micro ) , BioBERT v1.0 ( PubMed ) obtained a higher F1 score ( 2.80 higher ) than the state - of - the - art models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"Also , BioBERT achieved the highest F 1 scores on 2 out of 3 biomedical datasets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,The QA results are shown in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,We micro averaged the best scores of the state - of - the - art models from each batch .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,BERT obtained a higher micro averaged MRR score ( 7.0 higher ) than the state - of - the - art models .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,"All versions of BioBERT significantly outperformed BERT and the state - of - the - art models , and in particular , BioBERT v1.1 ( PubMed ) obtained a strict accuracy of 38.77 , a lenient accuracy of 53.81 and a mean reciprocal rank score of 44.77 , all of which were micro averaged .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,"On all the biomedical QA datasets , BioBERT achieved new state - of - the - art performance in terms of MRR .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,We used additional corpora of different sizes for pre-training and investigated their effect on performance .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,"For BioBERT v1.0 ( PubMed ) , we set the number of pre-training steps to 200K and varied the size of the PubMed corpus .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"shows that the performance of BioBERT v 1.0 ( PubMed ) on three NER datasets ( NCBI Disease , BC2GM , BC4CHEMD ) changes in relation to the size of the PubMed corpus .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"Pre-training on 1 billion words is quite effective , and the performance on each dataset mostly improves until 4.5 billion words .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,We also saved the pre-trained weights from BioBERT v 1.0 ( PubMed ) at different pre-training steps to measure how the number of pre-training steps affects its performance on fine - tuning tasks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,shows the performance changes of BioBERT v 1.0 ( PubMed ) on the same three NER datasets in relation to the number of pre-training steps .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,The results clearly show that the performance on each dataset improves as the number of pre-training steps increases .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,"Finally , shows the absolute performance improvements of BioBERT v 1.0 ( PubMed PMC ) over BERT on all 15 datasets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,"F1 scores were used for NER / RE , and MRR scores were used for QA .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,BioBERT significantly improves performance on most of the datasets .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,"As shown in , we sampled predictions from BERT and BioBERT v 1.1 ( PubMed ) to seethe effect of pre-training on downstream tasks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,entities .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,"While BERT often gives incorrect answers to simple biomedical questions , BioBERT provides correct answers to such questions .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,"Also , BioBERT can provide longer named entities as answers .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,"In this article , we introduced BioBERT , which is a pre-trained language representation model for biomedical text mining .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,We showed that pre-training BERT on biomedical corpora is crucial in applying it to the biomedical domain .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,"Requiring minimal task - specific architectural modification , BioBERT outperforms previous models on biomedical text mining tasks such as NER , RE and QA .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"The pre-released version of BioBERT ( January 2019 ) has already been shown to be very effective in many biomedical text mining tasks such as NER for clinical notes , human phenotype - gene RE and clinical temporal RE .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,The following updated versions of BioBERT will be available to the bioNLP community : ( i ) BioBERT BASE and BioBERT LARGE trained on only PubMed abstracts without initialization from the existing BERT model and ( ii ) BioBERT BASE and BioBERT LARGE trained on domain - specific vocabulary based on WordPiece .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"Notes : Precision ( P ) , Recall ( R ) and F1 ( F ) scores on each dataset are reported .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"The best scores are in bold , and the second best scores are underlined .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,"We list the scores of the state - of - the - art ( SOTA ) models on different datasets as follows : scores of Xu et al . Notes : Precision ( P ) , Recall ( R ) and F1 ( F ) scores on each dataset are reported .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",1
2,"The best scores are in bold , and the second best scores are underlined .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"The scores on GAD and EU - ADR were obtained from , and the scores on CHEMPROT were obtained from .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"Notes : Strict Accuracy ( S ) , Lenient Accuracy ( L ) and Mean Reciprocal Rank ( M ) scores on each dataset are reported .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"The best scores are in bold , and the second best scores are underlined .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,The best BioASQ 4 b / 5 b / 6 b scores were obtained from the BioASQ leaderboard ( http://participants-area.bioasq.org ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"BERT . . . a case of oral penicillin anaphylaxis is described , and the terminology . . .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,Corynebacterium minutissimum is the bacteria that leads to cutaneous eruptions of erythrasma . . .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"Like the DMA , but unlike all other mammalian class II A genes , the zebrafish gene codes for two cysteine residues . . .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,A total of 25 women affected by clinical stress urinary incontinence ( SUI ) were enrolled .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,"Q-tip test , . . .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,Corynebacterium minutissimum is the bacteria that leads to cutaneous eruptions of erythrasma . . .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
2,Predicted named entities for NER and predicted answers for QA are in bold .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/9/1901.08746v4-Stanza-out.txt,"Motivation : Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows . With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models . However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora . In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",0
0,It is common that entity mentions can contain other mentions recursively .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,This paper introduces a scalable transition - based method to model the nested structure of mentions .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",1
0,Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"There has been an increasing interest in named entity recognition or more generally recognizing entity mentions 2 ) that the nested hierarchical structure of entity mentions should betaken into account to better facilitate downstream tasks like question answering , relation extraction , event extraction , and coreference resolution .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"Practically , the mentions with nested structures frequently exist in news and biomedical documents .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"For example in Traditional sequence labeling models such as conditional random fields ( CRF ) do not allow hierarchical structures between segments , making them incapable to handle such problems .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,presented a chart - based parsing approach where each sentence with nested mentions is mapped to a rooted constituent tree .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,The issue of using a chart - based parser is its cubic time complexity in the number of words in the sentence .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"To achieve a scalable and effective solution for recognizing nested mentions , we design a transition - based system which is inspired by the recent success of employing transition - based methods for constituent parsing ) and named entity recognition , especially when they are paired with neural networks .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"Generally , each sentence with nested mentions is mapped to a forest where each outermost mention forms a tree consisting of its inner mentions .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,Then our transition - based system learns to construct this forest through a sequence of shift - reduce actions .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,shows an example of such a forest .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"In contrast , the tree structure by further uses a root node to connect all tree elements .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,Our forest representation eliminates the root node so that the number of actions required to construct it can be reduced significantly .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"Following , we employ Stack - LSTM to represent the system 's state , which consists of the states of input , stack and action history , in a continuous space incrementally .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,The ( partially ) processed nested mentions in the stack are encoded with recursive neural networks where composition functions are used to capture dependencies between nested mentions .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"Based on the observation that letter - level patterns such as capitalization and prefix can be beneficial in detecting mentions , we incorporate a characterlevel LSTM to capture such morphological information .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"Meanwhile , this character - level component can also help deal with the out - of - vocabulary problem of neural models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,We conduct experiments in three standard datasets .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,Our system achieves the state - of - the - art performance on ACE datasets and comparable performance in GENIA dataset .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,Entity mention recognition with nested structures has been explored first with rule - based approaches where the authors first detected the innermost mentions and then relied on rule - based postprocessing methods to identify outer mentions .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",1
0,proposed a structured multi-label model to represent overlapping segments in a sentence .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,but it came with a cubic time complexity in the number of words .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,proposed several ways to combine multiple conditional random fields ( CRF ) for such tasks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,Their best results were obtained by cascading several CRF models in a specific order while each model is responsible for detecting mentions of a particular type .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"However , such an approach can not model nested mentions of the same type , which frequently appear .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,and proposed new representations of mention hypergraph and mention separator to model overlapping mentions .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"However , the nested structure is not guaranteed in such approaches since overlapping structures additionally include the crossing structures 3 , which rarely exist in practice .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"Also , their representations did not model the dependencies between nested mentions explicitly , which may limit their performance .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"In contrast , the chart - based parsing method can capture the dependencies between nested mentions with composition rules which allow an outer entity to be influenced by its contained entities .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"However , their cubic time complexity makes them not scalable to large datasets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"As neural network based approaches are proven effective in entity or mention recognition , recent efforts focus on incorporating neural components for recognizing nested mentions .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"dynamically stacked multiple LSTM - CRF layers , detecting mentions in an inside - out manner until no outer entities are extracted .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,used recurrent neural networks to extract features fora hypergraph which encodes all nested mentions based on the BILOU tagging scheme .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",1
0,"Specifically , given a sequence of words {x 0 , x 1 , . . . , x n } , the goal of our system is to output a set of mentions where nested structures are allowed .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",1
0,"We use the forest structure to model the nested mentions scattered in a sentence , as shown in .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",1
0,The mapping is straightforward : each outermost mention forms a tree where the mention is the root and its contained mentions correspond to constituents of the tree .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"Our transition - based model is based on the shiftreduce parser for constituency parsing ( Watan - abe and Sumita , 2015 ) , which adopts .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"Generally , our system employs a stack to store ( partially ) processed nested elements .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"The system 's state is defined as [ S , i , A ] which denotes stack , buffer front index and action history respectively .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,In each step .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,an action is applied to change the system 's state .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,SHIFT pushes the next word from buffer to the stack .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,REDUCE - X pops the top two items t 0 and t 1 from the tack and combines them as anew tree element { X ? t 0 t 1 } which is then pushed onto the stack .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,UNARY - X pops the top item t 0 from the stack and constructs anew tree element { X ? t 0 } which is pushed back to the stack .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"Since the shift - reduce system assumes unary and binary branching , we binarize the trees in each forest in a left - branching manner .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"A , B} , C} where P erson * is a temporary label for P erson .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"Hence , the X in reduce - actions will also include such temporary labels .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"Note that since most words are not contained in any mention , they are only shifted to the stack and not involved in any reduce - or unary - actions .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,An example sequence of transitions can be found in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,Our shift - reduce system is different from previous parsers in terms of the terminal state .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,1 ) It does not require the terminal stack to be a rooted tree .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"Instead , the final stack should be a forest consisting of multiple nested elements with tree structures .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"2 ) To conveniently determine the ending of our transition process , we add an auxiliary symbol $ to each sentence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"Once it is pushed to the stack , it implies that all deductions of actual words are finished .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"X2 , the length of maximal action sequence is 3 n .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"In this case , each word is shifted ( n ) and involved in a unary action ( n ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,Then all elements are reduced to a single node ( n ? 1 ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,The last action is to shift the symbol $. tion to take .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"For example , reduce - action can only be conducted when there are at least two elements in the stack .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,Please seethe Appendix for the full list of restrictions .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"Formally , we use V(S , i , A ) to denote the valid actions given the parser state .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,Let us denote the feature vector for the parser state at time step k asp k .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"( 1 ) where w z is a column weight vector for action z , and b z is a bias term .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"We use neural networks to learn the representation of the parser state , which is pk in ( 1 ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,where e w i and e pi denote the embeddings for i - th word and it s POS tag respectively .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,cw i denotes the representation learned by a character - level model using a bidirectional LSTM .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"Generally , the buffer and action history are encoded using two vanilla LSTMs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"For the stack that involves popping out top elements , we use the Stack - LSTM to efficiently encode it .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,where each action is also mapped to a distributed representation ea .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"6 For the state of the stack , we also use an LSTM to encode a sequence of tree elements .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"However , the top elements of the stack are updated frequently .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,Stack - LSTM provides an efficient implementation that incorporates a stackpointer .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"where W u , l and W b , l denote the weight matrices for unary ( u ) and binary ( b ) composition with parent node being label ( l ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,Note that the composition function is distinct for each label l .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,Recall that the leaf nodes of each tree element are raw words .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"Instead of representing them with their original embeddings introduced in Section 3.3 , we found that 6 Note that LSTM b runs in a right - to - left order such that the output can represent the contextual information of x i.",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,Please refer to for details .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,concatenating the buffer state in ( 5 ) are beneficial during our initial experiments .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,We employ the greedy strategy to maximize the log -likelihood of the local action classifier in ( 1 ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,is the 2 coefficient .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"We mainly evaluate our models on the standard ACE - 04 , , and GENIA datasets with the same splits used by previous research efforts .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"In ACE datasets , more than 40 % of the mentions form nested structures with some other mention .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"In GENIA , this number is 18 % .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,Please see for the full statistics .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,Glo Ve of dimension 100 are used to initialize the word vectors for all three datasets .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,9 The embeddings of POS tags are initialized randomly with dimension 32 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,The model is trained using Adam and a gradient clipping of 3.0 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,Early stopping is used based on the performance of development sets .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,Dropout is used after the input layer .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,is also tuned during development process .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,The main results are reported in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,Our neural transition - based model achieves the best results in ACE datasets and comparable results in GENIA dataset in terms of F 1 measure .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,We hypothesize that the performance gain of our model compared with other methods is largely due to improved performance on the portions of nested mentions in our datasets .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"To verify this , we design an experiment to evaluate how well a system can recognize nested mentions .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,The idea is that we split the test data into two portions : sentences with and without nested mentions .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,The results of GENIA are listed in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"We can observe that the margin of improvement is more significant in the portion of nested mentions , revealing our model 's effectiveness in handling nested mentions .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,This observation helps explain why our model achieves greater improvement in ACE than in GENIA in since the former has much more nested structures than the latter .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"Moreover , performs better when it comes to non-nested mentions possibly due to the CRF they used , which globally normalizes each stacked layer .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"Note that and also feature linear - time complexity , but with a greater constant factor .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"To compare the decoding speed , we re-implemented their model with the same platform ( PyTorch ) and run them on the same machine ( CPU : Intel i5 2.7 GHz ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"Our model turns out to be around 3 - 5 times faster than theirs , showing its scalability .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,We also additionally tried using embeddings trained on PubMed for GENIA but the performance was comparable .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"To evaluate the contribution of neural components including pre-trained embeddings , the characterlevel LSTM and dropout layers , we test the performances of ablated models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,The results are listed in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,"From the performance gap , we can conclude that these components contribute significantly to the effectiveness of our model in all three datasets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",1
0,"In this paper , we present a transition - based model for nested mention recognition using a forest representation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",1
0,"Coupled with Stack - LSTM for representing the system 's state , our neural model can capture dependencies between nested mentions efficiently .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",1
0,"Moreover , the character - based component helps capture letter - level patterns in words .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,The system achieves the state - of - the - art performance in ACE datasets .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,One potential drawback of the system is the greedy training and decoding .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,We believe that alternatives like beam search and training with exploration could further boost the performance .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",0
0,Another direction that we plan to work on is to apply this model to recognizing overlapping and entities that involve discontinuous spans ( Muis and which frequently exist in the biomedical domain .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/7/1810.01808v1-Stanza-out.txt,"It is common that entity mentions can contain other mentions recursively . This paper introduces a scalable transition - based method to model the nested structure of mentions . We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest . Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length . Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns . Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",1
5,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,"Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,"As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,BERT is conceptually simple and empirically powerful .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,"It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,Jeremy Howard and Sebastian Ruder . 2018 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,Universal language model fine - tuning for text classification .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,In ACL .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,Association for Computational Linguistics .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,Language model pre-training has been shown to be effective for improving many natural language processing tasks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"These include sentence - level tasks such as natural language inference and paraphrasing , which aim to predict the relationships between sentences by analyzing them holistically , as well as token - level tasks such as named entity recognition and question answering , where models are required to produce fine - grained output at the token level .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,There are two existing strategies for applying pre-trained language representations to downstream tasks : feature - based and fine - tuning .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"The feature - based approach , such as ELMo , uses task - specific architectures that include the pre-trained representations as additional features .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"The fine - tuning approach , such as the Generative Pre-trained Transformer ( OpenAI GPT ) , introduces minimal task - specific parameters , and is trained on the downstream tasks by simply fine - tuning all pretrained parameters .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"The two approaches share the same objective function during pre-training , where they use unidirectional language models to learn general language representations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"We argue that current techniques restrict the power of the pre-trained representations , especially for the fine - tuning approaches .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"The major limitation is that standard language models are unidirectional , and this limits the choice of architectures that can be used during pre-training .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"For example , in Open AI GPT , the authors use a left - toright architecture , where every token can only attend to previous tokens in the self - attention layers of the Transformer .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Such restrictions are sub-optimal for sentence - level tasks , and could be very harmful when applying finetuning based approaches to token - level tasks such as question answering , where it is crucial to incorporate context from both directions .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"In this paper , we improve the fine - tuning based approaches by proposing BERT : Bidirectional Encoder Representations from Transformers .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"BERT alleviates the previously mentioned unidirectionality constraint by using a "" masked language model "" ( MLM ) pre-training objective , inspired by the Cloze task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"The masked language model randomly masks some of the tokens from the input , and the objective is to predict the original vocabulary id of the masked word based only on its context .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Unlike left - toright language model pre-training , the MLM objective enables the representation to fuse the left and the right context , which allows us to pretrain a deep bidirectional Transformer .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"In addition to the masked language model , we also use a "" next sentence prediction "" task that jointly pretrains text - pair representations .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,We demonstrate the importance of bidirectional pre-training for language representations .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Unlike , which uses unidirectional language models for pre-training , BERT uses masked language models to enable pretrained deep bidirectional representations .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"This is also in contrast to , which uses a shallow concatenation of independently trained left - to - right and right - to - left LMs .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,We show that pre-trained representations reduce the need for many heavily - engineered taskspecific architectures .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"BERT is the first finetuning based representation model that achieves state - of - the - art performance on a large suite of sentence - level and token - level tasks , outperforming many task - specific architectures .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,BERT advances the state of the art for eleven NLP tasks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,google-research/bert .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"There is along history of pre-training general language representations , and we briefly review the most widely - used approaches in this section .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Learning widely applicable representations of words has been an active area of research for decades , including non-neural and neural methods .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,"Pre-trained word embeddings are an integral part of modern NLP systems , offering significant improvements over embeddings learned from scratch .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,"To pretrain word embedding vectors , left - to - right language modeling objectives have been used , as well as objectives to discriminate correct from incorrect words in left and right context .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"These approaches have been generalized to coarser granularities , such as sentence embeddings or paragraph embeddings .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"To train sentence representations , prior work has used objectives to rank candidate next sentences , left - to - right generation of next sentence words given a representation of the previous sentence , or denoising autoencoder derived objectives .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,ELMo and its predecessor generalize traditional word embedding research along a different dimension .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,They extract context - sensitive features from a left - to - right and a right - to - left language model .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,The contextual representation of each token is the concatenation of the left - to - right and right - to - left representations .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"When integrating contextual word embeddings with existing task - specific architectures , ELMo advances the state of the art for several major NLP benchmarks including question answering , sentiment analysis , and named entity recognition .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Similar to ELMo , their model is feature - based and not deeply bidirectional .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,shows that the cloze task can be used to improve the robustness of text generation models .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"As with the feature - based approaches , the first works in this direction only pre-trained word embedding parameters from unlabeled text .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"More recently , sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine - tuned fora supervised downstream task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,The advantage of these approaches is that few parameters need to be learned from scratch .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"At least partly due to this advantage , OpenAI achieved previously state - of - the - art results on many sentencelevel tasks from the GLUE benchmark .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,Left - to - right language model - BERT BERT E E 1 E ...,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,CT 1 T ... E 1 E ...,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,Overall pre-training and fine - tuning procedures for BERT .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Apart from output layers , the same architectures are used in both pre-training and fine - tuning .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,The same pre-trained model parameters are used to initialize models for different down - stream tasks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,"During fine - tuning , all parameters are fine - tuned .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"[ CLS ] is a special symbol added in front of every input example , and [ SEP ] is a special separator token ( e.g. separating questions / answers ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,ing and auto - encoder objectives have been used for pre-training such models .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"There has also been work showing effective transfer from supervised tasks with large datasets , such as natural language inference and machine translation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Computer vision research has also demonstrated the importance of transfer learning from large pre-trained models , where an effective recipe is to fine - tune models pre-trained with I ma - geNet .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,We introduce BERT and its detailed implementation in this section .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,There are two steps in our framework : pre-training and fine - tuning .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,"During pre-training , the model is trained on unlabeled data over different pre-training tasks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,"For finetuning , the BERT model is first initialized with the pre-trained parameters , and all of the parameters are fine - tuned using labeled data from the downstream tasks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Each downstream task has separate fine - tuned models , even though they are initialized with the same pre-trained parameters .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,The question - answering example in will serve as a running example for this section .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,A distinctive feature of BERT is its unified architecture across different tasks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,There is mini-mal difference between the pre-trained architecture and the final downstream architecture .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,BERT 's model architecture is a multi - layer bidirectional Transformer encoder based on the original implementation and released in the tensor2 tensor library .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"In this work , we denote the number of layers ( i.e. , Transformer blocks ) as L , the hidden size as H , and the number of self - attention heads as A .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"We primarily report results on two model sizes : BERT BASE ( L=12 , H = 768 , A = 12 , Total Param-eters=110M ) and BERT LARGE ( L=24 , H = 1024 , A = 16 , Total Parameters=340M ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,BERT BASE was chosen to have the same model size as OpenAI GPT for comparison purposes .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Critically , however , the BERT Transformer uses bidirectional self - attention , while the GPT Transformer uses constrained self - attention where every token can only attend to context to its left .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"To make BERT handle a variety of down - stream tasks , our input representation is able to unambiguously represent both a single sentence and a pair of sentences ( e.g. , Question , Answer ) in one token sequence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Throughout this work , a "" sentence "" can bean arbitrary span of contiguous text , rather than an actual linguistic sentence .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"A "" sequence "" refers to the input token sequence to BERT , which maybe a single sentence or two sentences packed together .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"( Wu et al. , 2016 ) with a 30,000 token vocabulary .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,The first token of every sequence is always a special classification token ( [ CLS ] ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,Sentence pairs are packed together into a single sequence .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,We differentiate the sentences in two ways .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"First , we separate them with a special token ( [ SEP ] ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Second , we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"For a given token , its input representation is constructed by summing the corresponding token , segment , and position embeddings .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"A visualization of this construction can be seen in . ( 2018 ) , we do not use traditional left - to - right or right - to - left language models to pre-train BERT .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Instead , we pre-train BERT using two unsupervised tasks , described in this section .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,This step is presented in the left part of .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Intuitively , it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left - to - right model or the shallow concatenation of a left - toright and a right - to - left model .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,"Unfortunately , standard conditional language models can only be trained left - to - right or right - to - left , since bidirectional conditioning would allow each word to indirectly "" see itself "" , and the model could trivially predict the target word in a multi - layered context .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,"former is often referred to as a "" Transformer encoder "" while the left - context - only version is referred to as a "" Transformer decoder "" since it can be used for text generation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,"In order to train a deep bidirectional representation , we simply mask some percentage of the input tokens at random , and then predict those masked tokens .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,"We refer to this procedure as a "" masked LM "" ( MLM ) , although it is often referred to as a Cloze task in the literature .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,"In this case , the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary , as in a standard LM .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,"In all of our experiments , we mask 15 % of all WordPiece tokens in each sequence at random .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"In contrast to denoising auto - encoders , we only predict the masked words rather than reconstructing the entire input .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Although this allows us to obtain a bidirectional pre-trained model , a downside is that we are creating a mismatch between pre-training and fine - tuning , since the [ MASK ] token does not appear during fine - tuning .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"To mitigate this , we do not always replace "" masked "" words with the actual [ MASK ] token .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,The training data generator chooses 15 % of the token positions at random for prediction .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"If the i - th token is chosen , we replace the i - th token with ( 1 ) the [ MASK ] token 80 % of the time ( 2 ) a random token 10 % of the time ( 3 ) the unchanged i - th token 10 % of the time .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,Ti will be used to predict the original token with cross entropy loss .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,We compare variations of this procedure in Appendix C.2 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Task # 2 : Next Sentence Prediction ( NSP ) Many important downstream tasks such as Question Answering ( QA ) and Natural Language Inference ( NLI ) are based on understanding the relationship between two sentences , which is not directly captured by language modeling .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"In order to train a model that understands sentence relationships , we pre-train fora binarized next sentence prediction task that can be trivially generated from any monolingual corpus .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Specifically , when choosing the sentences A and B for each pretraining example , 50 % of the time B is the actual next sentence that follows A ( labeled as IsNext ) , and 50 % of the time it is a random sentence from the corpus ( labeled as NotNext ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"As we show in , C is used for next sentence prediction ( NSP ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"5 Despite its simplicity , we demonstrate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,5 The final model achieves 97 % - 98 % accuracy on NSP .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"The vector C is not a meaningful sentence representation without fine - tuning , since it was trained with NSP .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,Embeddings : BERT input representation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"The input embeddings are the sum of the token embeddings , the segmentation embeddings and the position embeddings .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,The NSP task is closely related to representationlearning objectives used in Jernite et al. and Logeswaran and Lee ( 2018 ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"However , in prior work , only sentence embeddings are transferred to down - stream tasks , where BERT transfers all parameters to initialize end - task model parameters .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,The pre-training procedure largely follows the existing literature on language model pre-training .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"For the pre-training corpus we use the Books Corpus ( 800M words ) and English Wikipedia ( 2,500 M words ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"For Wikipedia we extract only the text passages and ignore lists , tables , and headers .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,It is critical to use a document - level corpus rather than a shuffled sentence - level corpus such as the Billion Word Benchmark in order to extract long contiguous sequences .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,Fine- tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream tasks whether they involve single text or text pairs - by swapping out the appropriate inputs and outputs .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"For applications involving text pairs , a common pattern is to independently encode text pairs before applying bidirectional cross attention , such as Parikh et al. ; Seo et al. ( 2017 ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"BERT instead uses the self - attention mechanism to unify these two stages , as encoding a concatenated text pair with self - attention effectively includes bidirectional cross attention between two sentences .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"For each task , we simply plugin the taskspecific inputs and outputs into BERT and finetune all the parameters end - to - end .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,pair in text classification or sequence tagging .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"At the output , the token representations are fed into an output layer for tokenlevel tasks , such as sequence tagging or question answering , and the [ CLS ] representation is fed into an output layer for classification , such as entailment or sentiment analysis .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Compared to pre-training , fine - tuning is relatively inexpensive .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU , or a few hours on a GPU , starting from the exact same pre-trained model .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,We describe the task - specific details in the corresponding subsections of Section 4 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,More details can be found in Appendix A.5 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"In this section , we present BERT fine - tuning results on 11 NLP tasks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"The General Language Understanding Evaluation ( GLUE ) benchmark ( Wang et al. , 2018 a ) is a collection of diverse natural language understanding tasks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,Detailed descriptions of GLUE datasets are included in Appendix B.1 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,R H corresponding to the first input token ( [ CLS ] ) as the aggregate representation .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"R KH , where K is the number of labels .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Test results , scored by the evaluation server ( https://gluebenchmark.com/leaderboard ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,The number below each task denotes the number of training examples .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"The "" Average "" column is slightly different than the official GLUE score , since we exclude the problematic WNLI set .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"8 BERT and OpenAI GPT are singlemodel , single task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"F1 scores are reported for QQP and MRPC , Spearman correlations are reported for STS - B , and accuracy scores are reported for the other tasks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,We exclude entries that use BERT as one of their components .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,We use a batch size of 32 and fine - tune for 3 epochs over the data for all GLUE tasks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"For each task , we selected the best fine - tuning learning rate ( among 5 e - 5 , 4 e - 5 , 3 e - 5 , and 2 e - 5 ) on the Dev set .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Additionally , for BERT LARGE we found that finetuning was sometimes unstable on small datasets , so we ran several random restarts and selected the best model on the Dev set .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"With random restarts , we use the same pre-trained checkpoint but perform different fine - tuning data shuffling and classifier layer initialization .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,Results are presented in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Both BERT BASE and BERT LARGE outperform all systems on all tasks by a substantial margin , obtaining 4.5 % and 7.0 % respective average accuracy improvement over the prior state of the art .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,Note that BERT BASE and OpenAI GPT are nearly identical in terms of model architecture apart from the attention masking .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"For the largest and most widely reported GLUE task , MNLI , BERT obtains a 4.6 % absolute accuracy improvement .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"On the official GLUE leaderboard 10 , BERT LARGE obtains a score of 80.5 , compared to OpenAI GPT , which obtains 72.8 as of the date of writing .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"We find that BERT LARGE significantly outperforms BERT BASE across all tasks , especially those with very little training data .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,The effect of model size is explored more thoroughly in Section 5.2 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,The Stanford Question Answering Dataset ( SQuAD v1.1 ) is a collection of 100 k crowdsourced question / answer pairs .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Given a question and a passage from The GLUE data set distribution does not include the Test labels , and we only made a single GLUE evaluation server submission for each of BERTBASE and BERTLARGE .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Wikipedia containing the answer , the task is to predict the answer text span in the passage .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"As shown in , in the question answering task , we represent the input question and passage as a single packed sequence , with the question using the A embedding and the passage using the B embedding .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,R H during fine - tuning .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,The probability of word i being the start of the answer span is computed as a dot product between Ti and S followed by a softmax overall of the words in the paragraph : P i = e ST i j e ST j .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,The analogous formula is used for the end of the answer span .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,i is used as a prediction .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,The training objective is the sum of the log-likelihoods of the correct start and end positions .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,We fine - tune for 3 epochs with a learning rate of 5 e - 5 and a batch size of 32 .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,shows top leaderboard entries as well as results from top published systems .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,"The top results from the SQuAD leaderboard do not have up - to - date public system descriptions available , 11 and are allowed to use any public data when training their systems .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"We therefore use modest data augmentation in our system by first fine - tuning on TriviaQA ( Joshi et al. , 2017 ) befor fine - tuning on SQuAD .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,Our best performing system outperforms the top leaderboard system by + 1.5 F1 in ensembling and + 1.3 F1 as a single system .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"In fact , our single BERT model outperforms the top ensemble system in terms of F1 score .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,"Without Trivia QA fine - tuning data , we only lose 0.1 - 0.4 F1 , still outperforming all existing systems by a wide margin .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,"The SQuAD 2.0 task extends the SQuAD 1.1 problem definition by allowing for the possibility that no short answer exists in the provided paragraph , making the problem more realistic .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,We use a simple approach to extend the SQuAD v1.1 BERT model for this task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,We treat questions that do not have an answer as having an answer span with start and end at the [ CLS ] token .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,The probability space for the start and end answer span positions is extended to include the position of the [ CLS ] token .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,"For prediction , we compare the score of the no -answer span : s null = SC + EC to the score of the best non - null span The Trivia QA data we used consists of paragraphs from TriviaQA - Wiki formed of the first 400 tokens in documents , that contain at least one of the provided possible answers .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,is selected on the dev set to maximize F 1 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,We did not use Trivia QA data for this model .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,We fine - tuned for 2 epochs with a learning rate of 5 e - 5 and a batch size of 48 .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"The results compared to prior leaderboard entries and top published work are shown in , excluding systems that use BERT as one of their components .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,We observe a + 5.1 F1 improvement over the previous best system .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,The Situations With Adversarial Generations ( SWAG ) dataset contains 113 k sentence - pair completion examples that evaluate grounded commonsense inference .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Given a sentence , the task is to choose the most plausible continuation among four choices .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"When fine - tuning on the SWAG dataset , we construct four input sequences , each containing the concatenation of the given sentence ( sentence A ) and a possible continuation ( sentence B ) .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,The only task - specific parameters introduced is a vector whose dot product with the [ CLS ] token representation C denotes a score for each choice which is normalized with a softmax layer .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,We fine - tune the model for 3 epochs with a learning rate of 2 e - 5 and a batch size of 16 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,Results are presented in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,BERT LARGE outperforms the authors ' baseline ESIM + ELMo system by + 27.1 % and OpenAI GPT by 8.3 % .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"In this section , we perform ablation experiments over a number of facets of BERT in order to better understand their relative importance .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,Additional : Ablation over the pre-training tasks using the BERT BASE architecture .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,""" No NSP "" is trained without the next sentence prediction task .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,""" LTR & No NSP "" is trained as a left - to - right LM without the next sentence prediction , like OpenAI GPT .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,""" + BiLSTM "" adds a randomly initialized BiLSTM on top of the "" LTR + No NSP "" model during fine - tuning .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,ablation studies can be found in Appendix C.,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"A bidirectional model which is trained using the "" masked LM "" ( MLM ) but without the "" next sentence prediction "" ( NSP ) task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"A left - context - only model which is trained using a standard Left - to - Right ( LTR ) LM , rather than an MLM .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"The left - only constraint was also applied at fine - tuning , because removing it introduced a pre-train / fine - tune mismatch that degraded downstream performance .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Additionally , this model was pre-trained without the NSP task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"This is directly comparable to OpenAI GPT , but using our larger training dataset , our input representation , and our fine - tuning scheme .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,We first examine the impact brought by the NSP task .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"In , we show that removing NSP hurts performance significantly on QNLI , MNLI , and SQu AD 1.1 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Next , we evaluate the impact of training bidirectional representations by comparing "" No NSP "" to "" LTR & No NSP "" .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,"The LTR model performs worse than the MLM model on all tasks , with large drops on MRPC and SQuAD .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,"For SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions , since the token - level hidden states have no rightside context .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,"In order to make a good faith attempt at strengthening the LTR system , we added a randomly initialized BiLSTM on top .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,"This does significantly improve results on SQuAD , but the results are still far worse than those of the pretrained bidirectional models .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,The BiLSTM hurts performance on the GLUE tasks .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"We recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two models , as ELMo does .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"However : ( a ) this is twice as expensive as a single bidirectional model ; ( b ) this is non-intuitive for tasks like QA , since the RTL model would not be able to condition the answer on the question ; ( c ) this it is strictly less powerful than a deep bidirectional model , since it can use both left and right context at every layer .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"In this section , we explore the effect of model size on fine - tuning task accuracy .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"We trained a number of BERT models with a differing number of layers , hidden units , and attention heads , while otherwise using the same hyperparameters and training procedure as described previously .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,Results on selected GLUE tasks are shown in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"In this table , we report the average Dev Set accuracy from 5 random restarts of fine - tuning .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"We can see that larger models lead to a strict accuracy improvement across all four datasets , even for MRPC which only has 3,600 labeled training examples , and is substantially different from the pre-training tasks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,It is also perhaps surprising that we are able to achieve such significant improvements on top of models which are already quite large relative to the existing literature .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,"For example , the largest Transformer explored in is ( L=6 , H = 1024 , A = 16 ) with 100M parameters for the encoder , and the largest Transformer we have found in the literature is ( L=64 , H = 512 , A=2 ) with 235M parameters .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,"By contrast , BERT BASE contains 110M parameters and BERT LARGE contains 340M parameters .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,"It has long been known that increasing the model size will lead to continual improvements on large - scale tasks such as machine translation and language modeling , which is demonstrated by the LM perplexity of held - out training data shown in .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,"However , we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks , provided that the model has been sufficiently pre-trained .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,"presented mixed results on the downstream task impact of increasing the pre-trained bi - LM size from two to four layers and mentioned in passing that increasing hidden dimension size from 200 to 600 helped , but increasing further to 1,000 did not bring further improvements .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Both of these prior works used a featurebased approach - we hypothesize that when the model is fine - tuned directly on the downstream tasks and uses only a very small number of randomly initialized additional parameters , the taskspecific models can benefit from the larger , more expressive pre-trained representations even when downstream task data is very small .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"All of the BERT results presented so far have used the fine - tuning approach , where a simple classification layer is added to the pre-trained model , and all parameters are jointly fine - tuned on a downstream task .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"However , the feature - based approach , where fixed features are extracted from the pretrained model , has certain advantages .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"First , not all tasks can be easily represented by a Transformer encoder architecture , and therefore require a task - specific model architecture to be added .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Second , there are major computational benefits to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"In this section , we compare the two approaches by applying BERT to the CoNLL - 2003 Named Entity Recognition ( NER ) task .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"In the input to BERT , we use a case - preserving WordPiece model , and we include the maximal document context provided by the data .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Following standard practice , we formulate this as a tagging task but do not use a CRF layer in the output .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,We use the representation of the first sub-token as the input to the token - level classifier over the NER label set .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"To ablate the fine - tuning approach , we apply the feature - based approach by extracting the activations from one or more layers without fine - tuning any parameters of BERT .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,These contextual embeddings are used as input to a randomly initialized two - layer 768 - dimensional BiLSTM before the classification layer .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,Results are presented in .,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,BERT LARGE performs competitively with state - of - the - art methods .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,"The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer , which is only 0.3 F1 behind fine - tuning the entire model .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,This demonstrates that BERT is effective for both finetuning and feature - based approaches .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Recent empirical improvements due to transfer learning with language models have demonstrated that rich , unsupervised pre-training is an integral part of many language understanding systems .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"In particular , these results enable even low - resource tasks to benefit from deep unidirectional architectures .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Our major contribution is further generalizing these findings to deep bidirectional architectures , allowing the same pre-trained model to successfully tackle abroad set of NLP tasks .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,Additional ablation studies are presented in Appendix C.,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,- Effect of Number of Training Steps ; and - Ablation for Different Masking Procedures .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,We provide examples of the pre-training tasks in the following .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Masked LM and the Masking Procedure Assuming the unlabeled sentence is my dog is hairy , and during the random masking procedure we chose the 4 - th token ( which corresponding to hairy ) , our masking procedure can be further illustrated by The purpose of this is to bias the representation towards the actual observed word .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"The advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predictor which have been replaced by random words , so it is forced to keep a distributional contextual representation of every input token .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Additionally , because random replacement only occurs for 1.5 % of all tokens ( i.e. , 10 % of 15 % ) , this does not seem to harm the model 's language understanding capability .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"In Section C.2 , we evaluate the impact this procedure .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Compared to standard langauge model training , the masked LM only make predictions on 15 % of tokens in each batch , which suggests that more pre-training steps maybe required for the model to converge .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"In Section C.1 we demonstrate that MLM does converge marginally slower than a leftto - right model ( which predicts every token ) , but the empirical improvements of the MLM model far outweigh the increased training cost .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,The next sentence prediction task can be illustrated in the following examples .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"To generate each training input sequence , we sample two spans of text from the corpus , which we refer to as "" sentences "" even though they are typically much longer than single sentences ( but can be shorter also ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,The first sentence receives the A embedding and the second receives the B embedding .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"50 % of the time B is the actual next sentence that follows A and 50 % of the time it is a random sentence , which is done for the "" next sentence prediction "" task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,They are sampled such that the combined length is ? 512 tokens .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"The LM masking is applied after WordPiece tokenization with a uniform masking rate of 15 % , and no special consideration given to partial word pieces .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"We train with batch size of 256 sequences ( 256 sequences * 512 tokens = 128,000 tokens / batch ) for 1,000,000 steps , which is approximately 40 epochs over the 3.3 billion word corpus .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"We use Adam with learning rate of 1 e - 4 , ? 1 = 0.9 , ? 2 = 0.999 , L2 weight decay of 0.01 , learning rate warmup over the first 10,000 steps , and linear decay of the learning rate .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,We use a dropout probability of 0.1 on all layers .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"We use a gelu activation rather than the standard relu , following OpenAI GPT .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,Training of BERT BASE was performed on 4 Cloud TPUs in Pod configuration ( 16 TPU chips total ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,13 Training of BERT LARGE was performed on 16 Cloud TPUs ( 64 TPU chips total ) .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,Each pretraining took 4 days to complete .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,Longer sequences are disproportionately expensive because attention is quadratic to the sequence length .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"To speedup pretraing in our experiments , we pre-train the model with sequence length of 128 for 90 % of the steps .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Then , we train the rest 10 % of the steps of sequence of 512 to learn the positional embeddings .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"For fine - tuning , most model hyperparameters are the same as in pre-training , with the exception of the batch size , learning rate , and number of training epochs .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,The dropout probability was always kept at 0.1 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"We also observed that large data sets ( e.g. , 100 k + labeled training examples ) were far less sensitive to hyperparameter choice than small data sets .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Fine - tuning is typically very fast , so it is reasonable to simply run an exhaustive search over the above parameters and choose the model that performs best on the development set .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Here we studies the differences in recent popular representation learning models including ELMo , OpenAI GPT and BERT .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,The comparisons between the model architectures are shown visually in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Note that in addition to the architecture differences , BERT and OpenAI GPT are finetuning approaches , while ELMo is a feature - based approach .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"The most comparable existing pre-training method to BERT is OpenAI GPT , which trains a left - to - right Transformer LM on a large text corpus .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"In fact , many of the design decisions in BERT were intentionally made to make it as close to GPT as possible so that the two methods could be minimally compared .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"GPT is trained on the Books Corpus ( 800M words ) ; BERT is trained on the Books Corpus ( 800M words ) and Wikipedia ( 2,500 M words ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"GPT was trained for 1 M steps with a batch size of 32,000 words ; BERT was trained for 1 M steps with a batch size of 128,000 words .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,GPT used the same learning rate of 5 e - 5 for all fine - tuning experiments ; BERT chooses a task - specific fine - tuning learning rate which performs the best on the development set .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"To isolate the effect of these differences , we perform ablation experiments in Section 5.1 which demonstrate that the majority of the improvements are in fact coming from the two pre-training tasks and the bidirectionality they enable .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,The illustration of fine - tuning BERT on different tasks can be seen in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Our task - specific models are formed by incorporating BERT with one additional output layer , so a minimal number of parameters need to be learned from scratch .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Among the tasks , ( a ) and MNLI Multi - Genre Natural Language Inference is a large - scale , crowdsourced entailment classification task .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Given a pair of sentences , the goal is to predict whether the second sentence is an entailment , contradiction , or neutral with respect to the first one .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,Pairs is a binary classification task where the goal is to determine if two questions asked on Quora are semantically equivalent .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,E 1 E ...,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,...,1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,The Stanford Sentiment Treebank is a binary single - sentence classification task consisting of sentences extracted from movie reviews with human annotations of their sentiment .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"The Corpus of Linguistic Acceptability is a binary single - sentence classification task , where the goal is to predict whether an English sentence is linguistically "" acceptable "" or not .",1,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,Benchmark is a collection of sentence pairs drawn from news headlines and other sources .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,They were annotated with a score from 1 to 5 denoting how similar the two sentences are in terms of semantic meaning .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Microsoft Research Paraphrase Corpus consists of sentence pairs automatically extracted from online news sources , with human annotations for whether the sentences in the pair are semantically equivalent .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Recognizing Textual Entailment is a binary entailment task similar to MNLI , but with much less training data ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,14 WNLI Winograd NLI is a small natural language inference dataset .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"The GLUE webpage notes that there are issues with the construction of this dataset , 15 and every trained system that 's been submitted to GLUE has performed worse than the 65.1 baseline accuracy of predicting the majority class .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,We therefore exclude this set to be fair to OpenAI GPT .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"For our GLUE submission , we always predicted the ma-jority class .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,Dev accuracy after finetuning from a checkpoint that has been pre-trained fork steps .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",1
5,1 .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Answer : Yes , BERT BASE achieves almost 1.0 % additional accuracy on MNLI when trained on 1 M steps compared to 500 k steps .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,Answer : The MLM model does converge slightly slower than the LTR model .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"However , in terms of absolute accuracy the MLM model begins to outperform the LTR model almost immediately .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"In Section 3.1 , we mention that BERT uses a mixed strategy for masking the target tokens when pre-training with the masked language model ( MLM ) objective .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,The following is an ablation study to evaluate the effect of different masking strategies .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Note that the purpose of the masking strategies is to reduce the mismatch between pre-training and fine - tuning , as the [ MASK ] symbol never appears during the fine - tuning stage .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,We report the Dev results for both MNLI and NER .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"For NER , we report both fine - tuning and feature - based approaches , as we expect the mismatch will be amplified for the feature - based approach as the model will not have the chance to adjust the representations .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,The results are presented in .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"In the table , MASK means that we replace the target token with the [ MASK ] symbol for MLM ; SAME means that we keep the target token as is ; RND means that we replace the target token with another random token .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"The numbers in the left part of the table represent the probabilities of the specific strategies used during MLM pre-training ( BERT uses 80 % , 10 % , 10 % ) .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,The right part of the paper represents the Dev set results .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"For the feature - based approach , we concatenate the last 4 layers of BERT as the features , which was shown to be the best approach in Section 5.3 .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,From the table it can be seen that fine - tuning is surprisingly robust to different masking strategies .,0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"However , as expected , using only the MASK strategy was problematic when applying the featurebased approach to NER .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
5,"Interestingly , using only the RND strategy performs much worse than our strategy as well .",0,/Users/rohantondulkar/Projects/Typeset/trial-data/named-entity-recognition/8/1810.04805v2-Stanza-out.txt,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model fine - tuning for text classification . In ACL . Association for Computational Linguistics .",0
